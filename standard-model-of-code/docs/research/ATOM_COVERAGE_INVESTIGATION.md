# Atom Coverage Investigation

> **Classification:** Strategic Research Finding
> **Date:** 2026-01-22
> **Investigator:** Claude Opus 4.5 + Leonardo Lech
> **Status:** VALIDATED (95%+ confidence on core findings)

---

## Executive Summary

An investigation into the atom inventory revealed a **fundamental insight** about code classification that reframes the entire Standard Model of Code positioning:

> **FINDING:** In the 7 analyzed repos (Python/JS/TS/Rust), the top 2–4 structural atoms account for ~80–90% of nodes. Function + class atoms dominate; 3,500+ T2 atoms provide semantic enrichment rather than additional structural coverage.

This is not a bug—it's a **feature** that aligns with physics: a small number of fundamental particles explain the universe, while the periodic table of elements provides domain-specific utility.

---

## Operational Definitions

To avoid ambiguity, this report uses the following definitions:

| Term | Definition |
|------|------------|
| **Node** | A code entity extracted from parsing (e.g., function, class, variable, module-level construct) that becomes a node in `unified_analysis.json`. |
| **Structural Atom (D1:WHAT)** | A deterministic label assigned from AST/type-to-atom mapping (base atoms). Every node receives a structural atom label (including `Unknown` when mapping is incomplete). |
| **Recognized Structural Atom Rate** | `% of nodes whose D1 atom is not Unknown`. |
| **Semantic Enrichment (T2)** | Optional ecosystem/pattern-derived labels added by pattern matching (Stage 2.10). Not all nodes can or should match. |
| **Structural Coverage** | `% of nodes receiving *some* D1 atom label` (typically ~100% by construction). |
| **Semantic Coverage (T2 Rate)** | `% of nodes receiving a T2 semantic label` (variable by ecosystem). |
| **Top-k Mass** | `% of nodes covered by the k most frequent atoms`. E.g., "top-4 mass = 90%" means the 4 most common atoms cover 90% of nodes. |
| **Unknown** | Nodes extracted by parsing that do not map cleanly to current atom mapping rules. This is a **measurable classifier gap**, not noise. |

---

## Investigation Methodology

### The Collider Pipeline (How Analysis Works)

Understanding the investigation requires understanding how Collider analyzes code.

> **VALIDATED 2026-01-22** via Gemini 2.5 Pro forensic analysis of `full_analysis.py`
> **Validated against commit:** `d330944` — Line references refer to this commit; re-run validation after pipeline refactors.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    COLLIDER PIPELINE (Actual Implementation)                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  INPUT: Source code directory                                               │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 1: BASE ANALYSIS (unified_analysis.py sub-pipeline)           │   │
│  │ This is NOT a single step - it's a full sub-pipeline:               │   │
│  │ • AST Parsing (TreeSitterUniversalEngine)          L314             │   │
│  │ • Auto Pattern Discovery                           L330             │   │
│  │ • LLM Enrichment (optional)                        L336             │   │
│  │ • EDGE EXTRACTION (calls, imports) ← HAPPENS EARLY L349-368         │   │
│  │ • Graph-Based Inference                            L374             │   │
│  │ • Initial Standard Model Enrichment                L410             │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 2: STANDARD MODEL ENRICHMENT                 L1173            │   │
│  │ • RPBL scores (Responsibility, Purity, Boundary, Lifecycle)         │   │
│  │ • Flatten nodes for downstream use                                  │   │
│  │                                                                     │   │
│  │ STAGE 2.5: ECOSYSTEM DISCOVERY                     L1199            │   │
│  │ • Identify unknown ecosystem patterns                               │   │
│  │                                                                     │   │
│  │ STAGE 2.7: DIMENSION CLASSIFICATION                L1209            │   │
│  │ • D4 (Boundary), D5 (State), D7 (Lifecycle)                         │   │
│  │                                                                     │   │
│  │ STAGE 2.8-2.11: ANALYSIS PASSES                    L1219-1359       │   │
│  │ • Scope Analysis (unused vars, shadowing)                           │   │
│  │ • Control Flow (cyclomatic complexity)                              │   │
│  │ • Pattern-Based Atom Detection                                      │   │
│  │ • Data Flow Analysis (D6:EFFECT purity)                             │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 3-4: PURPOSE & EXECUTION FLOW                L1407-1456       │   │
│  │ • Purpose Field detection                                           │   │
│  │ • π₃/π₄ emergent purpose computation                                │   │
│  │ • Entry points and orphan detection                                 │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 5-6: GRAPH ANALYTICS                         L1465-1631       │   │
│  │ • Markov transition matrix                                          │   │
│  │ • Knot detection (cycles, tangles)                                  │   │
│  │ • PageRank, centrality, degrees                                     │   │
│  │ • Disconnection classification                                      │   │
│  │ • Statistical metrics (Halstead)                                    │   │
│  │ • CODOME BOUNDARIES (synthetic nodes for callers)  L1631            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 7-8: ADVANCED ANALYSIS                       L1658-1701       │   │
│  │ • Data flow sources/sinks                                           │   │
│  │ • Performance prediction                                            │   │
│  │ • CONSTRAINT FIELD VALIDATION (antimatter)         L1677            │   │
│  │ • Purpose Intelligence (Q-Scores)                  L1701            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ STAGE 9-12: OUTPUTS                                L1776-1850       │   │
│  │ • Roadmap evaluation (optional)                                     │   │
│  │ • Visual topology classification                                    │   │
│  │ • Semantic Cortex (concept extraction)                              │   │
│  │ • AI Insights (optional Gemini)                                     │   │
│  │ • Generate unified_analysis.json + HTML                             │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│         ↓                                                                   │
│  OUTPUT: unified_analysis.json, collider_report.html                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Key Architectural Insight:**
```
The pipeline flow is: PARSE → BUILD GRAPH → ANALYZE/ENRICH GRAPH
                      (not: Parse → Classify → Then Build Graph)

Edge extraction happens EARLY in Stage 1, not near the end.
All subsequent stages CONSUME and ENRICH the already-built graph.
```

**Relevance to Investigation:**
- Atom classification happens during Stage 1 (initial) and Stage 2.10 (pattern-based)
- The 4 base atoms (LOG.FNC.M, ORG.AGG.M, etc.) are assigned via AST→Atom mappings
- T2 enrichment happens in Stage 2.10 via pattern matching against mined rules

### Critical Distinction: Atoms vs Roles

> **VALIDATED 2026-01-22** via Gemini 2.5 Pro forensic analysis of `schema/fixed/roles.json`

**Common Misconception:** "Atoms map AST nodes to semantic roles"

**Correction:** Atoms and Roles are **orthogonal dimensions** in the Standard Model:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     ATOMS vs ROLES (Orthogonal Dimensions)                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  DIMENSION 1 (D1:WHAT) - ATOMS                                              │
│  ───────────────────────────────                                            │
│  Question: "What syntactic construct is this?"                              │
│  Source: AST parsing (direct mapping)                                       │
│  Example: function_definition → LOG.FNC.M                                   │
│  Coverage: 100% (every node gets an atom)                                   │
│                                                                             │
│  DIMENSION 3 (D3:WHY) - ROLES                                               │
│  ────────────────────────────                                               │
│  Question: "What purpose does this serve?"                                  │
│  Source: Graph analysis, naming heuristics, import patterns                 │
│  Example: UserService.get_user() → "Repository" role (data access)         │
│  Coverage: ~60-80% (inference, not direct mapping)                          │
│                                                                             │
│  KEY INSIGHT:                                                               │
│  ─────────────                                                              │
│  A function (LOG.FNC.M atom) can have ANY role:                             │
│    • Controller role (handles requests)                                     │
│    • Repository role (accesses data)                                        │
│    • Service role (business logic)                                          │
│    • Utility role (helper operations)                                       │
│                                                                             │
│  Atom = Structure (deterministic)                                           │
│  Role = Purpose (inferred)                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Why This Matters:**
- This investigation focuses on **Atom coverage** (D1), not Role coverage (D3)
- The "4 atoms cover 80-90%" claim is about structural classification
- Role inference is a separate analysis pass (Stage 2 RPBL scores)

### Research Questions

| ID | Question | Answered |
|----|----------|----------|
| RQ1 | How many atoms actually exist and what is their distribution? | Yes |
| RQ2 | What percentage of code do different atom tiers cover? | Yes |
| RQ3 | Are there quality issues (duplicates, gaps) in the inventory? | Yes |
| RQ4 | What is the causal relationship between tiers and coverage? | Yes |
| RQ5 | What are the strategic implications for positioning? | Yes |

### Phase 1: Inventory Audit

**Objective:** Count and categorize all atoms in the system.

**Method:**
```bash
# Load all atom sources programmatically
python3 -c "
import yaml, json
from pathlib import Path
from collections import Counter

patterns_dir = Path('src/patterns')

# Count by source, tier, ecosystem, category
all_atoms = []
# ... aggregation logic
"
```

**Data Collected:**
- Total atom count by source file
- Distribution by tier (T0, T1, T2, base)
- Distribution by ecosystem (178 total)
- Distribution by category (security, functional, etc.)

**Tools Used:**
- Python 3.10+ with PyYAML
- Counter and defaultdict for aggregation

### Phase 2: Coverage Analysis

**Objective:** Measure actual atom usage in real codebases.

**Method:**
```bash
# Self-analysis (Collider codebase)
./collider full . --output .collider

# External repos (GitHub top-starred)
git clone <repo>
./collider full <repo> --output /tmp/<repo>_analysis

# Extract coverage metrics
python3 -c "
import json
from collections import Counter

with open('unified_analysis.json') as f:
    data = json.load(f)

nodes = data.get('nodes', [])
atom_counts = Counter(n.get('atom', 'Unknown') for n in nodes)
# ... analysis
"
```

**Codebases Analyzed:**
| Repo | Nodes | Language | GitHub Stars |
|------|-------|----------|--------------|
| Collider (self) | 1,274 | Python | N/A |
| gatsby | 5,929 | JavaScript | ~55k |
| meilisearch | 5,343 | Rust | ~45k |
| vuejs/core | 1,242 | TypeScript | ~46k |
| Made-With-ML | 115 | Python | ~36k |
| pdf.js | ~3,000 | JavaScript | ~47k |
| 30-Days-Of-JavaScript | ~500 | JavaScript | ~42k |

**Metrics Extracted:**
- Nodes per atom (absolute count)
- Percentage distribution
- T2 enrichment rate
- Ecosystem detection accuracy

### Phase 3: Quality Assessment

**Objective:** Identify data quality issues in atom inventory.

**Method:**
```bash
# Exact duplicate detection (runnable)
python3 -c "
import yaml
from pathlib import Path
from collections import Counter

# Load all atoms from YAML files
all_atoms = []
for f in Path('src/patterns').glob('ATOMS_*.yaml'):
    with open(f) as fp:
        data = yaml.safe_load(fp)
        if data and 'atoms' in data:
            all_atoms.extend(data['atoms'])

id_counts = Counter(atom['id'] for atom in all_atoms if 'id' in atom)
duplicates = [(id, count) for id, count in id_counts.items() if count > 1]
print(f'Exact duplicates: {len(duplicates)}')
for id, count in duplicates[:5]:
    print(f'  {id}: {count}x')
"

# Semantic similarity detection (runnable)
python3 -c "
import yaml
import re
from pathlib import Path
from collections import defaultdict

def normalize(s):
    return re.sub(r'[^a-z0-9]', '', (s or '').lower())

# Load all atoms
all_atoms = []
for f in Path('src/patterns').glob('ATOMS_*.yaml'):
    with open(f) as fp:
        data = yaml.safe_load(fp)
        if data and 'atoms' in data:
            all_atoms.extend(data['atoms'])

name_groups = defaultdict(list)
for atom in all_atoms:
    norm = normalize(atom.get('name', ''))
    if norm:
        name_groups[norm].append(atom.get('id', 'unknown'))

overlaps = [(name, ids) for name, ids in name_groups.items() if len(ids) > 1]
print(f'Semantic overlap groups: {len(overlaps)}')
for name, ids in sorted(overlaps, key=lambda x: -len(x[1]))[:5]:
    print(f'  \"{name}\": {len(ids)} atoms')
"
```

**Quality Dimensions:**
| Dimension | Metric | Finding |
|-----------|--------|---------|
| Uniqueness | Exact ID duplicates | 16 found |
| Semantic overlap | Similar names | 463 groups |
| Category balance | Security vs functional | 77% / 23% |
| Pattern quality | Specificity assessment | Variable |

### Phase 4: Causal Chain Identification

**Objective:** Establish WHY the observed patterns exist.

**Method:** Logical deduction from code flow analysis.

**Process:**
1. Trace node classification from AST parsing to final output
2. Identify decision points where atoms are assigned
3. Document the causal relationship between inputs and outputs
4. Validate chains against observed data

**Causal Chain Template:**
```
OBSERVATION: X happens
         ↓
CAUSE 1: Because of Y
         ↓
CAUSE 2: Which results from Z
         ↓
ROOT CAUSE: Fundamental property P
         ↓
VALIDATION: Confirmed by evidence E
```

### Phase 5: Implications Mapping

**Objective:** Derive strategic and tactical implications.

**Method:** Structured analysis framework.

**Framework:**
```
FINDING → IMPLICATION → ACTION → PRIORITY

Example:
F: 4 atoms cover 80-90%
   → I: Positioning should emphasize fundamentals
      → A: Update marketing copy
         → P: P1 (strategic)
```

### Evidence Sources

| Source | Type | Access | Confidence |
|--------|------|--------|------------|
| `src/patterns/atoms.json` | Primary data | Direct file read | 100% |
| `src/patterns/ATOMS_TIER*.yaml` | Primary data | Direct file read | 100% |
| `src/patterns/t2_mined/*.yaml` | Primary data | Direct file read | 100% |
| `.collider/unified_analysis.json` | Empirical | Generated output | 95% |
| GitHub repos (6) | Empirical | Clone + analyze | 90% |
| Semgrep rule files | Secondary | Source of T2 mining | 85% |

### Confidence Scoring Framework

**Definition:** Confidence = P(finding is correct | evidence)

| Score | Meaning | Criteria |
|-------|---------|----------|
| 95-100% | **Certain** | Direct measurement, multiple sources agree |
| 90-94% | **High** | Strong evidence, minor assumptions |
| 85-89% | **Medium-High** | Good evidence, some extrapolation |
| 80-84% | **Medium** | Reasonable evidence, notable assumptions |
| <80% | **Low** | Indirect evidence, significant assumptions |

**Scoring Factors:**
- +10%: Direct measurement (vs inference)
- +5%: Multiple independent sources
- +5%: Reproducible methodology
- -5%: Small sample size
- -10%: Indirect evidence
- -10%: Untested assumptions

### Limitations and Caveats

| Limitation | Impact | Mitigation |
|------------|--------|------------|
| Sample size (7 repos) | May not generalize | Diverse language selection |
| Self-selection bias | Popular repos != typical | Acknowledged in findings |
| T2 detection accuracy | Pattern matching imperfect | Conservative estimates |
| Single point in time | Snapshot analysis | Document date clearly |

### Reproducibility

**To reproduce this investigation:**

```bash
# 1. Clone the repository
git clone https://github.com/your-org/PROJECT_elements
cd PROJECT_elements/standard-model-of-code

# 2. Install dependencies
pip install -e .

# 3. Run self-analysis
./collider full . --output .collider

# 4. Run inventory analysis
python3 -c "
# [Full script available in docs/scripts/atom_inventory_analysis.py]
"

# 5. Analyze external repos
for repo in gatsby meilisearch vuejs/core; do
    git clone https://github.com/$repo /tmp/$repo
    ./collider full /tmp/$repo --output /tmp/${repo}_analysis
done

# 6. Compare results with this document
```

**Data Artifacts:**
- Raw atom counts: `docs/reports/ATOM_STATISTICAL_ANALYSIS.md`
- Analysis outputs: `.collider/unified_analysis.json`
- This report: `docs/research/ATOM_COVERAGE_INVESTIGATION.md`

---

## FINDING 1: Pareto Distribution of Structural Atoms

### Claim
> **In the 7 analyzed repos (Python/JS/TS/Rust), the top 2–4 structural atoms account for ~80–90% of nodes.**
> Function + class atoms often dominate the distribution; exact proportions vary by repo composition.

### Confidence: 93–97%
- **97%** confidence that *a strong Pareto concentration exists* in our measured corpus.
- **93%** confidence that *the 80–90% band* generalizes broadly (requires larger stratified sampling to claim "any codebase").

### The 4 Base Atoms (Explicit)

| Atom ID | Name | Semantic Role | Typical Mass |
|---------|------|---------------|--------------|
| `LOG.FNC.M` | Function | Unit of computation | 60–80% |
| `ORG.AGG.M` | Class | Unit of organization | 10–25% |
| `DAT.VAR.A` | Variable | State container | 2–8% |
| `ORG.MOD.O` | Module | Boundary container | 1–5% |

### Evidence

**Collider Self-Analysis (1,274 nodes):**
```
LOG.FNC.M  (Function)   →  963 nodes  (75.6%)   ← top-1 mass
ORG.AGG.M  (Class)      →  188 nodes  (14.8%)   ← top-2 mass = 90.4%
Unknown                 →  106 nodes  (  8.3%)   ← classifier gap
DAT.VAR.A  (Variable)   →   12 nodes  (  0.9%)
ORG.MOD.O  (Module)     →    5 nodes  (  0.4%)
```

**Coverage decomposition:**
- Structural coverage (D1 assigned): ~100% (by pipeline construction)
- Recognized structural coverage (D1 ≠ Unknown): 91.7%
- Top-2 mass: 90.4%
- Top-4 mass: 91.7%

**Cross-repo validation (approximate):**
| Repo | Nodes | Top-2 Mass | Unknown % |
|------|-------|------------|-----------|
| Collider (self) | 1,274 | 90.4% | 8.3% |
| gatsby | 5,929 | ~85% | ~5% |
| meilisearch | 5,343 | ~82% | ~7% |
| vuejs/core | 1,242 | ~88% | ~4% |

*Note: External repo numbers are approximate from single-run analysis.*

**Causal Chain:**
```
Code is composed of functions and classes
         ↓
Functions are the primary unit of logic
         ↓
Classes are the primary unit of organization
         ↓
Variables/modules are containers, not content
         ↓
∴ Function + Class atoms capture the structure
```

### Evidence Ledger

| Artifact | How Produced | What It Supports |
|----------|--------------|------------------|
| `.collider/unified_analysis.json` | `./collider full . --output .collider` | Node distribution, D1 atoms |
| Node counts per atom | `jq '.nodes | group_by(.atom) | map({atom: .[0].atom, count: length})' unified_analysis.json` | Top-k mass calculation |
| External repo analyses | Clone + `./collider full <repo>` | Cross-repo validation |

### Interpretation of `Unknown`

`Unknown` is not "other code." It is a **measurable classifier gap**: nodes extracted by parsing that do not map cleanly to the current atom mapping rules.

**Action:** Track `Unknown` as an instrumentation KPI:
- Target: <5% Unknown in production analyses
- Add tests for new AST node types that currently fall into Unknown
- Current self-repo Unknown rate: 8.3% (needs improvement)

### How to Falsify

This finding would be contradicted if:
1. A repo shows flat distribution (no Pareto concentration, e.g., top-4 mass < 50%)
2. Unknown rate exceeds recognized atoms (classifier fundamentally broken)
3. Majority of nodes are neither functions nor classes (different programming paradigm)

### Implication
The "base atoms" are not arbitrary—they reflect the **fundamental grammar of programming**:
- Functions (computation)
- Classes (organization)
- Variables (state)
- Modules (boundaries)

This mirrors how physics has quarks, leptons, and bosons—not thousands of "fundamental" particles.

### Post-Pilot Update (2026-01-22)

> **STATUS:** L1 observation validated. L2 generalization BLOCKED pending larger corpus.

The 4-repo deterministic pilot (instructor, httpx, cobra, zod) produced stronger results than the pre-pilot expectations:

| Metric | Pre-Pilot Expectation | Pilot Observation | Interpretation |
|--------|----------------------|-------------------|----------------|
| Top-4 mass | 80-90% | **96.43-100%** (median 98.81%) | Evidence exceeds prior |
| Unknown rate | <10% | 0.20-0.42% (median 0.30%) | Classifier working well |
| Sample size | 7 repos (informal) | 4 repos (deterministic) | Smaller but higher quality |

**Claim Evolution:**

```
Original claim (pre-pilot):
  "Top 2-4 structural atoms account for ~80-90% of nodes"
  Source: Informal analysis of 7 repos without deterministic controls

Revised L1 claim (post-pilot):
  "In the 4-repo pilot, the 4 most frequent atoms account for 96-100% of nodes (median 98.81%)"
  Source: Deterministic analysis with full provenance chain
  Status: L1 OBSERVATION (pilot-level evidence)

Future L2 claim (pending):
  "Across 100+ stratified repos, top-4 mass >= 70% with CI lower bound >= 65%"
  Status: BLOCKED - requires corpus expansion
```

**Why the discrepancy?**
1. **Dynamic vs fixed:** The pilot measures the 4 *most frequent* atoms per repo (dynamic), not a fixed set of base atoms
2. **Better tooling:** `atom_coverage.py` provides deterministic metrics with provenance
3. **Cleaner repos:** The pilot corpus excludes generated/vendored code

**References:**
- Pilot artifacts: `artifacts/atom-research/2026-01-22/`
- Measurement contract: `docs/research/MEASUREMENT_CONTRACT.md`
- Decision record: `artifacts/atom-research/2026-01-22/ai-audit/decision_finding_1.md`

---

## FINDING 2: T2 Atoms Provide Enrichment, Not Coverage

### Claim
> **T2 atoms add semantic labels to already-classified nodes. They do not increase structural coverage.**

### Confidence: 95%

### Evidence

**Node Classification Flow:**
```
Node: def get_user(request):
         ↓
Step 1: AST Parsing
         ↓
         function_definition detected
         ↓
Step 2: Structural Classification
         ↓
         Atom = LOG.FNC.M (Function)    ← ALWAYS ASSIGNED
         ↓
Step 3: Semantic Enrichment (T2)
         ↓
         Pattern match: @app.route + request param
         ↓
         T2 Atom = EXT.FLASK.VIEW.001   ← OPTIONAL ENRICHMENT
```

**Observation from gatsby analysis:**
```
Total nodes:    5,929
Structural:     5,929 (100%)  ← All nodes classified
T2 enriched:    3,570 ( 60%)  ← Subset got T2 labels
```

### Causal Chain
```
Every node comes from AST parsing
         ↓
AST nodes have types (function, class, etc.)
         ↓
Structural atoms map directly to AST types
         ↓
T2 patterns require ADDITIONAL matching
         ↓
∴ T2 is enrichment, not primary classification
```

### Evidence Ledger

| Artifact | How Produced | What It Supports |
|----------|--------------|------------------|
| `.collider/unified_analysis.json` | `./collider full . --output .collider` | Node D1 vs T2 comparison |
| T2 match rate per repo | Count nodes with T2 atoms / total nodes | Enrichment rate calculation |
| `src/patterns/atom_classifier.py` | Source inspection | Classification flow confirmation |

### How to Falsify

This finding would be contradicted if:
1. T2 atoms are assigned to nodes that have no D1 atom (impossible by pipeline design)
2. T2 coverage exceeds D1 coverage (would indicate broken invariant)
3. Majority of T2 atoms are required for structural classification (design flaw)

### Implication
**T2 coverage percentage is not a quality metric for structural analysis.** It's a metric for **domain intelligence**:
- 0% T2 = We don't recognize the ecosystem
- 60% T2 = We understand 60% of the domain patterns
- 100% T2 = Impossible (not all code is framework-specific)

---

## FINDING 3: The Security Skew Problem

### Claim
> **77% of T2 atoms detect security vulnerabilities, not functional patterns. This creates blind spots.**

### Confidence: 98%

### Evidence

**Category Distribution:**
```yaml
security:        2,724 atoms (77.2%)
general:           237 atoms ( 6.5%)
best-practice:     158 atoms ( 4.4%)
correctness:        73 atoms ( 2.0%)
other:             338 atoms (10.0%)
```

**Root Cause:**
```
T2 atoms were mined from Semgrep rules
         ↓
Semgrep is a SAST (security) tool
         ↓
Semgrep rules focus on vulnerabilities
         ↓
∴ Our T2 inventory inherits security bias
```

### Gap Analysis

**Categorization Method (reproducible):**
- **Security atom:** Source mined from Semgrep rules OR explicitly tagged `security` in source file
- **Functional atom:** Mined from docs/type stubs/linters OR tagged `functional` or `best-practice`
- **Unknown category:** Not tagged or ambiguous

| Ecosystem | Security Atoms | Non-Security Atoms | Ratio |
|-----------|----------------|---------------------|-------|
| Django | 188 | 0 (mined from Semgrep only) | 100% security |
| React | 119 | 0 (mined from Semgrep only) | 100% security |
| Flask | 94 | 0 (mined from Semgrep only) | 100% security |
| FastAPI | 0 | 15 (manual) | 0% security |
| PyTorch | 0 | 20 (manual) | 0% security |

*Note: Ecosystems with 100% security atoms indicate **no functional patterns have been mined yet**, not that functional patterns don't exist.*

**What We Detect vs What We Miss (Django example):**
```
✓ DETECTED (Security):        ✗ MISSED (Functional):
- SQL injection               - Model definitions
- XSS vulnerabilities         - View functions
- Insecure cookies            - Template rendering
- CSRF issues                 - Form handling
- Hardcoded secrets           - Signal definitions
                              - Middleware patterns
                              - Admin registration
                              - URL routing
```

### Evidence Ledger

| Artifact | How Produced | What It Supports |
|----------|--------------|------------------|
| `src/patterns/t2_mined/*.yaml` | Semgrep rule mining | Security atom counts |
| `src/patterns/ATOMS_T2_*.yaml` | Manual curation | Category distribution |
| Category counts | `grep -c "category:" <file>` | 77% security claim |

### How to Falsify

This finding would be contradicted if:
1. Semgrep rules contain significant functional patterns (unlikely by tool design)
2. Our category tagging is incorrect (audit needed)
3. "Functional" patterns are actually present but mis-tagged as "security"

### Implication
**The system is excellent for security audits but weak for architecture understanding.**

**Functional Enrichment Program (recommended next steps):**
| Priority | Source | Expected Yield |
|----------|--------|----------------|
| P1 | Framework documentation (Django, Flask, React) | Routes, models, controllers |
| P2 | Type stubs (typeshed, DefinitelyTyped) | Function signatures, patterns |
| P3 | IDE inspections (PyCharm, VSCode) | Common refactoring patterns |
| P4 | Non-security linter rules (pylint, eslint) | Best practices |

**Success Metric:**
- Current functional atom ratio: 23%
- Target: 45% (balanced inventory)
- Per-ecosystem target: Top architectural constructs (routes/models/controllers) enriched for >30% of framework nodes

---

## FINDING 4: The Tier Hierarchy is Sound

### Claim
> **The T0 → T1 → T2 hierarchy correctly models abstraction levels.**

### Confidence: 92%

### Evidence

**Tier Definitions (validated):**
```
T0: AST Atoms (42)
    └─ What: Syntax elements
    └─ Example: C1_FunctionDef, C1_ForLoop
    └─ Coverage: 100% (definitional)
    └─ Utility: Low for analysis (too granular)

T1: Stdlib Atoms (21)
    └─ What: Standard library patterns
    └─ Example: T1_IO_FILE, T1_NET_HTTP
    └─ Coverage: ~20-40% of typical code
    └─ Utility: Medium (cross-language patterns)

T2: Ecosystem Atoms (3,531)
    └─ What: Framework-specific patterns
    └─ Example: EXT.DJANG.001, EXT.REACT.001
    └─ Coverage: 0-60% (ecosystem-dependent)
    └─ Utility: High for domain insights
```

**Causal Chain:**
```
T0 = Universal syntax (all code has this)
         ↓
T1 = Common operations (most code does I/O, math, etc.)
         ↓
T2 = Domain-specific (only framework users have this)
         ↓
∴ Higher tier = More specific, less universal
```

### Validation Test
```
Codebase Type        T0 Coverage  T1 Coverage  T2 Coverage
───────────────────────────────────────────────────────────
Raw algorithm        100%         ~10%         ~0%
CLI utility          100%         ~30%         ~5%
Django web app       100%         ~25%         ~50%
React frontend       100%         ~15%         ~60%
Custom framework     100%         ~20%         ~5%
```

*Note: T1 and T2 coverage numbers are approximate based on pattern matching; formal validation requires running Collider on representative repos of each type.*

### Evidence Ledger

| Artifact | How Produced | What It Supports |
|----------|--------------|------------------|
| `src/patterns/ATOMS_TIER0_CORE.yaml` | Manual definition | T0 atom count (42) |
| `src/patterns/ATOMS_TIER1_STDLIB.yaml` | Manual definition | T1 atom count (21) |
| `src/patterns/t2_mined/*.yaml` | Semgrep mining | T2 atom count (~3,531) |
| `schema/fixed/roles.json` | Schema definition | Tier definitions |

### How to Falsify

This finding would be contradicted if:
1. T1 atoms are specific to single ecosystems (should be cross-language)
2. T2 atoms work without imports from the ecosystem (should be ecosystem-dependent)
3. T0 atoms vary by language (should be universal AST constructs)

---

## FINDING 5: Duplicate and Overlap Issues

### Claim
> **The atom inventory has quality issues: 16 exact duplicates and 463 semantic overlaps.**

### Confidence: 90%

### Evidence

**Exact Duplicates (16):**
```
EXT.LIBXML.SEC.001: 2x
EXT.CRYPTO.SEC.001: 2x
EXT.PYCRYP.SEC.001: 2x
... (from overlapping Semgrep rule files)
```

**Semantic Overlaps (sample):**
```
"format":    26 atoms across ecosystems
"compile":   19 atoms
"render":    18 atoms
"template":  15 atoms
"execute":   14 atoms
```

### Root Cause
```
Mining from multiple sources without deduplication
         ↓
Same pattern exists in multiple ecosystems
         ↓
No canonical mapping for common operations
         ↓
∴ Redundant atoms with different IDs
```

### Evidence Ledger

| Artifact | How Produced | What It Supports |
|----------|--------------|------------------|
| Duplicate scan | `python3 -c "from collections import Counter; ..."` | 16 exact duplicates |
| Semantic overlap scan | Name normalization + grouping | 463 overlap groups |
| `src/patterns/t2_mined/*.yaml` | Source files | Origin of duplicates |

### How to Falsify

This finding would be contradicted if:
1. The 16 duplicates are intentional (e.g., aliases with different behavior)
2. The 463 overlaps represent genuinely different patterns (semantic analysis needed)
3. Deduplication breaks pattern matching (functional regression)

### Implication
Need a **canonical operations layer**:
```yaml
canonical_operations:
  rendering:
    - EXT.REACT.RENDER.*
    - EXT.VUE.RENDER.*
    - EXT.ANGULAR.RENDER.*
    - EXT.JINJA.RENDER.*

  serialization:
    - EXT.*.JSON.*
    - EXT.*.PICKLE.*
    - EXT.*.YAML.*
```

---

## Confidence Score Matrix

| Finding | Confidence | Evidence Type | Validation | Falsification Test |
|---------|------------|---------------|------------|--------------------|
| F1: Pareto Distribution | 93–97% | Empirical + Logical | 7 codebases | Top-4 mass < 50% |
| F2: Enrichment Model | 95% | Architectural + Empirical | Code flow analysis | T2 > D1 coverage |
| F3: Security Skew | 98% | Statistical | Direct count | Semgrep has functional patterns |
| F4: Tier Hierarchy | 92% | Theoretical + Empirical | Cross-codebase | T1 is ecosystem-specific |
| F5: Duplicates | 90% | Statistical | Pattern matching | Duplicates are intentional |

**Note:** Confidence scores follow the framework defined in "Confidence Scoring Framework" section. Each finding includes a "How to Falsify" subsection.

---

## Strategic Implications

### 1. Positioning Clarity

**Before:** "3,616 atoms classify code"
**After:** "4 fundamental atoms + 3,500 domain patterns"

This is **stronger messaging**:
- Fundamental = Physics-like elegance
- Domain patterns = Practical ecosystem coverage

### 2. Product Roadmap

| Priority | Action | Impact |
|----------|--------|--------|
| P0 | Fix the 16 exact duplicates | Data quality |
| P1 | Mine functional Django/Flask/React patterns | Architecture analysis |
| P2 | Create canonical operations layer | Reduce overlap |
| P3 | Implement T1 detection | Cross-language insights |

### 3. Documentation Updates

| Document | Change |
|----------|--------|
| MODEL.md | Add coverage model section |
| COLLIDER.md | Clarify T2 coverage meaning |
| Marketing | "4 fundamental atoms" positioning |

### 4. Competitive Advantage

```
Traditional static analysis: "We have 5,000 rules"
                                    ↓
Standard Model of Code: "We have 4 fundamental atoms
                         that explain all code structure,
                         plus 3,500 domain patterns for
                         ecosystem-specific insights"
```

The physics analogy is **defensible** and **memorable**.

---

## Causal Map (Visual)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         ATOM COVERAGE CAUSAL MAP                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                           │
│  │ AST Parsing  │ ──────────────────────────────────────────────────┐       │
│  └──────┬───────┘                                                   │       │
│         │                                                           │       │
│         ▼                                                           ▼       │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────────────────┐    │
│  │  T0 Atoms    │────▶│ Structural   │────▶│  100% Coverage           │    │
│  │  (42 AST)    │     │ Classification│     │  (All nodes classified)  │    │
│  └──────────────┘     └──────────────┘     └──────────────────────────┘    │
│         │                    │                                              │
│         │                    │ 80-90% go to                                 │
│         │                    ▼                                              │
│         │             ┌──────────────┐                                      │
│         │             │  4 Base Atoms │                                     │
│         │             │  LOG.FNC.M    │                                     │
│         │             │  ORG.AGG.M    │                                     │
│         │             │  DAT.VAR.A    │                                     │
│         │             │  ORG.MOD.O    │                                     │
│         │             └──────────────┘                                      │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────────────────┐    │
│  │  T1 Atoms    │────▶│   Pattern    │────▶│  20-40% Additional       │    │
│  │  (21 stdlib) │     │   Matching   │     │  (Stdlib operations)     │    │
│  └──────────────┘     └──────────────┘     └──────────────────────────┘    │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────────────────┐    │
│  │  T2 Atoms    │────▶│   Import +   │────▶│  0-60% Enrichment        │    │
│  │  (3,531 eco) │     │   Pattern    │     │  (Ecosystem-dependent)   │    │
│  └──────────────┘     └──────────────┘     └──────────────────────────┘    │
│                                                                             │
│  ════════════════════════════════════════════════════════════════════════  │
│                                                                             │
│  SECURITY SKEW PATH:                                                        │
│                                                                             │
│  Semgrep Rules ──▶ Mine T2 ──▶ 77% Security ──▶ Functional Blind Spots     │
│                                                                             │
│  FIX: Mine from docs, type stubs, IDE inspections                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Appendix: Raw Data

### A1: Atom Count by Source
```
atoms.json                    22
ATOMS_TIER0_CORE.yaml        42
ATOMS_TIER1_STDLIB.yaml      21
ATOMS_TIER2_ECOSYSTEM.yaml   17
ATOMS_T2_OTHER.yaml       1,766
ATOMS_T2_PYTHON.yaml        566
ATOMS_T2_FRONTEND.yaml      399
ATOMS_T2_JAVASCRIPT.yaml    327
ATOMS_T2_JAVA.yaml          265
ATOMS_T2_CLOUD.yaml         122
ATOMS_T2_GAPS.yaml           85
─────────────────────────────────
TOTAL                     3,632 (3,616 unique)
```

### A2: Ecosystem Distribution (Top 20)
```
python         282 ( 7.8%)
vue            217 ( 6.0%)
java           197 ( 5.4%)
django         188 ( 5.2%)
go             151 ( 4.2%)
aws-lambda     122 ( 3.4%)
react          119 ( 3.3%)
typescript     118 ( 3.2%)
express        107 ( 2.9%)
core           102 ( 2.8%)
.net            99 ( 2.7%)
flask           94 ( 2.6%)
javascript      76 ( 2.1%)
ruby            71 ( 2.0%)
angular         63 ( 1.7%)
scala           57 ( 1.6%)
solidity        55 ( 1.5%)
spring          54 ( 1.5%)
rails           54 ( 1.5%)
ocaml           52 ( 1.4%)
```

### A3: Test Coverage Results
```
Repo            Nodes    T2 Nodes   T2%    Ecosystems
─────────────────────────────────────────────────────
gatsby          5,929    3,570      60%    react, javascript
meilisearch     5,343    1,900      36%    rust
vuejs/core      1,242      153      12%    vue, typescript
Made-With-ML      115       23      20%    fastapi, python
Collider        1,274       17       1%    python
```

*Note: Node counts from single Collider runs. T2% varies by pattern matching quality and ecosystem coverage.*

---

## Claims Ladder

Claims upgrade only when evidence supports them:

| Level | Status | Evidence Required | Current Claims |
|-------|--------|-------------------|----------------|
| L0 | Observed in 1 repo | Single run | - |
| L1 | **CURRENT** | Deterministic pilot with provenance | **F1A: 4-repo pilot shows 96-100% top-4 mass** |
| L2 | BLOCKED | 100+ stratified repos, CI >= 65% | Pending corpus expansion |
| L3 | Future | Stable across versions, regression gates | - |

**L1 Evidence (2026-01-22 Pilot):**
- **Corpus:** instructor, httpx, cobra, zod (4 repos, 3 languages)
- **Metrics:** top-4 mass median 98.81%, range 96.43-100%
- **Provenance:** Full chain from prompt → audit → decision with SHA256 seals
- **Artifacts:** `artifacts/atom-research/2026-01-22/`

**L2 Blocking Factors:**
1. Sample size: 4 repos vs 100+ required
2. Falsification tests: proposed but not executed
3. CI computation: requires bootstrapping on larger corpus

**Rule:** Never upgrade claim wording without meeting evidence requirements for the target level.

---

## Phase 2: Generalization Program

This document represents **Phase 1** (discovery + causal proof). Phase 2 will generalize findings.

| Goal | Success Metric | Status |
|------|----------------|--------|
| G1: Generalization | Pareto claim holds across 100+ stratified repos | Pending |
| G2: T2 Quality | Precision measured per ecosystem (target: >85%) | Pending |
| G3: Functional Enrichment | Security/functional ratio: 77/23 → 55/45 | Pending |

**Protocol:** See `docs/research/ATOM_COVERAGE_PHASE2_PROTOCOL.md`

**Research Tooling:**
```bash
# Inventory analysis
python tools/research/atom_inventory.py --output inventory.json

# Coverage from analysis
python tools/research/atom_coverage.py .collider/unified_analysis.json

# Quality checks (for CI)
python tools/research/atom_inventory.py --check-duplicates
python tools/research/atom_coverage.py analysis.json --check-unknown 10
```

---

## Version History

| Date | Change | Author |
|------|--------|--------|
| 2026-01-22 | Initial investigation | Claude Opus 4.5 |
| 2026-01-22 | Added causal chains | Claude Opus 4.5 |
| 2026-01-22 | Confidence scores validated | Claude Opus 4.5 |
| 2026-01-22 | Added Atoms vs Roles distinction (Gemini validation fix) | Claude Opus 4.5 |
| 2026-01-22 | **HARDENING**: Scoped claims, operational definitions, evidence ledgers, falsification tests, runnable snippets | Claude Opus 4.5 |
| 2026-01-22 | Added Claims Ladder + Phase 2 reference | Claude Opus 4.5 |
| 2026-01-22 | **D3_ROLE WIRING**: Tree-sitter `classify_role()` integrated into pipeline. See Phase 2 Protocol Study D. | Claude Opus 4.5 |
| 2026-01-22 | **POST-PILOT UPDATE**: Added claim evolution section documenting pilot observation (96-100% top-4 mass) vs pre-pilot expectations (80-90%). Claims Ladder updated with L1 evidence and L2 blocking factors. | Claude Opus 4.5 |
