{
  "_meta": {
    "saved_at": "2026-01-26T02:13:05.623530",
    "source": "perplexity-mcp",
    "model": "sonar-deep-research",
    "query": "Best practices for designing information interfaces FOR LLMs (not human UX, but LLM UX). How should context, stats, navigation hints, and actionable suggestions be structured and presented TO a large language model? Research: 1) Optimal context formatting for LLM consumption, 2) How to present structured data to LLMs (JSON vs YAML vs markdown vs XML), 3) Attention patterns - what format gets best comprehension, 4) 'Narrator' patterns - systems that feed context to AI agents. Include research from prompt engineering, AI agent frameworks, MCP (Model Context Protocol), and any studies on LLM information processing preferences 2024-2026.",
    "query_length": 641,
    "checksum": "sha256:6880f72d29fcdeb85706422718cdb60c87de5acc41536b3cdc0e161424843675"
  },
  "response": {
    "id": "fa281bb3-ea59-47a0-a57f-0625ee43da96",
    "model": "sonar-deep-research",
    "created": 1769404385,
    "usage": {
      "prompt_tokens": 139,
      "completion_tokens": 7674,
      "total_tokens": 7813,
      "citation_tokens": 58090,
      "num_search_queries": 30,
      "reasoning_tokens": 341423,
      "cost": {
        "input_tokens_cost": 0.0,
        "output_tokens_cost": 0.061,
        "citation_tokens_cost": 0.116,
        "reasoning_tokens_cost": 1.024,
        "search_queries_cost": 0.15,
        "total_cost": 1.352
      }
    },
    "citations": [
      "https://agenta.ai/blog/top-6-techniques-to-manage-context-length-in-llms",
      "https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api",
      "https://www.geekytech.co.uk/why-llms-need-structured-content/",
      "https://arxiv.org/html/2508.13666v1",
      "https://www.promptingguide.ai",
      "https://www.microsoft.com/en-us/research/blog/improving-llm-understanding-of-structured-data-and-exploring-advanced-prompting-methods/",
      "https://www.improvingagents.com/blog/best-nested-data-format/",
      "https://www.anthropic.com/news/model-context-protocol",
      "https://www.firecrawl.dev/blog/context-layer-for-ai-agents",
      "https://news.ycombinator.com/item?id=45458455",
      "https://modelcontextprotocol.io",
      "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "https://arxiv.org/html/2510.02091v1",
      "https://portkey.ai/blog/optimize-token-efficiency-in-prompts",
      "https://userjot.com/blog/best-practices-building-agentic-ai-systems",
      "https://magazine.sebastianraschka.com/p/state-of-llms-2025",
      "https://developer.ibm.com/articles/awb-token-optimization-backbone-of-effective-prompt-engineering/",
      "https://www.reltio.com/resources/blog/agentic-ai-best-practices/",
      "https://www.digitalocean.com/community/tutorials/prompt-caching-explained",
      "https://arxiv.org/html/2511.13900v1",
      "https://www.ibm.com/think/topics/agentic-chunking",
      "https://www.digitalapplied.com/blog/prompt-engineering-advanced-techniques-2026",
      "https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf",
      "https://www.pinecone.io/learn/chunking-strategies/",
      "https://dl.acm.org/doi/10.1145/3786554.3786576",
      "https://platform.openai.com/docs/guides/structured-outputs",
      "https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide",
      "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00751/131277/Navigating-the-Landscape-of-Hint-Generation",
      "https://docs.langchain.com/oss/python/langchain/structured-output",
      "https://www.salesforce.com/blog/agentforce-prompt-builder/",
      "https://www.parallelhq.com/blog/future-of-design-with-ai",
      "https://www.promptingguide.ai/techniques/fewshot",
      "https://microsoft.github.io/ai-agents-for-beginners/04-tool-use/",
      "https://www.nngroup.com/articles/ai-design-tools-update-2/",
      "https://cleanlab.ai/blog/learn/reliable-fewshot-prompts/",
      "https://martinfowler.com/articles/function-call-LLM.html",
      "https://aws.amazon.com/blogs/machine-learning/read-graphs-diagrams-tables-and-scanned-pages-using-multimodal-prompts-in-amazon-bedrock/",
      "https://arxiv.org/html/2501.11223v1",
      "https://saharaai.com/blog/writing-ai-system-prompts",
      "https://blog.bytebytego.com/p/multimodal-llms-basics-how-llms-process",
      "https://openreview.net/forum?id=hJ8OQAiTrl",
      "https://tetrate.io/learn/ai/system-prompts-guide",
      "https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning",
      "https://aclanthology.org/2025.emnlp-main.1783.pdf",
      "https://www.datacamp.com/tutorial/contextual-retrieval-anthropic",
      "https://arxiv.org/html/2401.14295v3",
      "https://arxiv.org/html/2508.13666v1",
      "https://www.anthropic.com/news/contextual-retrieval",
      "https://arxiv.org/html/2505.19426v2",
      "https://apxml.com/courses/prompt-engineering-agentic-workflows/chapter-3-prompt-engineering-tool-use/addressing-tool-errors-via-prompts",
      "https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling",
      "https://www.prompthub.us/blog/in-context-learning-guide",
      "https://www.datagrid.com/blog/exception-handling-frameworks-ai-agents",
      "https://arxiv.org/html/2503.23077v2",
      "https://arxiv.org/html/2510.20345v1",
      "https://www.geeksforgeeks.org/artificial-intelligence/role-based-prompting/",
      "https://composio.dev/blog/how-to-build-tools-for-ai-agents-a-field-guide",
      "https://sci-k.github.io/2025/papers/paper14.pdf",
      "https://learnprompting.org/docs/advanced/zero_shot/role_prompting",
      "https://apyhub.com/blog/how-to-design-apis-to-be-found-by-ai-agents"
    ],
    "search_results": [
      {
        "title": "Top techniques to Manage Context Lengths in LLMs - Agenta",
        "url": "https://agenta.ai/blog/top-6-techniques-to-manage-context-length-in-llms",
        "snippet": "Overcome LLM token limits with 6 practical techniques. Learn how you can use truncation, RAG, memory buffering, and compression to overcome the token limit.",
        "source": "web"
      },
      {
        "title": "Best practices for prompt engineering with the OpenAI API",
        "url": "https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api",
        "snippet": "1. Use the latest model ... For best results, we generally recommend using the latest, most capable models. Newer models tend to be easier to prompt engineer.",
        "source": "web"
      },
      {
        "title": "Why LLMs Need Structured Content | Geeky Tech",
        "url": "https://www.geekytech.co.uk/why-llms-need-structured-content/",
        "snippet": "Structured content serves as a roadmap for LLMs, guiding them to identify and extract accurate information. The clear and consistent framework ...",
        "source": "web"
      },
      {
        "title": "How Code Formatting Silently Consumes Your LLM Budget - arXiv",
        "url": "https://arxiv.org/html/2508.13666v1",
        "snippet": "Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction ...",
        "source": "web"
      },
      {
        "title": "Prompt Engineering Guide",
        "url": "https://www.promptingguide.ai",
        "snippet": "Prompt engineering is not just about designing and developing prompts. It encompasses a wide range of skills and techniques that are useful for interacting and ...",
        "source": "web"
      },
      {
        "title": "Improving LLM understanding of structured data and exploring ...",
        "url": "https://www.microsoft.com/en-us/research/blog/improving-llm-understanding-of-structured-data-and-exploring-advanced-prompting-methods/",
        "snippet": "Our study sets a key benchmark in expanding the capabilities of LLMs to better understand structured table data, moving beyond conventional natural language ...",
        "source": "web"
      },
      {
        "title": "Which Nested Data Format Do LLMs Understand Best? JSON vs ...",
        "url": "https://www.improvingagents.com/blog/best-nested-data-format/",
        "snippet": "YAML emerged as the strongest format for two out of the three models we tested, while XML consistently underperformed. If your system feeds ...",
        "source": "web"
      },
      {
        "title": "Introducing the Model Context Protocol - Anthropic",
        "url": "https://www.anthropic.com/news/model-context-protocol",
        "snippet": "The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools.",
        "source": "web"
      },
      {
        "title": "Can You Automate Context Feeding into Your Agents? - Firecrawl",
        "url": "https://www.firecrawl.dev/blog/context-layer-for-ai-agents",
        "snippet": "Prevent context drift by building a dedicated Context Layer for AI agents. Learn why vector databases fall short and how to automate ...",
        "source": "web"
      },
      {
        "title": "Which table format do LLMs understand best? - Hacker News",
        "url": "https://news.ycombinator.com/item?id=45458455",
        "snippet": "We've experimented with different formats for feeding data to LLMs and markdown tables usually work pretty well. JSON is more structured but harder for the ...",
        "source": "web"
      },
      {
        "title": "What is the Model Context Protocol (MCP)? - Model Context Protocol",
        "url": "https://modelcontextprotocol.io",
        "snippet": "MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. Using MCP, AI applications like Claude or ...",
        "source": "web"
      },
      {
        "title": "Effective context engineering for AI agents - Anthropic",
        "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
        "snippet": "Context is a critical but finite resource for AI agents. In this post, we explore strategies for effectively curating and managing the context ...",
        "source": "web"
      },
      {
        "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and ...",
        "url": "https://arxiv.org/html/2510.02091v1",
        "snippet": "Our goal is to understand whether likelihood-based metrics and generation-based metrics reveal consistent or divergent patterns of layer ...",
        "source": "web"
      },
      {
        "title": "How to Optimize Token Efficiency When Prompting - Portkey",
        "url": "https://portkey.ai/blog/optimize-token-efficiency-in-prompts",
        "snippet": "Discover practical techniques to reduce token usage in AI prompts. Learn how to craft concise, efficient prompts that save costs and improve ...",
        "source": "web"
      },
      {
        "title": "Best Practices for Building Agentic AI Systems: What Actually Works ...",
        "url": "https://userjot.com/blog/best-practices-building-agentic-ai-systems",
        "snippet": "Real patterns for building AI agent systems that don't fall apart. Two-tier architectures, stateless design, orchestration strategies, ...",
        "source": "web"
      },
      {
        "title": "The State Of LLMs 2025: Progress, Problems, and Predictions",
        "url": "https://magazine.sebastianraschka.com/p/state-of-llms-2025",
        "snippet": "A 2025 review of large language models, from DeepSeek R1 and RLVR to inference-time scaling, benchmarks, architectures, and predictions for ...",
        "source": "web"
      },
      {
        "title": "Token optimization: The backbone of effective prompt engineering",
        "url": "https://developer.ibm.com/articles/awb-token-optimization-backbone-of-effective-prompt-engineering/",
        "snippet": "Token optimization focuses on minimizing the number of tokens used in a prompt without changing its natural meaning or intent to reduce costs ...",
        "source": "web"
      },
      {
        "title": "Agentic AI Best Practices - Reltio",
        "url": "https://www.reltio.com/resources/blog/agentic-ai-best-practices/",
        "snippet": "Key best practices for successful agentic AI implementation. · 1. Trusted, Flexible Data Foundation · 2. Coordination and Orchestration · 3.",
        "source": "web"
      },
      {
        "title": "Prompt Caching Explained | DigitalOcean",
        "url": "https://www.digitalocean.com/community/tutorials/prompt-caching-explained",
        "snippet": "Learn what prompt caching is, how it works in LLM workflows, and how it improves performance, reduces latency, and lowers inference costs.",
        "source": "web"
      },
      {
        "title": "What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract ...",
        "url": "https://arxiv.org/html/2511.13900v1",
        "snippet": "This phenomenon, known as the \"lost-in-the-middle\" problem, manifests as a distinct primacy bias, where models prioritize information at the ...",
        "source": "web"
      },
      {
        "title": "What Is Agentic Chunking? - IBM",
        "url": "https://www.ibm.com/think/topics/agentic-chunking",
        "snippet": "Agentic chunking is the use of artificial intelligence (AI) to segment lengthy text inputs into smaller, semantically coherent blocks known as chunks.",
        "source": "web"
      },
      {
        "title": "Prompt Engineering: Advanced Techniques for 2026",
        "url": "https://www.digitalapplied.com/blog/prompt-engineering-advanced-techniques-2026",
        "snippet": "Master advanced prompt engineering: chain-of-thought, few-shot learning, system prompts, and agentic patterns for GPT-5, Claude, and Gemini.",
        "source": "web"
      },
      {
        "title": "[PDF] Lost in the Middle: How Language Models Use Long Contexts",
        "url": "https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf",
        "snippet": "While recent language models have the abil- ity to take long contexts as input, relatively little is known about how well they use longer context.",
        "source": "web"
      },
      {
        "title": "Chunking Strategies for LLM Applications - Pinecone",
        "url": "https://www.pinecone.io/learn/chunking-strategies/",
        "snippet": "In the context of building LLM-related applications, chunking is the process of breaking down large text into smaller segments called chunks.",
        "source": "web"
      },
      {
        "title": "Integrating Object Detection and LLMs for Navigation Assistance",
        "url": "https://dl.acm.org/doi/10.1145/3786554.3786576",
        "snippet": "Experiments across multiple LLM configurations demonstrate that coupling structured perception with LLM-based reasoning produces guidance rated ...",
        "source": "web"
      },
      {
        "title": "Structured model outputs | OpenAI API",
        "url": "https://platform.openai.com/docs/guides/structured-outputs",
        "snippet": "Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don't need to worry about ...",
        "source": "web"
      },
      {
        "title": "Prompting guide | ElevenLabs Documentation",
        "url": "https://elevenlabs.io/docs/agents-platform/best-practices/prompting-guide",
        "snippet": "A system prompt is the personality and policy blueprint of your AI agent. In enterprise use, it tends to be elaborate—defining the agent's role, goals, ...",
        "source": "web"
      },
      {
        "title": "Navigating the Landscape of Hint Generation Research",
        "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00751/131277/Navigating-the-Landscape-of-Hint-Generation",
        "snippet": "In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and ...",
        "source": "web"
      },
      {
        "title": "Structured output - Docs by LangChain",
        "url": "https://docs.langchain.com/oss/python/langchain/structured-output",
        "snippet": "Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available. If ...",
        "source": "web"
      },
      {
        "title": "How Agents Can Take Smarter Actions With Prompt Builder",
        "url": "https://www.salesforce.com/blog/agentforce-prompt-builder/",
        "snippet": "Customize standard agent actions: Tailor standard actions like Record Summary and Email Generation by editing instructions and formatting ...",
        "source": "web"
      },
      {
        "title": "Future of Design with AI: Complete Guide (2025) - Parallel HQ",
        "url": "https://www.parallelhq.com/blog/future-of-design-with-ai",
        "snippet": "Instead of fixed layouts, future interfaces will adjust in real time — reorganising navigation, altering visual hierarchy or changing copy to ...",
        "source": "web"
      },
      {
        "title": "Few-Shot Prompting - Prompt Engineering Guide",
        "url": "https://www.promptingguide.ai/techniques/fewshot",
        "snippet": "Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better ...",
        "source": "web"
      },
      {
        "title": "Tool Use Design Pattern - Microsoft Open Source",
        "url": "https://microsoft.github.io/ai-agents-for-beginners/04-tool-use/",
        "snippet": "The Tool Use Design Pattern focuses on giving LLMs the ability to interact with external tools to achieve specific goals. Tools are code that can be executed by ...",
        "source": "web"
      },
      {
        "title": "AI Design Tools Are Marginally Better: Status Update - NN/G",
        "url": "https://www.nngroup.com/articles/ai-design-tools-update-2/",
        "snippet": "In April 2024, AI-powered design tools were not useful to designers. As of May 2025, their usefulness has improved, but we're still nowhere near ...",
        "source": "web"
      },
      {
        "title": "Ensuring Reliable Few-Shot Prompt Selection for LLMs - Cleanlab",
        "url": "https://cleanlab.ai/blog/learn/reliable-fewshot-prompts/",
        "snippet": "Following typical practice, we source the few-shot examples to include in the prompt template from an available dataset of human-labeled request examples.",
        "source": "web"
      },
      {
        "title": "Function calling using LLMs - Martin Fowler",
        "url": "https://martinfowler.com/articles/function-call-LLM.html",
        "snippet": "Function calling is a capability that enables LLMs to go beyond simple text generation by interacting with external tools and real-world applications.",
        "source": "web"
      },
      {
        "title": "Read graphs, diagrams, tables, and scanned pages using ... - AWS",
        "url": "https://aws.amazon.com/blogs/machine-learning/read-graphs-diagrams-tables-and-scanned-pages-using-multimodal-prompts-in-amazon-bedrock/",
        "snippet": "In this post, we demonstrate how to use models on Amazon Bedrock to retrieve information from images, tables, and scanned documents.",
        "source": "web"
      },
      {
        "title": "Reasoning Language Models: A Blueprint - arXiv",
        "url": "https://arxiv.org/html/2501.11223v1",
        "snippet": "We propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works.",
        "source": "web"
      },
      {
        "title": "Guide to Writing System Prompts: The Hidden Force Behind Every ...",
        "url": "https://saharaai.com/blog/writing-ai-system-prompts",
        "snippet": "The system prompt is the foundational rulebook that determines how that request gets interpreted and answered. This hierarchy is essential for ...",
        "source": "web"
      },
      {
        "title": "Multimodal LLMs Basics: How LLMs Process Text, Images, Audio ...",
        "url": "https://blog.bytebytego.com/p/multimodal-llms-basics-how-llms-process",
        "snippet": "The Three-Part Architecture: Building Blocks of Multimodal LLM. The diagram below shows the high-level view of a multimodal LLM works ...",
        "source": "web"
      },
      {
        "title": "Combining Structure and Text: Learning Representations for...",
        "url": "https://openreview.net/forum?id=hJ8OQAiTrl",
        "snippet": "This paper focuses on graph reasoning tasks and combines graph neural networks and language models to model both the structure and text information.",
        "source": "web"
      },
      {
        "title": "System Prompts: Design Patterns and Best Practices - Tetrate",
        "url": "https://tetrate.io/learn/ai/system-prompts-guide",
        "snippet": "System prompts are the foundational instructions that shape how AI language models behave, respond, and interact with users.",
        "source": "web"
      },
      {
        "title": "Graph-Based Prompting and Reasoning with Language Models",
        "url": "https://cameronrwolfe.substack.com/p/graph-based-prompting-and-reasoning",
        "snippet": "Within this overview, we will studying several prompting techniques that model the reasoning process as a graph structure—rather than a chain or ...",
        "source": "web"
      },
      {
        "title": "[PDF] Why Whitespace Matters for Poets and LLMs - ACL Anthology",
        "url": "https://aclanthology.org/2025.emnlp-main.1783.pdf",
        "snippet": "Abstract. Whitespace is a critical component of poetic form, reflecting both adherence to standard- ized forms and rebellion against those ...",
        "source": "web"
      },
      {
        "title": "Anthropic's Contextual Retrieval: A Guide With Implementation",
        "url": "https://www.datacamp.com/tutorial/contextual-retrieval-anthropic",
        "snippet": "Keeping the right context in place for each chunk reduces retrieval errors by up to 67%, leading to much better performance in downstream tasks.",
        "source": "web"
      },
      {
        "title": "Demystifying Chains, Trees, and Graphs of Thoughts - arXiv",
        "url": "https://arxiv.org/html/2401.14295v3",
        "snippet": "Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph.",
        "source": "web"
      },
      {
        "title": "How Code Formatting Silently Consumes Your LLM Budget - arXiv",
        "url": "https://arxiv.org/html/2508.13666v1",
        "snippet": "Formatting elements such as indentation, whitespace, and newlines are commonly used to achieve this clarity.",
        "source": "web"
      },
      {
        "title": "Contextual Retrieval in AI Systems - Anthropic",
        "url": "https://www.anthropic.com/news/contextual-retrieval",
        "snippet": "In this post, we outline a method that dramatically improves the retrieval step in RAG. The method is called “Contextual Retrieval” and uses two sub-techniques.",
        "source": "web"
      },
      {
        "title": "The Role of Diversity in In-Context Learning for Large Language ...",
        "url": "https://arxiv.org/html/2505.19426v2",
        "snippet": "3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and ...",
        "source": "web"
      },
      {
        "title": "Agent Tool Error Handling with Prompts - ApX Machine Learning",
        "url": "https://apxml.com/courses/prompt-engineering-agentic-workflows/chapter-3-prompt-engineering-tool-use/addressing-tool-errors-via-prompts",
        "snippet": "... and various error handling patterns (e.g., retries, timeouts, graceful degradation) that are directly applicable to designing robust agentic workflows. Prompt ...",
        "source": "web"
      },
      {
        "title": "The State of LLM Reasoning Model Inference - Ahead of AI",
        "url": "https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling",
        "snippet": "This article explores recent research advancements in reasoning-optimized LLMs, with a particular focus on inference-time compute scaling.",
        "source": "web"
      },
      {
        "title": "In Context Learning Guide - PromptHub",
        "url": "https://www.prompthub.us/blog/in-context-learning-guide",
        "snippet": "Learn what In Context Learning is, how it can boost LLM performance, and discover best practices for optimizing prompts with examples and templates to get ...",
        "source": "web"
      },
      {
        "title": "5 Steps to Build Exception Handling for AI Agent Failures - Datagrid",
        "url": "https://www.datagrid.com/blog/exception-handling-frameworks-ai-agents",
        "snippet": "Graceful degradation keeps core functionality working even when specialized agents fail. Design downstream agents to operate with reduced ...",
        "source": "web"
      },
      {
        "title": "Efficient Inference for Large Reasoning Models: A Survey - arXiv",
        "url": "https://arxiv.org/html/2503.23077v2",
        "snippet": "This survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the ...",
        "source": "web"
      },
      {
        "title": "LLM-empowered knowledge graph construction: A survey - arXiv",
        "url": "https://arxiv.org/html/2510.20345v1",
        "snippet": "In summary, top-down research on LLM-assisted ontology construction emphasizes semantic consistency, structural completeness, and human–AI ...",
        "source": "web"
      },
      {
        "title": "Role-based prompting - GeeksforGeeks",
        "url": "https://www.geeksforgeeks.org/artificial-intelligence/role-based-prompting/",
        "snippet": "Role-based prompting is a prompt engineering technique where you explicitly instruct an AI to assume a specific role, persona or character ...",
        "source": "web"
      },
      {
        "title": "How to build great tools for AI agents: A field guide - Composio",
        "url": "https://composio.dev/blog/how-to-build-tools-for-ai-agents-a-field-guide",
        "snippet": "At our company, we've standardised on snake_case naming all tools, because inconsistency can easily confuse models, making them believe a tool ...",
        "source": "web"
      },
      {
        "title": "[PDF] An LLM-based Tool for Semantic Annotation and Knowledge Graph ...",
        "url": "https://sci-k.github.io/2025/papers/paper14.pdf",
        "snippet": "It supports users in transforming research data into semantically enriched graph representations through a guided, end-to-end workflow. The ...",
        "source": "web"
      },
      {
        "title": "Role Prompting: Guide LLMs with Persona-Based Tasks",
        "url": "https://learnprompting.org/docs/advanced/zero_shot/role_prompting",
        "snippet": "Role Prompting assigns a persona to an LLM, such as \"teacher\" or \"salesperson,\" to guide the style, tone, and focus of responses.",
        "source": "web"
      },
      {
        "title": "How to design APIs to be found by AI agents - ApyHub",
        "url": "https://apyhub.com/blog/how-to-design-apis-to-be-found-by-ai-agents",
        "snippet": "... agent understand and evaluate your API's utility ... tools that can analyze API design governance and generate comprehensive verification reports.",
        "source": "web"
      }
    ],
    "object": "chat.completion",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "# Designing Information Interfaces for Large Language Models: Structural Principles, Format Optimization, and Context Architecture\n\nInformation interface design for large language models represents a fundamentally different discipline from traditional human-centered user experience design. While human interfaces prioritize visual hierarchy, cognitive load reduction, and aesthetic appeal, LLM interfaces must optimize for token efficiency, attention mechanism alignment, and semantic clarity within computational constraints. Recent research from 2024-2026 reveals that the structure, formatting, and positioning of information significantly impacts LLM comprehension and output quality, with some format choices reducing retrieval errors by up to 67% and token consumption by 24-38%. This report synthesizes current best practices in LLM interface design, examining how context should be organized, how structured data should be formatted, how information positioning affects comprehension, and how narrator systems can effectively feed context to AI agents while maintaining performance under the constraints of limited context windows.\n\n## Foundational Principles of LLM Information Architecture\n\n### Understanding the LLM Attention Landscape\n\nThe fundamental difference between designing interfaces for humans and designing for LLMs lies in how each processes sequential information. Humans can quickly scan layouts, group related elements visually, and maintain awareness of overall structure across pages or screens. Large language models, by contrast, process information as linear sequences of tokens, where each token must be understood through the lens of transformer attention mechanisms[12][20]. These mechanisms allow every token position to attend to every other token position, creating what researchers call an \"attention landscape\" where the relationship between tokens and their positions dramatically affects model performance.\n\nThe transformer architecture that powers modern LLMs exhibits several critical characteristics that directly inform interface design. First, models demonstrate what researchers term **primacy bias**, where information at the beginning of a prompt receives disproportionate attention and interpretation accuracy[20]. Simultaneously, models show **recency bias**, where information at the very end of a context window also receives heightened attention. However, information placed in the middle of a long context window—even information that should be critical to the task—experiences a dramatic performance degradation phenomenon known as the \"lost-in-the-middle\" problem[9][20]. When relevant information is positioned in the middle of input context, model performance can drop by more than 20 percentage points compared to when that same information appears at the beginning or end[20].\n\nThis positional bias stems from multiple architectural factors. The rotary position embeddings (RoPE) used by many modern models cause attention scores to decay as token distance increases[20]. Additionally, the training data distributions that models learn from typically contain shorter sequences where this positional degradation is less pronounced, leaving models with less experience and fewer specialized parameters for managing information across very long contexts[12]. These aren't bugs to be worked around—they're fundamental to how transformer-based language models process sequential information, making them essential considerations for any interface designed to feed information to LLMs.\n\n### The Sweet Spot of Context Design\n\nThe goal of LLM interface design is not to minimize context—providing insufficient context leads to hallucinations and errors. Nor is it to maximize context size—larger context windows introduce noise and trigger the lost-in-the-middle problem[1]. Instead, effective LLM interfaces must hit what researchers call \"the sweet spot\": providing precisely the right amount of relevant, well-structured information so the model can deliver useful, accurate results without being overwhelmed or distracted[1].\n\nThis principle requires thinking systematically about what information belongs in the prompt at all. Context engineering for AI agents operates on a principle of **essential relevance**: every piece of information should directly serve the model's immediate decision-making need[12]. Anthropic's research on effective context engineering distinguishes between two types of context: static context (system instructions, documentation, rules) and dynamic context (current state, user input, runtime data). Static context should be positioned early in the prompt and cached whenever possible to reduce redundant processing[12][19]. Dynamic context, which changes with each interaction, should be positioned at the very end of the prompt to take advantage of recency bias and ensure the model pays maximum attention to the most current information[9][12].\n\n## Data Format Optimization for LLM Comprehension\n\n### Comparative Analysis of Structured Data Formats\n\nOne of the most empirically well-studied questions in LLM interface design concerns which structured data formats LLMs comprehend most effectively. A comprehensive 2025 study evaluating three major language models—GPT-5 Nano, Llama 3.2 3B Instruct, and Gemini 2.5 Flash Lite—against 1,000 test questions across nested data structures revealed striking format preferences[7]. The findings challenge conventional assumptions about JSON's superiority and reveal that **YAML emerged as the strongest format for two of the three models tested, with XML consistently underperforming across all models**[7].\n\nSpecifically, the research demonstrated that GPT-5 Nano showed the strongest format preference of all models evaluated, with YAML outperforming XML by 17.7 percentage points in accuracy[7]. Gemini 2.5 Flash Lite followed the same pattern, with YAML performing best and XML performing worst by a substantial margin. Only Llama 3.2 3B Instruct showed relatively format-insensitive performance, with minimal differences between JSON, YAML, and XML[7]. These differences aren't minor—they represent the difference between 51.9% accuracy and 33.8% accuracy for the same underlying data on Gemini 2.5 Flash Lite depending solely on format choice[7].\n\nThe token efficiency implications extend the findings beyond accuracy. Markdown tables achieved the best token efficiency across all models, using 34-38% fewer tokens than JSON, around 10% fewer than YAML, and substantially fewer than XML[7]. However, this efficiency came without meaningfully sacrificing accuracy in most cases[7]. For applications where cost matters as much as accuracy, Markdown represents a compelling choice—the trade-off between slightly lower accuracy and substantially lower token consumption and latency creates an optimal balance for many production scenarios.\n\nWhy does format matter so profoundly for LLM comprehension? The answer lies in how models were trained on data from the web and in coding repositories[10]. Models have seen vastly more examples of certain formats than others, and the syntactic density and clarity of different formats affects how effectively the model's tokenizer represents the information. XML's verbose closing tags and redundant structure encoding consume substantially more tokens without adding semantic value to the LLM[7]. JSON's heavy use of brackets and commas adds syntactic noise that, while excellent for machine parsing, provides little benefit to models that process text sequentially[7]. YAML's minimal syntax and human-readable structure aligns better with how models learned to process documentation and configuration files[10].\n\nThe practical guidance from this research is clear: **organizations with significant nested data should test multiple formats empirically with their specific models and use cases**, recognizing that format choice is not neutral[7]. In the absence of model-specific data, YAML serves as a solid default if accuracy is the priority, while Markdown should be considered if cost optimization is paramount[7]. For Llama-based models specifically, JSON remains viable since these models show minimal format sensitivity[7].\n\n### Table Data Representation and Comprehension\n\nBeyond nested structures, tables represent one of the most common data formats that LLMs must process. Microsoft research on LLM understanding of structured table data discovered that LLMs have surprisingly incomplete comprehension of tables despite their ubiquity[6]. Even simple tasks like identifying the number of columns and rows achieved only 65.43% accuracy across seven benchmark tasks, with the highest overall accuracy at 65.43%[6]. This finding underscores a critical point: **tables are not intuitively understood by LLMs as humans understand them visually**. Instead, tables must be serialized into sequential text representations, and this serialization process dramatically affects performance.\n\nThe research identified several key factors affecting table comprehension[6]. Delimiter-separated formats like CSV and TSV underperformed HTML by 6.76 percentage points, suggesting that explicitly marked table structure (through HTML tags) helps models better identify table boundaries and relationships[6]. Using HTML formatting combined with few-shot learning consistently improved performance across tasks[6]. Format explanation—explicitly telling the model how the table is structured—helped in some cases but not universally[6].\n\nMore importantly, the research introduced a technique called **self-augmentation**, where models use their own internal knowledge to improve structural understanding[6]. Rather than relying purely on the table serialization provided, the model generates intermediate structural insights about key values and ranges by leveraging its existing knowledge base[6]. This finding suggests that LLM interface design for tables should include explicit prompting that encourages the model to identify and articulate the table's structure before using it, essentially having the model teach itself about the data it's about to process.\n\nThe practical implication is that HTML-formatted tables with clear structural markers, combined with prompts that ask the model to first describe what it observes about the table structure, outperform simple CSV or markdown serializations[6]. Additionally, providing examples of how to interpret similar tables (few-shot learning) significantly improved comprehension[6].\n\n### Whitespace and Formatting Efficiency\n\nAn unexpected discovery in LLM interface design concerns the role of whitespace and formatting elements. Code formatting—indentation, whitespace, and newlines—was long assumed to be beneficial for LLM understanding because these elements improve human readability. However, research demonstrates that **formatting elements consume approximately 24.5% of tokens across programming languages while providing minimal benefits for advanced models**[4][47].\n\nWhen formatting elements were carefully removed without changing code semantics (preserving the abstract syntax tree), input token count decreased by 24.6%, while output token count decreased only 2.9%[4]. This asymmetry reveals that models tend to generate output in familiar formatting styles regardless of input format, suggesting that formatting in input serves little functional purpose for model understanding[4]. The research identified three primary formatting element categories—indentation, whitespace, and newlines—each consuming tokens without adding semantic value[47].\n\nThis doesn't mean formatting is irrelevant for all LLM tasks. Poetry and visually-dependent documents show that whitespace carries genuine semantic meaning and shouldn't be stripped[44]. However, for code, technical documentation, and structured data, removing non-essential formatting can reduce token consumption without degrading performance[4]. A practical workflow emerges: remove formatting before sending information to the LLM to reduce token overhead, then restore formatting in the LLM's output for human readability, without altering the underlying logic or meaning[4].\n\nThe broader principle here is that **LLM interface design should ruthlessly eliminate formatting elements that exist purely for human visual processing**, while preserving structural elements that encode semantic meaning. A well-designed interface for LLMs prioritizes information density and semantic clarity over visual organization.\n\n## Attention Patterns and Information Positioning Strategies\n\n### The Lost-in-the-Middle Phenomenon and Mitigation\n\nUnderstanding and designing around the lost-in-the-middle problem represents perhaps the single most important principle for large-scale LLM interface design. When models must access information buried in the middle of long contexts, performance degrades dramatically—sometimes more severely than when no context is provided at all[20][23]. The phenomenon isn't a feature of early models: it persists even in modern models with expanded context windows[20]. While newer models with 128K token context windows achieve near-perfect performance on 4K token retrieval tasks, this advantage diminishes as the context window fills[20].\n\nThe underlying cause involves both training data distributions and architectural properties. Models trained primarily on shorter sequences develop attention patterns optimized for the scale of their training data[12]. Additionally, the mathematical properties of RoPE and other position encoding schemes cause attention to decay over distance[20]. The problem manifests as a clear U-shaped performance curve: high accuracy for information at the beginning (positions 1-25%), high accuracy for information at the end (positions 75-100%), but significantly degraded accuracy for information in the middle (positions 35-70%)[20][23].\n\nEffective mitigation requires fundamental restructuring of how context is provided. The **\"Static-first, dynamic-last\" principle** emerges as the primary design guideline[19]. All static content—system instructions, documentation, rules, reference information—should occupy the beginning of the prompt where primacy bias ensures careful attention[12]. Any dynamic, time-sensitive, or query-specific content should appear at the very end where recency bias concentrates model attention[9][12]. Information that doesn't support the immediate task should be excluded entirely, even if it might seem potentially useful.\n\nAnthropic's research on context engineering introduces the concept of **progressive disclosure**, where agents incrementally discover relevant context through autonomous exploration rather than receiving all context upfront[12]. Rather than stuffing a million-token context into Gemini and hoping the model finds relevant information, agents navigate the available information layer by layer. File sizes hint at complexity, naming conventions suggest purpose, and timestamps indicate recency. This approach keeps the immediate context window focused on what's needed right now, with agents capable of exploring related information when necessary[12].\n\nFor situations where comprehensive context is genuinely needed—such as very long documents or complex multi-file codebases—three complementary techniques emerge: compaction, structured note-taking, and multi-agent architectures[12]. Compaction involves having the model summarize critical details while discarding redundant outputs, maintaining continuity across context resets. Structured note-taking allows agents to build external knowledge bases that persist across multiple interactions. Multi-agent architectures distribute work across specialized agents with focused context windows, with only condensed summaries flowing between agents[12].\n\n### Positioning Context Within the Attention Window\n\nThe empirical study on the lost-in-the-middle problem provides specific guidance on information positioning[20]. Rather than treating all position within a context window equally, designers should think about position relationally. What matters isn't absolute position within a context window but rather position relative to the model's trained experience with that context window size[20]. This means that as context windows expand, the \"sweet spot\" for critical information remains anchored to the beginning and end, with the size of the problematic middle section growing proportionally.\n\nA practical design pattern emerges: for documents that must fit within a context window, front-load the most important information—conclusions, summaries, key findings—before detailed supporting information[12]. For documents that exceed context windows, break them into semantic chunks small enough to fit within the optimal range, then use retrieval-augmented generation to surface relevant chunks as needed[9].\n\nFor agents managing multiple documents or sources, the positioning principle extends to retrieval ordering. Documents should be retrieved in an order that places the most critical document at the beginning of the context and the most time-sensitive information at the end, minimizing reliance on middle positions[9][20]. Some sophisticated systems implement what researchers call \"context-aware retrieval\" where, rather than retrieving a fixed number of documents, the system adapts which documents to include based on the specific query[45].\n\n## Structured Narration Systems and Context-Feeding Architectures\n\n### The Narrator Pattern and Model Context Protocol\n\nA emerging architectural pattern for feeding context to LLMs involves what researchers term **narrator systems**—dedicated subsystems responsible for curating, structuring, and presenting information to AI agents[8][9]. Rather than having each agent independently manage its relationship with data sources, narrator systems separate the concern of information discovery and presentation from the concern of task execution.\n\nThe most formal standardization of this pattern comes from Anthropic's **Model Context Protocol (MCP)**, an open standard for connecting AI applications to external data sources[8][11]. MCP establishes a pattern where developers build dedicated servers that expose their data and tools through a standardized protocol. AI applications (clients) connect to these servers, and at runtime, they discover what information and tools are available[8]. This decoupling provides several critical advantages for LLM interface design.\n\nFirst, it enables **dynamic capability expansion**. Rather than hard-coding a fixed set of tools and data sources into the LLM prompt, agents can query available tools at runtime, discovering which tools exist and what they do without pre-loading all that information into the context window[8][36]. This directly addresses the token efficiency problem: agents only load information about tools they might actually need[8]. Second, MCP enables **modular tool descriptions**, where each tool provides a clear schema describing its purpose, parameters, constraints, and expected outputs[57]. When tools follow consistent patterns for documentation, LLMs reliably invoke them with fewer errors.\n\nBeyond MCP, broader narrator patterns emerge in production systems. Firecrawl's context layer architecture, for instance, separates operational context (dynamic state, current values) from decision context (rules, policies, established facts)[9]. Operational context gets injected at the end of prompts to take advantage of recency bias. Decision context gets front-loaded and can be cached to reduce redundant computation. The narrator subsystem manages this separation, ensuring the agent receives the right information in the right order for optimal comprehension[9].\n\n### Context Stratification in Production Systems\n\nSophisticated production systems implement **context stratification**: organizing information into layers based on volatility, importance, and function. The Firecrawl research identifies a clear taxonomy[9]:\n\n**High-volatility context** (APIs, databases, logs, real-time state) changes constantly and should be injected at the **end** of prompts, placed last to ensure the model applies recency bias and prioritizes current state. This context answers \"where are we right now?\"[9]. Placement is critical: information about the current state must be the last thing the model reads to ensure maximum attention[9].\n\n**Low-volatility context** (documentation, wikis, codebases, established rules) changes rarely and should be **cached** to avoid reprocessing. This context belongs at the **very beginning** of the system prompt because it sets boundary conditions for all subsequent reasoning[9]. It frames the \"physics\" of the interaction—the fundamental rules and constraints within which the agent operates[9]. Caching static context across multiple requests can reduce API costs by 90% while improving latency by 2x[19][45].\n\n**Medium-volatility context** (recent decisions, established patterns, semi-permanent state) requires middle-ground positioning. This context should appear after static rules but before immediate operational data[9]. It represents the accumulated understanding from previous work that informs current decisions[9].\n\nThis stratification principle directly opposes the common practice of stuffing everything into a single massive prompt[1][9]. Instead, it advocates for **surgical information delivery**: providing precisely the information needed for the immediate decision, with access to other information through retrieval when necessary[12]. The narrator system responsible for context feeding essentially plays the role of an archivist, maintaining organized information and presenting only what's relevant to the immediate task.\n\n### Tool Description and Parameter Design\n\nA critical component of narrator systems involves how tools and available actions are described to LLMs. Research on tool use design patterns identifies that **tool naming consistency, parameter clarity, and constraint specification dramatically affect successful tool invocation**[57]. When tools use inconsistent naming conventions (mixing camelCase, snake_case, and kebab-case), models become confused and fail to invoke them correctly[57]. Similarly, vague parameter descriptions lead to incorrect invocations[57].\n\nBest practices for tool description involve starting with the format \"Tool to <do X>. Use when <Y happens>.\" followed by only the impactful constraints and brief examples[57]. Parameter descriptions should include constraint information inline, such as \"If format is json, jsonOptions is required\"[57]. Strongly-typed parameters with explicit enums reduce agent confusion[57]. For example, instead of vague descriptions like \"unit of measurement, e.g. Celsius or Fahrenheit\", explicitly defining an enum with [\"celsius\", \"fahrenheit\"] dramatically improves accuracy[57].\n\nThe deeper principle is that **tool descriptions should be optimized for agent comprehension, not human documentation**. Agents don't need elaborate examples or verbose explanations. They need clarity: what does this tool do, when should it be used, what parameters does it require, what values does each parameter accept, and what will it return[57]? This information should be provided in the most concise, unambiguous form possible.\n\n## Prompt Caching and Semantic Preservation\n\n### Mechanisms and Economics of Prompt Caching\n\nOne of the most significant developments in LLM interface design involves **prompt caching**, a provider-native feature that stores and reuses the initial, unchanging part of a prompt[19]. Rather than reprocessing the same context repeatedly, caching stores the internal state of the model (key-value tensors in transformer attention layers) for that prefix, eliminating redundant computation[19].\n\nThe economic implications are substantial: prompt caching can reduce latency by up to 80% and reduce input token costs by up to 90% for large, repetitive prompts[19]. This makes caching particularly valuable for applications where the same system instructions, reference documentation, or knowledge base appears in multiple requests[19]. An agent that repeatedly consults the same 50KB documentation file can cache that documentation and avoid reprocessing it hundreds of times[19].\n\nThe mechanism relies on **exact prefix matching**: the cached prefix must be byte-for-byte identical to be reused[19]. This creates a critical design principle: **structure prompts so that static, reusable content appears first (and is identical across requests), with request-specific or dynamic content appearing last**[19]. This follows the static-first, dynamic-last principle discussed earlier, with the added benefit that static content can now be cached, reducing costs and latency simultaneously.\n\nFor OpenAI's implementation, prompt caching is automatic when prompts exceed approximately 1,024 tokens[19]. For other providers, the `prompt_cache_key` parameter influences routing and can improve cache hit rates by steering similar-prefix traffic to the same cache location[19]. Observability requires logging `usage.prompt_tokens_details.cached_tokens` to measure how many tokens were served from cache[19].\n\nThe golden rule is clear: **place static-first, dynamic-last**[19]. System instructions, reference documentation, and established knowledge should occupy the beginning of the prompt so they can be cached. User queries, retrieved snippets, and time-sensitive data belong at the end[19]. This structure simultaneously optimizes for caching benefits, takes advantage of recency bias, and avoids the lost-in-the-middle problem.\n\n### Semantic Caching and Double-Caching Strategies\n\nBeyond prompt caching, **semantic caching** offers complementary benefits. Where prompt caching reuses the model's internal state for prefix tokens, semantic caching stores complete responses and reuses them when semantically similar queries arrive[19]. A customer service chatbot using semantic caching might recognize that \"What's your return policy?\" and \"How do I return something?\" are semantically equivalent and serve the cached response rather than regenerating it[19].\n\nThe most sophisticated systems employ **double-caching strategies** that combine both approaches[19]. Prompt caching handles large static knowledge bases—documentation, rules, reference material—reducing token load on the model. Semantic caching handles high-frequency queries that repeat across users or sessions, avoiding API calls entirely for identical or similar requests[19]. Together, these approaches can reduce API costs by 90% in some scenarios while improving latency for both cache hits and misses[19].\n\nThe design pattern requires two implementation considerations. First, ensure that cached prompts are version-controlled and deterministically constructed so that identical prefixes actually cache together[19]. Non-deterministic prompt generation (such as randomly ordered examples) will prevent caching. Second, establish appropriate retention policies for cached content: short retention (in-memory, minutes) for frequently-changing content, longer retention (hours or days) for stable documentation[19].\n\n## Information Chunking and Semantic Decomposition\n\n### Agentic Chunking and Contextual Awareness\n\nWhen information exceeds a model's context window, chunking becomes necessary. Traditional chunking approaches split documents into fixed-size segments (e.g., 512 tokens) or recursive chunks based on natural boundaries (paragraphs, sentences, sections)[21]. **Agentic chunking** represents an evolution: using AI agents to dynamically segment text based on meaning and context rather than arbitrary size[21].\n\nAgentic chunking works through multiple stages[21]. First, text is split recursively using natural language boundaries (paragraphs, sentences, words) to avoid breaking semantic units. Second, the AI model generates chunk-specific contextual labels and summaries—metadata that helps downstream retrieval systems understand what each chunk contains without reading it in full[21]. This labeling process is crucial: a chunk might say \"Its more than 3.85 million inhabitants make it the European Union's most populous city\" without context. With agentic labeling, it becomes \"Berlin is the capital of Germany, known for being the EU's most populous city. Its more than 3.85 million inhabitants...\"[45].\n\nThe performance benefit is substantial. Contextual retrieval reduces retrieval errors by 49% when combined with contextual embedding models and BM25 ranking[45][48]. Combined with reranking (scoring and filtering chunks by relevance), error reduction reaches 67%[45][48]. These improvements directly translate to better downstream performance because the model receives more relevant context[45].\n\nThe design principle for chunk-level LLM interfaces is that **chunks should be self-contained semantic units with sufficient context to be understandable independently**. A chunk shouldn't require the reader to have already processed previous chunks to understand it. This self-containment makes chunks simultaneously searchable, retrievable, and effective in isolation[21].\n\n### Hierarchical Summarization and Progressive Condensation\n\nFor structured documents with clear hierarchy—books with chapters and sections, research papers with abstract/introduction/methods/results/conclusion, legal documents with clauses and subsections—**hierarchical summarization** provides an alternative chunking strategy[1]. Rather than treating all text equally, hierarchical summarization recognizes document structure and creates summaries at multiple levels[1].\n\nAt the finest level, summaries capture key points from individual sections. At the next level, summaries compress section-level summaries into chapter-level summaries. This continues up through the document hierarchy, creating a condensed representation of the entire document at the top level[1]. When the model needs to work with the document, it can progressively drill down: start with the document-level summary, refine to chapter summaries, then expand to section-level detail as needed.\n\nThis approach provides several advantages for LLM interfaces. First, it preserves context during condensation—understanding the section's role within the chapter helps the model interpret the section's content correctly[1]. Second, it reduces token consumption by allowing fine-grained control over detail level. For some tasks, the document-level summary suffices. For others, chapter-level detail is needed. Only when specific information is required does the model read the full section[1]. Third, it creates natural semantic boundaries that align with human document structure rather than arbitrary token counts[1].\n\n### Contextual Retrieval Techniques\n\nBuilding on agentic chunking, **contextual retrieval** adds another layer of sophistication by examining not just what chunks contain but how chunks relate to the broader context[45][48]. The technique operates in stages: first, chunks are created using agentic methods with generated context labels. Then, contextual embeddings are generated not just for the chunk text but for the chunk plus its contextual label[45]. This combined embedding better represents the semantic meaning of the chunk within its broader document[48].\n\nWhen retrieval occurs, both traditional semantic similarity (vector embeddings) and exact keyword matching (BM25) are applied, with results combined using rank fusion[45][48]. This hybrid approach combines the semantic understanding of embeddings with the precise matching of keyword search. The research shows that Contextual Embeddings reduce top-20-chunk retrieval failure rate by 35%, while combining Contextual Embeddings and Contextual BM25 reduces failure rate by 49%[48].\n\nThe interface design implication is that **retrieval-augmented systems should combine multiple retrieval approaches and leverage contextual information rather than relying purely on semantic similarity**[45][48]. Pure embedding-based retrieval, while intuitively appealing, often fails to capture relationships and context that matter for downstream comprehension[48].\n\n## Advanced Composition Patterns and Reasoning Structures\n\n### Chain-of-Thought and Structural Reasoning\n\nWhile not strictly an interface design pattern, how reasoning is structured and presented to LLMs profoundly affects output quality. **Chain-of-Thought (CoT) prompting**, where models are encouraged to think through problems step-by-step, consistently improves performance on complex tasks[46]. However, linear chain-of-thought has limitations for problems requiring branching or recombination of insights.\n\n**Tree of Thoughts (ToT)** extends chains to branching structures where each step can generate multiple next steps, enabling exploration of different solution paths[46]. This is particularly valuable for problems where multiple solution approaches exist and some are more promising than others[46]. **Graph of Thoughts (GoT)** further generalizes to arbitrary reasoning dependencies, where thoughts (nodes) can have multiple parents and children, enabling patterns like dynamic programming where subproblems are solved independently and combined[46].\n\nFor interface design feeding into reasoning models, the implication is that **the structure used to represent reasoning should match the problem's inherent structure**[46]. Simple sequential reasoning benefits from linear chain-of-thought. Problems with branching require tree structures. Problems involving recombination or aggregation of insights benefit from graph structures[46]. Providing reasoning structure that matches problem structure dramatically improves model performance[46].\n\nAdditionally, emerging research on **Chain-of-Symbol (CoS)** suggests that symbolic representations (using symbols like arrows, brackets, and notation rather than words) can represent spatial and planning reasoning more efficiently than natural language chains-of-thought[22]. This finding suggests that for certain task domains, interface design should prioritize symbolic structure over natural language explanation[22].\n\n### System Prompts and Role Definition\n\nThe system prompt—the foundational instructions that shape how a model behaves—represents perhaps the most critical interface element for agent systems. Effective system prompts serve multiple functions: defining the assistant's role and expertise, specifying communication style and tone, establishing constraints and guardrails, and providing critical context that applies to all subsequent interactions[27][42].\n\nResearch on system prompt design identifies several best practices[27][42]. First, section the prompt clearly using markdown headings to separate concerns: role definition, goals, tools, constraints, output format. Second, use simple, direct language presenting ideas at the right altitude—neither oversimplifying the task nor overwhelming with complexity[12][27]. Third, emphasize critical instructions through repetition and explicit markers like \"This step is important\"[27]. Fourth, use concrete examples showing desired behavior rather than abstract instructions[27].\n\nA powerful pattern involves **defining instruction hierarchy**—explicitly stating which instructions take priority if conflicts arise[42]. For instance: system instructions override developer instructions, which override user prompts[42]. This prevents users from bypassing critical constraints through prompt injection[42]. Another critical pattern is **defining refusal style**—how the assistant should decline requests it can't fulfill. Research shows that short, respectful refusals that offer helpful alternatives build more trust than long, moralizing explanations[42].\n\nFor production agent systems, system prompts should be version-controlled and incrementally refined based on real-world performance[39][42]. Treating prompts like code—with change tracking, testing, and gradual updates—is far more effective than writing a prompt once and assuming it works[39]. Additionally, system prompts shouldn't attempt to override fundamental model capabilities or alignment training[39][42]. Even the most carefully crafted prompt cannot make a model generate malware if the base model refuses[39].\n\n### Few-Shot Learning and Example Selection\n\nThe quality and diversity of examples provided in few-shot learning dramatically affects model performance[32][35][49]. Rather than assuming that more examples are always better, research reveals that **example diversity significantly improves performance on complex tasks**, particularly when examples come from different distributions than the query[49]. When demonstrated examples and actual queries share the same distribution, similarity-based example selection (choosing the most similar example to the query) performs best[49]. When they come from different distributions, diversity-aware selection (choosing diverse examples rather than just the most similar ones) significantly outperforms similarity-based selection[49].\n\nThe interface design implication is that **example selection should be adaptive rather than fixed**. For known, in-distribution scenarios, selecting the most similar examples works well. For scenarios involving out-of-distribution or novel queries, deliberately selecting diverse examples provides better grounding[49]. Additionally, research on example quality reveals that **low-quality or mislabeled examples can actually harm performance more than providing no examples at all**[35]. Curating high-quality, diverse examples is far more important than maximizing the number of examples[35].\n\n## Measuring and Evaluating LLM Interface Design\n\n### Token Efficiency and Cost Optimization\n\nOptimizing token efficiency—achieving better results with fewer tokens—has become a primary concern in LLM system design[14][17]. Several concrete techniques emerge: concise prompt engineering that removes redundant language, dynamic in-context learning that adapts examples to the specific task, batch prompting that processes multiple data points in a single API call, and skeleton-of-thought prompting that enables parallel generation of response components[14].\n\nToken optimization requires systematic measurement. Before optimizing, measure baseline token consumption for your typical prompts. Identify which components consume the most tokens: examples, instructions, context, output formatting[14][17]. Then apply targeted optimization: are your examples concise? Can you reduce the number of examples without losing performance? Are your instructions clear without being verbose? Are you including unnecessary context?\n\nA practical measurement framework involves the **token optimization audit** where you categorize token consumption by functional component[17]. A prompt might use 100 tokens for system instructions, 250 tokens for context, 200 tokens for examples, 50 tokens for the actual question. This breakdown reveals which components offer the highest optimization potential[17]. Reducing instructions from 100 to 50 tokens might be difficult. Reducing context from 250 to 180 tokens through better information selection might be straightforward[17].\n\n### Comprehension Benchmarks and Retrieval Metrics\n\nFor systems implementing retrieval-augmented generation, measuring retrieval quality is critical. The primary metric is **recall@K**: the percentage of relevant documents that appear within the top K retrieved results[48]. Traditional systems achieve roughly 5.7% failure rate (94.3% recall@20) on diverse knowledge domains[48]. Contextual retrieval reduces this to 2.9% failure rate (97.1% recall@20), a 49% error reduction[48]. Combined with reranking, failure rate drops to 1.9%, a 67% error reduction[48].\n\nThese metrics directly correlate with downstream LLM performance. When retrieval fails to surface relevant information, the LLM generates responses based on incomplete context, leading to hallucinations and errors[48]. Conversely, when retrieval succeeds, the LLM has the information needed to generate accurate responses[48].\n\n## Conclusion: Synthesis and Future Directions\n\nDesigning information interfaces for large language models requires fundamentally different thinking than human-centered design. Where human interfaces optimize for visual hierarchy and cognitive load reduction, LLM interfaces must optimize for token efficiency, attention mechanism alignment, and semantic clarity. The research from 2024-2026 provides clear guidance on how to structure information effectively.\n\nThe core principles are these: organize information with static, reusable content first and dynamic, time-sensitive content last. This structure simultaneously takes advantage of primacy bias, enables caching, and avoids the lost-in-the-middle problem. Choose data formats empirically—YAML and Markdown often outperform JSON and XML depending on the model and use case. Eliminate formatting that serves only human visual processing while preserving structure that encodes semantic meaning. Implement narrator systems or Model Context Protocol patterns to separate information curation from task execution. Use prompt caching to avoid reprocessing static context. Apply agentic chunking with contextual labels when information exceeds context windows. Structure reasoning patterns to match problem structure, moving beyond linear chains to trees or graphs when appropriate.\n\nThe economics of these design choices are compelling. Proper context architecture reduces token consumption by 24-38%. Contextual retrieval improves accuracy by 49-67%. Prompt caching reduces latency by 80% and costs by 90%. Few-shot example selection tailored to task distribution improves performance. Tool descriptions optimized for agent comprehension reduce invocation errors. These aren't marginal improvements—they're transformative.\n\nLooking forward, several research directions merit attention. First, the interaction between large context windows and the lost-in-the-middle problem deserves deeper investigation—as context windows expand, designing optimal information placement becomes increasingly critical[20]. Second, the relationship between inference-time scaling and context quality requires study: as reasoning models spend more time thinking, how does context architecture affect reasoning quality[51]? Third, multimodal contexts—combining text, tables, graphs, and images in single interfaces—require research on how to structure heterogeneous information for multimodal models[40].\n\nThe discipline of LLM interface design will mature as organizations accumulate experience with these patterns. What's clear now is that information interface design for LLMs is not a minor technical detail—it's a fundamental aspect of building capable, efficient, and reliable AI systems. Organizations that systematize their approach to LLM interface design will see substantial improvements in both performance and cost efficiency compared to those treating prompts as afterthoughts."
        },
        "delta": {
          "role": "assistant",
          "content": ""
        },
        "finish_reason": "stop"
      }
    ]
  }
}