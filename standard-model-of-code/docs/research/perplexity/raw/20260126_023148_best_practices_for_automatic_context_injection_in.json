{
  "_meta": {
    "saved_at": "2026-01-26T02:31:48.094503",
    "source": "perplexity-mcp",
    "model": "sonar-deep-research",
    "query": "Best practices for automatic context injection in LLM agent workflows 2024-2026. How do production AI agent systems pre-load context WITHOUT requiring tool calls? Research: 1) Pre-hook patterns for injecting state before LLM inference, 2) MCP (Model Context Protocol) resource auto-loading vs explicit tool calls, 3) 'Heads-up display' or 'instrument panel' patterns for AI agents - always-visible context, 4) Claude Code hooks architecture, 5) How do Cursor, Windsurf, Cline, and other AI coding tools inject project context automatically? Looking for zero-latency, zero-agency-cost context injection patterns.",
    "query_length": 611,
    "checksum": "sha256:051434a6fe9122e61caac35f777c747b192f19c98f25be166cbf76e00008972f"
  },
  "response": {
    "id": "795bd225-dd8b-4fb2-9bf8-744a3fb66df5",
    "model": "sonar-deep-research",
    "created": 1769405507,
    "usage": {
      "prompt_tokens": 139,
      "completion_tokens": 7282,
      "total_tokens": 7421,
      "citation_tokens": 56559,
      "num_search_queries": 30,
      "reasoning_tokens": 353501,
      "cost": {
        "input_tokens_cost": 0.0,
        "output_tokens_cost": 0.058,
        "citation_tokens_cost": 0.113,
        "reasoning_tokens_cost": 1.061,
        "search_queries_cost": 0.15,
        "total_cost": 1.382
      }
    },
    "citations": [
      "https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/",
      "https://modelcontextprotocol.info/docs/concepts/resources/",
      "https://arxiv.org/html/2509.09505v2",
      "https://arxiv.org/pdf/2506.08837.pdf",
      "https://modelcontextprotocol.io/specification/2025-06-18/server/resources",
      "https://docs.agno.com/basics/hooks/overview",
      "https://www.augmentcode.com/tools/cline-vs-cursor",
      "https://www.datacamp.com/tutorial/claude-code-hooks",
      "https://exalt-studio.com/blog/designing-for-ai-agents-7-ux-patterns-that-drive-engagement",
      "https://uibakery.io/blog/cursor-vs-windsurf-vs-cline",
      "https://code.claude.com/docs/en/hooks-guide",
      "https://www.vellum.ai/blog/multi-agent-systems-building-with-context-engineering",
      "https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/",
      "https://nebius.com/blog/posts/serving-llms-with-vllm-practical-guide",
      "https://support.zendesk.com/hc/en-us/articles/4408832735130-Showing-the-customer-context-panel-by-default-in-the-ticket-interface-standard-agent-interface",
      "https://www.zenml.io/blog/what-1200-production-deployments-reveal-about-llmops-in-2025",
      "https://docs.salad.com/container-engine/explanation/ai-machine-learning/llm-overview",
      "https://learn.microsoft.com/en-us/clarity/brand-agents/dashboard/agent-dashboard-overview",
      "https://modelcontextprotocol.io/specification/2025-06-18/server/resources",
      "https://saurabhalone.com/blog/agent",
      "https://arxiv.org/html/2311.04934v2",
      "https://github.com/microsoft/vscode/issues/260689",
      "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
      "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html",
      "https://www.pento.ai/blog/a-year-of-mcp-2025-review",
      "https://docs.langchain.com/oss/python/langchain/context-engineering",
      "https://dev.to/akshaygupta1996/production-grade-ai-agents-architecture-patterns-that-actually-work-19h",
      "https://www.anthropic.com/engineering/code-execution-with-mcp",
      "https://galileo.ai/blog/context-engineering-for-agents",
      "https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/",
      "https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf",
      "https://arxiv.org/html/2406.13352v3",
      "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "https://aws.plainenglish.io/agent-framework-python-overview-b9ef84ac80d9",
      "https://www.evidentlyai.com/llm-guide/prompt-injection-llm",
      "https://tweag.github.io/agentic-coding-handbook/WORKFLOW_MEMORY_BANK/",
      "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
      "https://arxiv.org/html/2601.06007v1",
      "https://www.marthakelly.com/blog/prompt-caching-design-for-reuse",
      "https://blog.sugiv.fyi/context-engineering",
      "https://arxiv.org/pdf/2601.06007.pdf",
      "https://platform.claude.com/docs/en/build-with-claude/prompt-caching",
      "https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf?hsLang=en",
      "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
      "https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ai-agents/build-secure-process",
      "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "https://cookbook.openai.com/examples/agents_sdk/session_memory",
      "https://aws.amazon.com/blogs/machine-learning/multi-agent-collaboration-patterns-with-strands-agents-and-amazon-nova/",
      "https://williamzujkowski.github.io/posts/from-150k-to-2k-tokens-how-progressive-context-loading-revolutionizes-llm-development-workflows/",
      "https://galileo.ai/blog/context-engineering-for-agents",
      "https://cookbook.openai.com/examples/agents_sdk/context_personalization",
      "https://arxiv.org/html/2601.10729v1",
      "https://www.descope.com/learn/post/mcp",
      "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
      "https://vercel.com/blog/ai-sdk-6",
      "https://atalupadhyay.wordpress.com/2025/11/11/mcp-token-problem-building-efficient-ai-agents-with-skills/",
      "https://arxiv.org/html/2509.02121v1",
      "https://ai-sdk.dev/docs/foundations/streaming",
      "https://www.anthropic.com/engineering/advanced-tool-use",
      "https://natesnewsletter.substack.com/p/i-read-everything-google-anthropic"
    ],
    "search_results": [
      {
        "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
        "url": "https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/",
        "snippet": "This new paper provides a robust explanation of prompt injection, then proposes six design patterns to help protect against it.",
        "source": "web"
      },
      {
        "title": "Resources - Model Context Protocol （MCP）",
        "url": "https://modelcontextprotocol.info/docs/concepts/resources/",
        "snippet": "Resources are a core primitive in the Model Context Protocol (MCP) that allow servers to expose data and content that can be read by clients and used as ...",
        "source": "web"
      },
      {
        "title": "Optimization Pathways for Long-Context Agentic LLM Inference - arXiv",
        "url": "https://arxiv.org/html/2509.09505v2",
        "snippet": "These agentic LLM inference tasks are fundamentally different from chatbot-focused inference — they often have much larger context lengths to ...",
        "source": "web"
      },
      {
        "title": "[PDF] Design Patterns for Securing LLM Agents against Prompt Injections",
        "url": "https://arxiv.org/pdf/2506.08837.pdf",
        "snippet": "The research community has proposed various defensive measures against prompt injections, from heuristic approaches like detection and ...",
        "source": "web"
      },
      {
        "title": "Resources - Model Context Protocol",
        "url": "https://modelcontextprotocol.io/specification/2025-06-18/server/resources",
        "snippet": "Resources allow servers to share data that provides context to language models, such as files, database schemas, or application-specific information.",
        "source": "web"
      },
      {
        "title": "Pre-hooks and Post-hooks - Agno",
        "url": "https://docs.agno.com/basics/hooks/overview",
        "snippet": "Pre-hooks execute at the very beginning of your Agent run, giving you complete control over what reaches the LLM. They're perfect for implementing input ...",
        "source": "web"
      },
      {
        "title": "Cline vs Cursor: Which AI Coding Tool Wins in 2025?",
        "url": "https://www.augmentcode.com/tools/cline-vs-cursor",
        "snippet": "Cline transforms VS Code into an autonomous development workspace through Model Context Protocol integration. ... Cursor vs Windsurf ...",
        "source": "web"
      },
      {
        "title": "Claude Code Hooks: A Practical Guide to Workflow Automation",
        "url": "https://www.datacamp.com/tutorial/claude-code-hooks",
        "snippet": "Claude Code Hooks are shell commands that run automatically when specific events happen during your AI coding session. Think of them as ...",
        "source": "web"
      },
      {
        "title": "Designing for AI Agents: 7 UX Patterns That Drive Engagement",
        "url": "https://exalt-studio.com/blog/designing-for-ai-agents-7-ux-patterns-that-drive-engagement",
        "snippet": "Explore seven essential UX patterns that enhance user engagement and trust in AI-driven experiences, from smart predictions to mood-based responses.",
        "source": "web"
      },
      {
        "title": "Cursor vs Windsurf vs Cline: Which AI Dev Tool Is Right for You?",
        "url": "https://uibakery.io/blog/cursor-vs-windsurf-vs-cline",
        "snippet": "Cursor is built on top of VS Code and designed specifically for developers who want GitHub Copilot-like AI, but with tighter project context and ...",
        "source": "web"
      },
      {
        "title": "Get started with Claude Code hooks",
        "url": "https://code.claude.com/docs/en/hooks-guide",
        "snippet": "Claude Code hooks are user-defined shell commands that execute at various points in Claude Code's lifecycle. Hooks provide deterministic control over Claude ...",
        "source": "web"
      },
      {
        "title": "How to Build Multi Agent AI Systems With Context Engineering",
        "url": "https://www.vellum.ai/blog/multi-agent-systems-building-with-context-engineering",
        "snippet": "Understand the basics of AI multi agent systems, their benefits, and what best practices and approaches to consider before you start building one.",
        "source": "web"
      },
      {
        "title": "Architecting efficient context-aware multi-agent framework for ...",
        "url": "https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/",
        "snippet": "ADK's Context Engineering scales production AI agents. Architect tiered context for efficiency, reliability, and multi-agent context ...",
        "source": "web"
      },
      {
        "title": "Serving LLMs with vLLM: A practical inference guide - Nebius",
        "url": "https://nebius.com/blog/posts/serving-llms-with-vllm-practical-guide",
        "snippet": "This guide teaches the essentials of serving large language models with vLLM. It builds from foundational neural network concepts, ...",
        "source": "web"
      },
      {
        "title": "Showing the customer context panel by default in the ticket interface ...",
        "url": "https://support.zendesk.com/hc/en-us/articles/4408832735130-Showing-the-customer-context-panel-by-default-in-the-ticket-interface-standard-agent-interface",
        "snippet": "In Admin Center, click the Workspaces icon ( ) in the sidebar, then select Agent tools > Agent interface. · Locate the Default to Customer Context setting and ...",
        "source": "web"
      },
      {
        "title": "What 1,200 Production Deployments Reveal About LLMOps in 2025",
        "url": "https://www.zenml.io/blog/what-1200-production-deployments-reveal-about-llmops-in-2025",
        "snippet": "Analysis of 1200+ production LLM deployments reveals that context engineering, architectural guardrails, and traditional software ...",
        "source": "web"
      },
      {
        "title": "LLM Inference on SaladCloud",
        "url": "https://docs.salad.com/container-engine/explanation/ai-machine-learning/llm-overview",
        "snippet": "Most inference servers provide options to manage the VRAM usage, including MAX_INPUT_TOKENS (the maximum prompt length that users can send), ...",
        "source": "web"
      },
      {
        "title": "Agent Dashboard Overview | Microsoft Learn",
        "url": "https://learn.microsoft.com/en-us/clarity/brand-agents/dashboard/agent-dashboard-overview",
        "snippet": "The Agent Dashboard provides key metrics that help analyze agent-assisted user interactions on your site. At a glance, you can view how ...",
        "source": "web"
      },
      {
        "title": "Resources - Model Context Protocol",
        "url": "https://modelcontextprotocol.io/specification/2025-06-18/server/resources",
        "snippet": "Resource permissions SHOULD be checked before operations. Was this page helpful? Yes No. PromptsTools. Ctrl+I. github. Copyright © Model Context ...",
        "source": "web"
      },
      {
        "title": "The Hitchhikers Guide to LLM Agent",
        "url": "https://saurabhalone.com/blog/agent",
        "snippet": "After Prefill phase the model start generating tokens one by one autoregressively this known as Decode Phase 3. And each new tokens depends on ...",
        "source": "web"
      },
      {
        "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
        "url": "https://arxiv.org/html/2311.04934v2",
        "snippet": "We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts.",
        "source": "web"
      },
      {
        "title": "Automatic MCP Resources inclusion to the context #260689 - GitHub",
        "url": "https://github.com/microsoft/vscode/issues/260689",
        "snippet": "You can create an MCP Tool which return content you want to expose to AI. MCP tools are invoked automatically, based on the user intention, so ...",
        "source": "web"
      },
      {
        "title": "Context Engineering for AI Agents: Lessons from Building Manus",
        "url": "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
        "snippet": "This makes the ratio between prefilling and decoding highly skewed in agents compared to chatbots. In Manus, for example, the average input-to- ...",
        "source": "web"
      },
      {
        "title": "Prompt caching for faster model inference - Amazon Bedrock",
        "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html",
        "snippet": "Prompt caching is an optional feature that you can use with supported models on Amazon Bedrock to reduce inference response latency and input token costs.",
        "source": "web"
      },
      {
        "title": "A Year of MCP: From Internal Experiment to Industry Standard - Pento",
        "url": "https://www.pento.ai/blog/a-year-of-mcp-2025-review",
        "snippet": "What we learned building with the Model Context Protocol in 2025, and why 2026 will be the year AI agents actually do things.",
        "source": "web"
      },
      {
        "title": "Context engineering in agents - Docs by LangChain",
        "url": "https://docs.langchain.com/oss/python/langchain/context-engineering",
        "snippet": "The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format. ​. Defining ...",
        "source": "web"
      },
      {
        "title": "Production-Grade AI Agents: Architecture Patterns That Actually Work",
        "url": "https://dev.to/akshaygupta1996/production-grade-ai-agents-architecture-patterns-that-actually-work-19h",
        "snippet": "Learn the 7 essential patterns for building AI agents that scale in production - from goal-oriented design to tool orchestration.",
        "source": "web"
      },
      {
        "title": "Code execution with MCP: building more efficient AI agents - Anthropic",
        "url": "https://www.anthropic.com/engineering/code-execution-with-mcp",
        "snippet": "Learn how code execution with the Model Context Protocol enables agents to handle more tools while using fewer tokens, reducing context ...",
        "source": "web"
      },
      {
        "title": "Deep Dive into Context Engineering for Agents - Galileo AI",
        "url": "https://galileo.ai/blog/context-engineering-for-agents",
        "snippet": "Tools Context contains tool descriptions, feedback from tool calls, and error messages that enable the agent to interact with external systems.",
        "source": "web"
      },
      {
        "title": "Architecting efficient context-aware multi-agent framework for ...",
        "url": "https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/",
        "snippet": "ADK's Context Engineering scales production AI agents. Architect tiered context for efficiency, reliability, and multi-agent context ...",
        "source": "web"
      },
      {
        "title": "Amazon Bedrock AgentCore - Developer Guide",
        "url": "https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf",
        "snippet": "... agents can access tools and resources. • Translation - Converts agent requests using protocols like Model Context Protocol (MCP) into. API ...",
        "source": "web"
      },
      {
        "title": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection ...",
        "url": "https://arxiv.org/html/2406.13352v3",
        "snippet": "We have introduced AgentDojo, a standardized agent evaluation framework for prompt injection attacks and defenses, consisting of 97 realistic tasks and 629 ...",
        "source": "web"
      },
      {
        "title": "Effective context engineering for AI agents - Anthropic",
        "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
        "snippet": "Our overall guidance across the different components of context (system prompts, tools, examples, message history, etc) is to be thoughtful and ...",
        "source": "web"
      },
      {
        "title": "Agent Framework (Python) Overview - AWS in Plain English",
        "url": "https://aws.plainenglish.io/agent-framework-python-overview-b9ef84ac80d9",
        "snippet": "The agent_framework is Microsoft's Python library for building AI agent applications. It provides abstractions for chat agents, workflows, ...",
        "source": "web"
      },
      {
        "title": "What is prompt injection? Example attacks, defenses and testing.",
        "url": "https://www.evidentlyai.com/llm-guide/prompt-injection-llm",
        "snippet": "Prompt injection is a system-level vulnerability where user input is blended with system instructions – and the LLM follows the wrong logic.",
        "source": "web"
      },
      {
        "title": "Memory Bank System | Agentic Coding Handbook - tweag.github.io",
        "url": "https://tweag.github.io/agentic-coding-handbook/WORKFLOW_MEMORY_BANK/",
        "snippet": "A Memory Bank is a structured, markdown-based documentation system that acts as long-term memory for your AI agent. It allows the assistant to “remember” your ...",
        "source": "web"
      },
      {
        "title": "Context Engineering for AI Agents: Lessons from Building Manus",
        "url": "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
        "snippet": "Using this, we constrain action selection by masking token logits directly. For example, when the user provides a new input, Manus must ...",
        "source": "web"
      },
      {
        "title": "An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
        "url": "https://arxiv.org/html/2601.06007v1",
        "snippet": "Google offers both implicit caching, which activates automatically with no guaranteed cost savings, and explicit context caching, where ...",
        "source": "web"
      },
      {
        "title": "Prompt Caching: Design for Reuse | Martha Kelly",
        "url": "https://www.marthakelly.com/blog/prompt-caching-design-for-reuse",
        "snippet": "Anthropic's prompt caching lets you reuse static context across API calls—tool definitions, system instructions, ...",
        "source": "web"
      },
      {
        "title": "Context Engineering: The Art and Science of Prompt Architecture",
        "url": "https://blog.sugiv.fyi/context-engineering",
        "snippet": "The evolution from complex logit masking to elegant response prefill demonstrates a crucial insight for context engineering practitioners: the ...",
        "source": "web"
      },
      {
        "title": "[PDF] An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
        "url": "https://arxiv.org/pdf/2601.06007.pdf",
        "snippet": "Google offers both implicit caching, which activates automatically with no guaranteed cost savings, and explicit context caching, where ...",
        "source": "web"
      },
      {
        "title": "Prompt caching - Claude API Docs",
        "url": "https://platform.claude.com/docs/en/build-with-claude/prompt-caching",
        "snippet": "Most blocks in the request can be designated for caching with cache_control . This includes: Tools: Tool definitions in the tools array; System messages: ...",
        "source": "web"
      },
      {
        "title": "[PDF] 2026 Agentic Coding Trends Report | Anthropic",
        "url": "https://resources.anthropic.com/hubfs/2026%20Agentic%20Coding%20Trends%20Report.pdf?hsLang=en",
        "snippet": "In 2025, coding agents moved from experimental tools to production systems that ship real features to real customers. Engineering teams ...",
        "source": "web"
      },
      {
        "title": "Context Engineering for AI Agents: Lessons from Building Manus",
        "url": "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus",
        "snippet": "Language models hallucinate, environments return errors, external tools misbehave, and unexpected edge cases show up all the time. In multi ...",
        "source": "web"
      },
      {
        "title": "Process to build agents across your organization - Microsoft Learn",
        "url": "https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ai-agents/build-secure-process",
        "snippet": "This article outlines the standardized process that teams must follow to build AI agents consistently across the organization.",
        "source": "web"
      },
      {
        "title": "Effective context engineering for AI agents - Anthropic",
        "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
        "snippet": "Effective context engineering for AI agents. Published Sep 29, 2025. Context is a critical but finite resource for AI agents. In this post, we ...",
        "source": "web"
      },
      {
        "title": "Short-Term Memory Management with Sessions from OpenAI ...",
        "url": "https://cookbook.openai.com/examples/agents_sdk/session_memory",
        "snippet": "In this cookbook, we'll explore how to manage context effectively using the Session object from the OpenAI Agents SDK, focusing on two proven ...",
        "source": "web"
      },
      {
        "title": "Multi-Agent collaboration patterns with Strands Agents and Amazon ...",
        "url": "https://aws.amazon.com/blogs/machine-learning/multi-agent-collaboration-patterns-with-strands-agents-and-amazon-nova/",
        "snippet": "In this post, we explore four key collaboration patterns for multi-agent, multimodal AI systems – Agents as Tools, Swarms Agents, Agent Graphs, ...",
        "source": "web"
      },
      {
        "title": "From 150K to 2K Tokens: How Progressive Context Loading ...",
        "url": "https://williamzujkowski.github.io/posts/from-150k-to-2k-tokens-how-progressive-context-loading-revolutionizes-llm-development-workflows/",
        "snippet": "Progressive context loading cuts LLM token usage by 98% (150K → 2K) while maintaining full codebase context. Instead of dumping your entire ...",
        "source": "web"
      },
      {
        "title": "Deep Dive into Context Engineering for Agents - Galileo AI",
        "url": "https://galileo.ai/blog/context-engineering-for-agents",
        "snippet": "The challenge lies in selecting relevant knowledge while avoiding information overload. Tools Context contains tool descriptions, feedback from ...",
        "source": "web"
      },
      {
        "title": "State Management with Long-Term Memory Notes using OpenAI ...",
        "url": "https://cookbook.openai.com/examples/agents_sdk/context_personalization",
        "snippet": "At its core, context engineering is about shaping what the model knows at any given moment. By managing what's stored, recalled, and injected ...",
        "source": "web"
      },
      {
        "title": "SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache ...",
        "url": "https://arxiv.org/html/2601.10729v1",
        "snippet": "This overhead markedly delays GPU computations and subsequent token decode steps for every request in a batch, making it difficult to ...",
        "source": "web"
      },
      {
        "title": "What Is the Model Context Protocol (MCP) and How It Works",
        "url": "https://www.descope.com/learn/post/mcp",
        "snippet": "The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools.",
        "source": "web"
      },
      {
        "title": "Effective context engineering for AI agents - Anthropic",
        "url": "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents",
        "snippet": "Context refers to the set of tokens included when sampling from a large-language model (LLM). The engineering problem at hand is optimizing the ...",
        "source": "web"
      },
      {
        "title": "AI SDK 6 - Vercel",
        "url": "https://vercel.com/blog/ai-sdk-6",
        "snippet": "Agents automatically integrate with the full AI SDK ecosystem, giving you type-safe UI streaming, structured outputs, and seamless framework ...",
        "source": "web"
      },
      {
        "title": "MCP Token Problem: Building Efficient AI Agents with Skills",
        "url": "https://atalupadhyay.wordpress.com/2025/11/11/mcp-token-problem-building-efficient-ai-agents-with-skills/",
        "snippet": "Tool selection accuracy drops significantly with 30+ tools. It's like trying to remember someone's name at a party where you were just ...",
        "source": "web"
      },
      {
        "title": "Batch Query Processing and Optimization for Agentic Workflows",
        "url": "https://arxiv.org/html/2509.02121v1",
        "snippet": "To avoid redundant computation, we maintain a shared prefix cache of the corresponding hidden states. ... Zero-Code Framework for LLM Agents ...",
        "source": "web"
      },
      {
        "title": "Foundations: Streaming - AI SDK",
        "url": "https://ai-sdk.dev/docs/foundations/streaming",
        "snippet": "As you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the ...",
        "source": "web"
      },
      {
        "title": "Introducing advanced tool use on the Claude Developer Platform",
        "url": "https://www.anthropic.com/engineering/advanced-tool-use",
        "snippet": "Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window; Programmatic Tool ...",
        "source": "web"
      },
      {
        "title": "Get the Cheat Code on Long-Running AI Agents—Here's What ...",
        "url": "https://natesnewsletter.substack.com/p/i-read-everything-google-anthropic",
        "snippet": "The sub-agent works in its own context and returns a structured result. For complex tasks with shared state, the planner shares full context ...",
        "source": "web"
      }
    ],
    "object": "chat.completion",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "# Automatic Context Injection in LLM Agent Workflows: Production-Grade Patterns and Best Practices\n\nThe modern challenge of building production-grade AI agents centers on a critical architectural problem: how to provide language models with precisely relevant context at the moment of inference without incurring excessive latency costs or requiring explicit agent decision-making. Rather than forcing agents to deliberate about which context to load—consuming valuable inference cycles and token budgets—production systems increasingly employ sophisticated automatic context injection patterns that pre-load state before the language model even begins reasoning. This comprehensive report examines the evolution of these patterns from 2024 through early 2026, synthesizing approaches from industry leaders including Google, Anthropic, and emerging coding tool vendors, while establishing a foundational understanding of how zero-latency, zero-agency-cost context delivery has become the defining architectural requirement for agents operating at meaningful scale.\n\n## The Fundamental Problem: Context as Architecture, Not Configuration\n\nModern LLM agents face an acute constraint that grows more pressing with each additional inference step taken: context windows, despite expanding dramatically from thousands to millions of tokens, become less effective at supporting complex reasoning as they accumulate irrelevant information.[30] Research demonstrates that models experience measurable performance degradation as context length increases beyond optimal thresholds, with some studies showing 15-30% accuracy reductions when contexts exceed 100,000 tokens[49], and performance continuing to degrade even as context windows grow substantially larger.[54] This phenomenon stems not from memory limitations but from fundamental architectural constraints of the transformer model—every token competes for the model's attention through n² pairwise relationships, creating what researchers term an \"attention scarcity\" problem.[54] The implication is profound: simply expanding context windows offers no solution. Instead, building capable agents requires treating context as a first-class architectural system with its own lifecycle, compilation process, and retrieval mechanisms, separate from storage.[30][13]\n\nThe distinction between context and memory has become central to how organizations architect production agents. Context refers to tokens actually present in the model's input at inference time—the information the model currently \"sees\" and can directly reason about.[30][54] Memory, by contrast, refers to persistent storage outside the context window where long-term information lives—conversation history, learned patterns, completed task summaries, and reference materials.[30] In early agent implementations, teams commonly fell into what practitioners call the \"context dumping\" trap: placing large payloads directly into chat history, whether 5MB CSVs, massive JSON responses, or full document transcripts.[30][13] Each subsequent turn then carries this payload forward, creating a permanent tax on the session that buries critical instructions and inflates costs.[30]\n\nThe organizations building the most capable agents—Google's ADK framework, Anthropic's Claude Code, the team behind Manus—have converged on a shared architectural insight: context should be a computed projection rather than a raw accumulation. The session remains the ground truth repository for all events and interactions, while the working context is a dynamically constructed view of the session, continuously refined through explicit processors and filters.[30] This separation enables both efficiency and reliability. A session can contain thousands of raw events; the working context presented to the model might contain only hundreds of carefully selected tokens. The result is what Google's infrastructure team calls the \"context engineering discipline\"—treating context management with the same rigor applied to storage and compute in traditional systems engineering.[30]\n\n## Pre-hooks and Automatic State Injection: The Execution Boundary Pattern\n\nOne of the most powerful patterns that has emerged for zero-latency context injection is the pre-hook or pre-execution hook architecture, most thoroughly documented through Claude Code's implementation but increasingly adopted across coding tools and agent frameworks.[8][11] Pre-hooks execute at the very beginning of an agent run, giving complete control over what reaches the language model before any reasoning begins.[6][8] Unlike tool calls—which consume tokens, require model decisions, and contribute to inference latency—pre-hooks are deterministic, programmatic, and execute outside the model's inference loop entirely.[8]\n\nClaude Code's hooks system provides the clearest implementation of this pattern.[11] The system monitors Claude Code's actions and matches them against rules defined in configuration files. When a match occurs, specified commands run with full access to context about what is about to happen.[8] The flow is straightforward but powerful: when the agent is invoked (or when resuming an existing session), the SessionStart hook fires before the model is invoked, allowing applications to populate state, initialize context variables, and prepare the information landscape before any reasoning begins.[11] This occurs outside model inference time, incurring zero latency cost to the model's decision-making process.\n\nThe JSON input that pre-hooks receive contains crucial contextual information: session_id identifying the current conversation, transcript_path pointing to conversation history, cwd showing the working directory, and hook_event_name indicating which event triggered execution.[8] This allows hooks to make intelligent decisions about what state to prepare. A preprocessing hook might analyze the working directory structure, identify relevant project files without loading them into context, and prepare file paths and metadata that the model can reference without token overhead. Critically, pre-hooks can write structured information to the model's context through stdout—they can format and present information in ways optimized for the model's understanding.[8]\n\nExit codes from pre-hooks determine behavior: exit code 0 signals success and allows execution to continue, while non-zero codes can block subsequent execution.[8] This allows pre-hooks to serve not just as context injection points but as security and validation boundaries. A pre-hook could verify that certain files should not be modified, that database connections are available, or that the agent has appropriate permissions, all before the model begins reasoning about what to do.\n\nThe architectural advantage of pre-hooks extends beyond latency. Because they run outside the model's inference loop, they do not consume tokens in the context window. Information prepared by pre-hooks reaches the model through structured channels—file system references, environment variables, state objects—rather than as raw text stuffed into the prompt.[8][11] This enables what might be called \"reference-based context\" rather than \"content-based context.\" The model knows which files exist, their modification times, their sizes, and their paths without those files consuming tokens unless the model explicitly decides to read them.[33][54]\n\nAnother layer of sophistication in pre-hook patterns involves conditional context preparation. Different tasks require different context. A pre-hook can parse the incoming user request (before sending it to the model) to determine which context to prepare. For instance, if a user asks a question about a specific project file, a pre-hook could identify that file, prepare its path, and potentially stage relevant metadata or neighboring files—all before the model sees anything.[8] This represents true zero-latency context matching: the decision about what context matters is made before inference, not during it.\n\n## Model Context Protocol (MCP): From Tool Calls to Resource-Based Auto-loading\n\nThe Model Context Protocol, released as open-source by Anthropic in November 2024 and donated to the Agentic AI Foundation in December 2025, represents a fundamental shift in how context becomes available to agents.[25][28] MCP provides a standardized protocol for connecting agents to external systems, tools, and data sources.[2][5][53] However, what makes MCP particularly relevant to context injection patterns is its distinction between tools and resources—a distinction that directly addresses the zero-latency, zero-agency-cost context challenge.\n\nResources in MCP are designed as application-controlled primitives that expose data and content readable by clients without requiring explicit tool calls from the model.[2][5] Unlike traditional tool calling where every data access decision goes through the model—consuming tokens, requiring inference, and adding latency—resources can be automatically loaded by the host application based on application logic, user selection, or intelligent discovery mechanisms.[2][19] MCP servers expose resources via URIs following standard formats like file:// for filesystem resources, postgres:// for database schemas, or custom schemes defined by server implementations.[2][5][19]\n\nThe critical distinction is application-driven versus model-driven context. MCP resources are fundamentally designed to be application-controlled, meaning the client application decides how and when resources should be used.[2] Different MCP clients implement this differently: Claude Desktop currently requires users to explicitly select resources before use, other clients might automatically select resources based on heuristics, and some implementations allow the model itself to determine which resources to use.[2] This flexibility enables zero-agency patterns where the application makes intelligent context decisions without involving the model.\n\nResource discovery in MCP operates through two complementary mechanisms: direct resources where servers expose a concrete list of available resources (like a database schema or file directory), and resource templates where servers define parameterized resources that clients can instantiate dynamically.[2][5] For context injection, the templates pattern proves particularly powerful. A server might expose a template like `file:///{path}` that allows the client application to construct specific resource URIs without the model needing to reason about file paths. The application's logic determines which instances of that template to resolve into actual resources and load into context.[2][5]\n\nWhen resources are read, servers respond with contents that can be either text or binary data (base64-encoded).[2][5][19] This standardization enables true interoperability—any MCP-compliant client can work with any MCP-compliant server without custom integration code. More importantly for context injection, resources can be subscribed to, with servers notifying clients when resource contents change through the `resources/subscribe` and `notifications/resources/updated` mechanism.[2][5] This enables reactive context updates: when external state changes, the agent's context automatically refreshes without explicit model decision-making or tool calls.\n\nThe evolution of MCP toward advanced tool management patterns like Tool Search Tool represents a significant advancement in reducing context overhead while maintaining access to capability.[59] Rather than loading all tool definitions upfront (which can consume 55,000+ tokens or more for comprehensive MCP servers), the Tool Search Tool allows agents to discover tools on-demand through search operations.[59] Tools marked with `defer_loading: true` remain undescribed in initial context, with only the search capability loaded upfront (consuming approximately 500 tokens).[59] When the model determines it needs a tool for a task, it searches for relevant tools, and only then are specific tool definitions loaded—usually only 3-5 tools per task, consuming roughly 3,000 tokens total.[59] This approach reduces context overhead from 77,000+ tokens to approximately 8,700 tokens, preserving 95% of the context window while maintaining access to the entire tool library.[59] Internal testing showed dramatic accuracy improvements: Claude Opus 4 improved from 49% to 74% accuracy, and Opus 4.5 from 79.5% to 88.1% when Tool Search Tool was enabled.[59]\n\n## Programmatic Tool Calling and Code Execution: The Execution Environment as Context Filter\n\nA complementary but distinct pattern to resource-based auto-loading is programmatic tool calling (PTC), where agents generate and execute code within a sandboxed execution environment rather than making direct tool calls through the model.[28][59] This pattern directly addresses the context pollution problem: intermediate results from tools never enter the model's context unless explicitly returned by the code.[28]\n\nAnthropic documented this pattern in the context of code execution with MCP servers.[28] Rather than loading all MCP tool definitions and having the model make direct calls, agents write code that interacts with MCP servers as code APIs. The agent discovers tools by exploring the filesystem, reading specific tool definitions it needs, and then writing code to orchestrate calls to those tools. The crucial difference: intermediate results stay in the execution environment by default, with only final summaries or critical outputs returned to the model.[28] This enables dramatic reductions in token consumption—one documented case reduced token usage from 150,000 tokens to 2,000 tokens, representing a 98.7% reduction in context overhead.[28]\n\nThe benefits compound across different task dimensions. First, context efficiency: agents load only the tools they need for current tasks, filtering results before they reach the model. Second, reduced latency: complex logic like loops, conditionals, and error handling can be expressed in code rather than alternating between model inference and tool calls. A deployment notification in Slack that requires checking multiple conditions before sending becomes a single code block rather than multiple sequential model calls, each waiting for inference to complete.[28] Third, privacy preservation: sensitive data can flow through code execution without ever entering the model's context. For example, importing customer contact details from a spreadsheet into Salesforce can be accomplished without customer data ever appearing in the model's working memory.[28]\n\nThis pattern requires infrastructure—sandboxed code execution environments with appropriate resource limits, monitoring, and security controls—which adds operational complexity. However, the efficiency gains justify this overhead for agents handling production workloads.[28] The architectural insight is that agents should leverage their strengths: LLMs excel at code generation and high-level reasoning. By delegating code execution and data processing to specialized execution environments, agents maintain clean context for reasoning while accomplishing complex tasks efficiently.\n\n## Tool-Specific Context Injection: Implementation Patterns from Cursor, Windsurf, and Cline\n\nThe emergence of AI-powered coding tools has driven significant innovation in practical context injection patterns. These tools operate in constrained environments (code editors) with specific structural requirements, making them ideal proving grounds for zero-latency context patterns. Examination of how Cursor, Windsurf, Cline, and Claude Code handle project context reveals distinct architectural philosophies with important tradeoffs.\n\nClaude Code's approach represents perhaps the most thoroughly documented pattern. Claude Code implements a hybrid strategy for context availability: some data (like CLAUDE.md procedural memory files) is proactively dropped into context upfront for speed, while other data access patterns (like glob and grep commands for file discovery) enable just-in-time autonomous retrieval.[33][46] This hybrid model recognizes that the \"right\" level of autonomy depends on the specific task. For structured metadata about a project (team conventions, architecture decisions, active work), upfront loading ensures immediate availability. For exploratory data access (discovering which files might contain relevant information), autonomous discovery prevents stale indexing problems and complex syntax tree updates.[33][46]\n\nThe CLAUDE.md file serves a specific architectural role: it acts as procedural memory, loaded at session start but not constantly maintained in the context window.[29][33] This addresses a key tension in agent design—how to maintain coherence about project structure and conventions without bloating context through the entire interaction. By treating CLAUDE.md as an optional reference that the agent can consult (rather than keeping all information constantly visible), Claude Code achieves a balance between continuity and context efficiency.\n\nCursor operates from a different architectural philosophy, prioritizing speed-optimized autocomplete over agent autonomy.[7][10] Cursor integrates deep project context through cached codebase indexing rather than just-in-time file loading. The tool maintains awareness of entire projects through proprietary infrastructure while exposing functionality through natural language queries and inline completions.[10] Cursor's approach emphasizes that context injection need not be visible to the model—the context can be managed entirely by infrastructure, with the model interacting with pre-processed, indexed information rather than raw codebase content.[10]\n\nWindsurf introduces rule-based context customization.[10] Through Windsurf rules, developers can specify how the AI should behave when writing code or generating UI, effectively encoding project conventions and patterns as rules that guide context assembly. This represents a different form of automatic context injection: rather than loading project files or architectural documents, context is shaped through behavioral rules that guide the model's reasoning process.[10] The rules themselves become contextual information—they tell the model \"in this project, we use hexagonal architecture\" not through documentation but through executable constraints on code generation.\n\nCline, the experimental multi-agent development tool, leverages Model Context Protocol integration for extensible context management.[7] By supporting MCP natively, Cline enables custom tool connections and workflow automation without vendor lock-in limitations.[7] Cline's architecture allows developers to define custom MCP servers that expose project-specific context through standardized interfaces, enabling sophisticated context injection patterns tailored to specific development workflows while remaining portable across different environments.\n\nThe common pattern across these tools is progressive disclosure: context is revealed to the model incrementally as needed, rather than dumped into prompts upfront. File sizes suggest complexity; naming conventions hint at purpose; timestamps serve as proxies for relevance.[33][46] Each interaction yields context that informs the next decision, allowing agents to assemble understanding layer by layer while maintaining only what's necessary in working memory.[33][46] This mirrors human cognition: people don't memorize entire corpora; instead, they use external organization systems (file systems, bookmarks, indexes) to retrieve relevant information on demand.\n\n## Memory Architecture: Separating Context, Storage, and State\n\nProduction agent systems increasingly implement sophisticated memory architectures that clearly delineate what information belongs where. This separation is not mere organizational preference—it is a fundamental enabler of scalability and reliability.[30][29][51] The typical architecture distinguishes between four layers, each with distinct storage, access patterns, and lifecycle characteristics.\n\nThe first layer is working context—the tokens actually present in the model's current inference call. This layer contains only information directly relevant to the current decision point: recent conversation turns, immediate task objectives, active error states, current constraints, and immediately relevant facts.[29][30][51] Working context is transient by design; it's recreated for each inference call, compiled from the layers below based on what's currently relevant.[30] The working context is where context engineering concentrates its effort—every token in this layer must earn its presence through clear relevance to the current decision.\n\nThe second layer is session state—the ground truth record of everything that has happened in this particular conversation or interaction thread.[30][13] The session accumulates all raw events: model calls, tool executions, user messages, intermediate results, errors and recoveries.[30][13] Unlike working context, the session is persistent within a conversation thread and immutable (events are append-only, never modified after creation). The session is massive compared to working context—it might contain hundreds of thousands of tokens. But the session is not directly shown to the model; instead, processors compile the session into working context.[30][13]\n\nThe third layer is persistent memory, stored outside any particular session—conversation summaries, learned patterns about the user, completed task descriptions, preferences, and architectural decisions that should carry forward across multiple sessions.[29][30][33][51] This layer enables agents to maintain continuity across time, remembering who the user is, what they've done before, what went wrong previously, and what patterns have worked well. Persistent memory is typically stored in managed data systems—databases, vector stores, or file systems—where enterprise security controls, backup policies, and access auditing can be applied.[30][45]\n\nThe fourth layer is artifacts—substantial persistent objects that exist outside the context window but remain directly accessible through reference.[30][33] Artifacts are the answer to the \"context dumping\" problem. Rather than putting a 5MB CSV or a massive JSON response into the context, the artifact layer stores these objects externally and represents them in context through references: file paths, URLs, database query parameters, or storage identifiers.[30] An agent can write intermediate results to artifacts (like the notes files in Claude Code, or task-tracking documents in other systems) and reference those artifacts in context without those contents consuming tokens unless explicitly retrieved.[30][33]\n\nThis four-layer memory architecture enables several critical capabilities. First, progressive contextualization: as a session progresses, the system can selectively pull information from persistent memory into working context when relevant, as determined by retrieval triggers or explicit agent requests. Second, session resets without information loss: when working context reaches capacity, the system can compress old session events into summaries, store those summaries in persistent memory, and continue with fresh working context—the agent maintains coherence across this boundary through references to stored information.[30][46] Third, multi-agent communication without context explosion: when one agent hands work to another, the system can selectively pass only the relevant context subset to the sub-agent, keeping the full history in session storage, preventing sub-agents from being overwhelmed by irrelevant information from previous agent steps.[30][13]\n\nGoogle's ADK framework provides concrete implementation details for this architecture.[30][13] Every LLM-based agent is backed by an LLM Flow, which maintains ordered lists of processors. Request processors run before model inference, progressively building the model's context window by filtering and formatting information from session storage. Response processors run after model generation, potentially storing results, updating session state, or triggering side effects. This pipeline architecture means that context composition is explicit and modular—each processor handles one aspect of context preparation, making the entire flow auditable and modifiable.[30][13]\n\n## Context Compaction and Summarization: Managing Unbounded Growth\n\nAs agents run for longer periods, accumulating hundreds or thousands of interaction steps, even append-only session logs threaten to overwhelm available context. The standard response is context compaction—using the model itself to summarize older session events into condensed summaries, then removing the raw events from session storage.[30][33][46] Claude Code implements this pattern by passing the message history to the model to summarize and compress, preserving architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs or messages.[33][46] The compressed session plus the five most recent message turns becomes the new session history, allowing the agent to continue with full coherence.\n\nCompaction introduces subtle architectural requirements. First, compression must be lossless with respect to critical information—losing information about a failed debugging approach or an important architectural decision breaks agent coherence. Second, compaction boundaries matter for caching. As agents move toward relying on prompt caching to reduce inference costs, compaction events should occur at natural boundaries where cache invalidation is acceptable.[30][33] Third, frequency of compaction affects memory growth—too frequent and the model spends excessive inference on summarization; too infrequent and context still bloats. Google's ADK framework triggers compaction asynchronously when a configurable threshold is reached, typically based on the number of invocations rather than raw token count, providing predictability.[30]\n\nAn alternative to compaction for preserving long-horizon coherence is structured note-taking, or agentic memory.[33][46] Rather than having the model summarize historical events, the agent explicitly writes structured notes to persistent memory as it progresses. Claude Code creates to-do lists and project notes; other agents maintain NOTES.md files or structured memory objects.[33][46][33] These notes capture key decisions, pending work, discovered patterns, and current state. Unlike compaction (which is pushed onto the model as a task), note-taking is pull-based—the agent decides what to record and when. This can be more efficient for tasks with clear milestones where the agent naturally identifies what matters. The notes then become part of persistent memory, automatically loaded into context at the start of future sessions or retrieved when relevant during long-running sessions.\n\nA third approach, increasingly practical with larger context windows, is multi-agent architectures where specialized sub-agents handle focused tasks with clean context windows.[30][33][46] Rather than one agent accumulating context through an entire project migration or research task, specialized sub-agents can handle specific aspects (research, analysis, implementation, validation) with fresh context windows for each. The main agent orchestrates by providing high-level direction, while sub-agents work independently and return condensed summaries. Each sub-agent might use tens of thousands of tokens, but the coordination agent receives only 1,000-2,000 token summaries, maintaining clean coordination context.[30][33][46]\n\n## Prompt Caching: Optimizing the Cost of Repeated Context\n\nPrompt caching has emerged as a critical infrastructure optimization for agents, particularly for long-running workflows where context windows contain substantial static content that reoccurs across multiple inference calls.[21][24][38] Modern LLM providers—Anthropic, OpenAI, and Google—now offer prompt caching, a feature that reuses previously computed key-value (KV) tensors from attention layers to avoid redundant computation on repeated prompt prefixes.[21][24][38][41]\n\nThe fundamental mechanism is straightforward: when the model processes a prompt during the prefill phase, it computes Key and Value tensors for all input tokens, then caches these tensors. For subsequent requests that begin with identical content (the same system prompt, the same tool definitions, the same project documentation), the system can skip recomputing those tensors and reuse the cached results. This reduces both time-to-first-token (TTFT) latency and API costs.[21][24][38]\n\nHowever, prompt caching effectiveness depends critically on strategic cache boundary control.[38][41] Naive full-context caching—attempting to cache everything—often provides no benefit or even decreases performance.[38] The reason is that many elements of agent context change between requests: the user's current query, timestamp information, tool results from the previous step, or dynamic content. If the cache boundary includes these dynamic elements, the cache misses on every subsequent request, wasting computational cycles on cache operations.[38][41] Strategic cache boundaries instead place stable, reusable content (system prompts, tool definitions, reference documentation) at the front of the context and push dynamic content (current queries, recent tool results) toward the end.[38][41][39]\n\nResearch on prompt caching in agentic workloads reveals nuanced findings.[38][41] System-prompt-only caching—caching just the system prompt and tool definitions while leaving tool results out of the cache—provides the most consistent benefits across both cost and latency dimensions.[38] Strategic exclusion of dynamic tool results prevents wasted cache writing for content that won't be reused. For Claude models, Anthropic offers simplified cache management where developers place a single cache checkpoint at the end of static content; the system automatically finds the longest matching prefix from the cache.[24] Multi-level caching with up to four cache breakpoints allows more granular control for content that changes at different frequencies.[24][39]\n\nPrompt caching requirements vary by model. Most Claude models require a minimum of 1024 tokens before caching activates; Haiku variants require 2048 tokens.[39] Cache lifetime defaults to five minutes but can be extended to one hour at additional cost.[24][39] The trade-off is intentional: five-minute caching auto-refreshes whenever the context is reused, providing cost-free refreshes for frequently accessed content, while one-hour caching is appropriate for prompts used occasionally but still within an extended session.[24]\n\nPractical implementation of prompt caching in agentic contexts requires design discipline. Context should be structured with tools and system instructions first (stable, highly reusable), followed by example patterns or reference material that changes less frequently, and finally dynamic content like user queries and tool results.[39][42] For multi-turn conversations, previous conversation turns can be cached, with only the new user query falling outside the cache boundary.[39] This enables rapid response in follow-up turns without recomputing attention over the entire conversation history.[39]\n\nThe interaction between prompt caching and KV cache offloading introduces additional complexity for infrastructure teams managing long-context agentic workloads. In batch processing scenarios where multiple agents execute against the same base infrastructure, intelligent scheduling can exploit prefix caching to reduce total computational load.[57] When consecutive operators share the same base model and identical prompt prefixes, KV cache reuse is possible, allowing the system to amortize the prefill phase latency across multiple operators.[57] This represents true systems-level optimization where context engineering intersects with infrastructure design.\n\n## Context Minimization and Dynamically Scoped Information\n\nOne particularly elegant pattern for preventing certain classes of prompt injection attacks while simultaneously maintaining clean agent context is the context-minimization pattern.[1][4] The idea is to constrain what information the model sees at each stage by explicitly removing unnecessary content from context over the interaction lifecycle.[1][4]\n\nThe classic example involves a customer service chatbot where a user attempts prompt injection by requesting an unauthorized discount. The system could ensure that the agent first translates the user's request into a database query (finding current offers available), then before returning results to the customer, explicitly removes the user's original prompt from the context.[1][4] The model sees the results of its own structured query but never sees the untrusted user text again, making it impossible for that text to trigger subsequent harmful actions.[1][4]\n\nThis pattern combines security benefits with context efficiency. By removing processed input after it has served its purpose, the agent maintains clean context focused on current decision-making rather than accumulating historical text that might pollute reasoning. It's a form of intentional forgetting—the agent deliberately discards information that is no longer relevant to the current task.\n\nA related pattern is the action-selector pattern where agents trigger external actions based on structured decisions but cannot observe the results of those actions directly.[1][4] An agent might decide to \"send the user to this webpage\" or \"display this message\" without being exposed to or acting on responses from those actions.[1][4] This prevents tool responses from re-entering context where they might be misinterpreted or cause unintended behavior changes. The agent remains in a clear decision-making state without the noise of side effect responses.\n\n## Production-Grade Best Practices and Architectural Decisions\n\nBuilding agents that reliably operate in production requires systematic attention to several interconnected architectural decisions, all grounded in how context gets injected, managed, and retired.\n\nFirst, establish explicit task boundaries with context isolation.[27] Different tasks should receive different context. When a user switches from creating a quality plan to maintenance scheduling, that task switch should trigger fresh context loading—not carrying forward the quality plan context that might confuse reasoning about maintenance tasks.[27] This requires explicit boundaries in the agent orchestration layer, not just in conversation flow. Task-isolated context loading prevents cross-contamination and maintains logical separation between problem domains.\n\nSecond, implement goal-oriented completion signals rather than open-ended conversations.[27] Agents should know when they've succeeded, rather than continuing to offer help indefinitely. This is partly about user experience but also about context management: when a task completes, cleanup becomes possible. Intermediate results can be discarded, context can be reset for the next task, and caches can be invalidated at meaningful boundaries.[27]\n\nThird, validate context assembly through tracing and observability.[29][45] Don't assume that context assembly is correct—audit what's actually in the model's context window at each step. Use tracing systems to capture the full context window state, allowing post-hoc analysis of failures or performance degradation.[29][45] When agents produce unexpected results, the ability to see exactly what context they were working with is invaluable for diagnosis.\n\nFourth, implement explicit human oversight escalation points rather than relying on the model to recognize when human input is needed.[33][46] Agents should have structured mechanisms for flagging decisions that exceed their authority or confidence, explicitly escalating to humans rather than attempting all tasks.[33][46] This separation prevents context bloat from attempted recovery of failed decisions and maintains clear accountability boundaries.\n\nFifth, treat context engineering as an iterative discipline.[29] Start with static prompts and straightforward context loading. Add dynamics only when needed. Monitor metrics like token usage, latency, and accuracy to identify whether context engineering changes provide measurable improvement or add complexity without benefit.[29] Use metrics like hallucination detection and retrieval quality to validate that the right information is reaching the model at the right times.[29]\n\nSixth, design memory retention policies explicitly.[51] Don't accumulate all conversation history indefinitely. Define what information should persist beyond sessions (preferences, learned patterns, architectural decisions), what should summarize (completed tasks, resolved issues), and what should discard (individual error messages, failed intermediate attempts).[51] Forgetting is not a bug—it's essential for preventing context quality degradation over time.[51]\n\n## The Emerging Convergence: Skills, MCP, and Resource-Based Architectures\n\nLooking across the 2025-2026 evolution of these patterns, a clear architectural convergence is emerging around resource-based, skill-oriented agent design.[25][56] Rather than loading tool definitions upfront or having models deliberate about context acquisition, the emerging standard treats capabilities and context as resources discoverable and loadable on demand through standardized protocols.\n\nAnthropic's recent Skills framework represents this evolution: Skills are folders of instructions, scripts, and resources that agents can discover and load dynamically, with significantly lower token overhead than comprehensive tool definitions.[25] While Skills and MCP connections initially appeared as opposite approaches—MCP connections can consume tens of thousands of tokens loading all tool information, while Skills are lightweight and load on demand—the actual evolution shows they're overlapping rather than opposed.[25] The architectural insight both embody is the same: deferred, on-demand loading of capabilities beats upfront comprehensive loading.\n\nThe donation of MCP to the Agentic AI Foundation under the Linux Foundation in December 2025, with OpenAI, Block, AWS, Google, Microsoft, Cloudflare, and Bloomberg as founding members, signals that this is becoming infrastructure-level standardization.[25][28] The implication is that by 2026, the standard agent architecture will not involve custom integrations or monolithic tool lists. Instead, agents will compose capabilities from modular, discoverable, standardized resources accessed through MCP-like protocols, with intelligent application-level decisions about what resources to load when.\n\nThis represents a fundamental shift from agents that ask \"what should I do?\" to agents that are simply presented with capability context shaped by intelligent infrastructure. The model's context becomes a projection of available capabilities filtered by the application's understanding of what's relevant. Context injection becomes not about the model deciding what it needs, but about the application intelligently determining what would be helpful.\n\n## Conclusion: Context as the Defining Architectural Challenge of 2026\n\nThe evolution from prompt engineering to context engineering represents a maturation in how organizations build agents. The focus has shifted from \"how should I write prompts?\" to \"how should I architect the information landscape agents operate within?\" This shift is not incremental—it's fundamental. It changes where value is created (in infrastructure decisions, not model selection), where optimization happens (in context architecture, not model fine-tuning), and what skills matter (systems thinking and information architecture, not prompt writing).\n\nThe patterns discussed throughout this report—pre-hooks that inject state before inference, Model Context Protocol resources that enable standardized access without tool calls, memory architectures that separate context from storage, prompt caching that optimizes repeated prefixes, context minimization that prevents pollution, and skill-based capability loading that defers expensive definitions—are not independent techniques. Rather, they form a coherent architectural discipline where each addresses specific bottlenecks in the broader problem of maintaining clean, relevant context across extended agent operation.\n\nProduction systems in 2026 will increasingly treat context management with the same formality and investment applied to database design or API architecture in traditional systems. Organizations that recognize this architectural priority and invest in context engineering infrastructure will build agents capable of reliable, extended reasoning over complex tasks. Those that treat context as incidental will find their agents hitting fundamental limitations—not from model capability, but from architectural constraints that better engineering can solve.\n\nThe best practices for automatic context injection ultimately converge on a single principle: separate decision-making from context preparation, and make context preparation systematic and deterministic. Rather than asking the model \"what context do you need?\" at each step, ask \"what context should be available based on what we know about this task?\" Implement that decision programmatically through pre-hooks, resource-based protocols, and structured memory architectures. The result is agents that maintain focus and coherence because the context they operate within has been carefully engineered to support exactly the reasoning needed for the task at hand."
        },
        "delta": {
          "role": "assistant",
          "content": ""
        },
        "finish_reason": "stop"
      }
    ]
  }
}