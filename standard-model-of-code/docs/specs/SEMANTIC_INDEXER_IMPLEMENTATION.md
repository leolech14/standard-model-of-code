# Semantic Indexer Implementation Plan

> PURPOSE = f(edges) - Building state-of-the-art semantic code analysis

**Created:** 2026-01-23
**Sources:** Gemini 2.0 Flash analysis, Perplexity research (60 citations)
**Validation:** 85% confidence from PURPOSE_ONTOLOGY framework

---

## Executive Summary

Build a 4-phase semantic indexer that determines code purpose through relationships, not content. Integrates with existing Collider pipeline.

| Phase | Component | Effort | New Files |
|-------|-----------|--------|-----------|
| 1 | Graph Framework + Basic Metrics | Low | `graph_framework.py` |
| 2 | Top-Down Context Propagation | Medium | Extend `graph_framework.py` |
| 3 | Advanced Centrality Metrics | Low | `graph_metrics.py` |
| 4 | Multi-Modal Intent Extraction | Medium | `intent_extractor.py` |

---

## Phase 1: Graph Framework (Foundation)

### New File: `src/core/graph_framework.py`

```python
"""Graph-based semantic analysis framework.

Implements PURPOSE = f(edges) by building NetworkX graphs from
Collider's unified_analysis.json and computing relationship metrics.
"""
import networkx as nx
from typing import Dict, List, Any

def build_nx_graph(nodes: List[Dict], edges: List[Dict]) -> nx.DiGraph:
    """Build NetworkX DiGraph from Collider analysis output.

    Args:
        nodes: List of node dicts with 'id' and attributes
        edges: List of edge dicts with 'source', 'target', and attributes

    Returns:
        Directed graph with all node/edge attributes preserved
    """
    G = nx.DiGraph()
    for node in nodes:
        G.add_node(node['id'], **node)
    for edge in edges:
        G.add_edge(edge['source'], edge['target'], **edge)
    return G

def compute_degree_metrics(G: nx.DiGraph) -> Dict[str, Dict[str, int]]:
    """Compute in-degree and out-degree for each node.

    High in-degree = utility (called by many)
    High out-degree = orchestrator (calls many)
    """
    return {
        node: {
            'in_degree': G.in_degree(node),
            'out_degree': G.out_degree(node)
        }
        for node in G.nodes
    }

def classify_node_role(in_degree: int, out_degree: int) -> str:
    """Classify architectural role from degree metrics.

    Based on PURPOSE_ONTOLOGY research:
    - High in, low out = UTILITY
    - Low in, high out = ORCHESTRATOR
    - High both = HUB
    - Low both = LEAF
    """
    high_in = in_degree > 5
    high_out = out_degree > 5

    if high_in and not high_out:
        return "utility"
    elif high_out and not high_in:
        return "orchestrator"
    elif high_in and high_out:
        return "hub"
    else:
        return "leaf"
```

### Integration: `src/core/full_analysis.py`

Add after Stage 3 (edge extraction):

```python
# Stage 3.1: Build Semantic Graph
from graph_framework import build_nx_graph, compute_degree_metrics, classify_node_role

print("\nðŸ“Š Stage 3.1: Building Semantic Graph...")
G = build_nx_graph(nodes, edges)
degree_metrics = compute_degree_metrics(G)

# Enrich nodes with relationship-based purpose
for node in nodes:
    if node['id'] in degree_metrics:
        metrics = degree_metrics[node['id']]
        node['in_degree'] = metrics['in_degree']
        node['out_degree'] = metrics['out_degree']
        node['semantic_role'] = classify_node_role(
            metrics['in_degree'],
            metrics['out_degree']
        )
print(f"âœ… Stage 3.1: {len(G.nodes)} nodes, {len(G.edges)} edges analyzed")
```

---

## Phase 2: Top-Down Context Propagation

### Extend: `src/core/graph_framework.py`

```python
from collections import deque

def propagate_context(
    G: nx.DiGraph,
    root_nodes: List[str],
    max_depth: int = 10
) -> Dict[str, Dict]:
    """Propagate context via BFS from root nodes downstream.

    Implements cognitive research finding: experts understand code
    top-down, with parent context informing child interpretation.

    Args:
        G: Code graph
        root_nodes: Entry points (main, routes, CLI)
        max_depth: Maximum propagation depth

    Returns:
        Dict mapping node_id to enriched context
    """
    node_context = {}
    queue = deque([(node, 0, {}) for node in root_nodes])
    visited = set()

    while queue:
        node_id, depth, parent_context = queue.popleft()

        if node_id in visited or depth > max_depth:
            continue
        visited.add(node_id)

        # Get node data
        node_data = dict(G.nodes[node_id]) if node_id in G.nodes else {}

        # Build context with parent info
        context = {
            **node_data,
            'parent_context': parent_context,
            'depth_from_entry': depth
        }
        node_context[node_id] = context

        # Propagate to callees
        for callee in G.successors(node_id):
            child_context = {
                'parent_id': node_id,
                'parent_role': node_data.get('semantic_role', 'unknown'),
                'call_chain_depth': depth + 1
            }
            queue.append((callee, depth + 1, child_context))

    return node_context

def find_entry_points(G: nx.DiGraph) -> List[str]:
    """Find likely entry points (nodes with 0 in-degree or special names)."""
    entry_points = []

    for node in G.nodes:
        # Zero in-degree = nothing calls it = entry point
        if G.in_degree(node) == 0:
            entry_points.append(node)
        # Common entry point patterns
        elif any(p in node.lower() for p in ['main', 'cli', 'app', 'server', 'index']):
            entry_points.append(node)

    return list(set(entry_points))
```

---

## Phase 3: Advanced Centrality Metrics

### New File: `src/core/graph_metrics.py`

```python
"""Advanced graph metrics for semantic purpose detection.

Based on Perplexity research (60 citations):
- Betweenness: Critical infrastructure (bridges)
- Closeness: Coordination points (reach many quickly)
- Eigenvector: Important neighbors matter
- PageRank: Influence propagation
"""
import networkx as nx
from typing import Dict, Any

def compute_centrality_metrics(G: nx.DiGraph) -> Dict[str, Dict[str, float]]:
    """Compute multiple centrality measures for architectural analysis.

    Returns dict mapping node_id to centrality scores.
    """
    metrics = {}

    # Betweenness: bridges between components
    betweenness = nx.betweenness_centrality(G, normalized=True)

    # Closeness: ability to reach others quickly
    try:
        closeness = nx.closeness_centrality(G)
    except:
        closeness = {n: 0.0 for n in G.nodes}

    # PageRank: influence propagation
    try:
        pagerank = nx.pagerank(G, alpha=0.85)
    except:
        pagerank = {n: 1.0/len(G.nodes) for n in G.nodes}

    for node in G.nodes:
        metrics[node] = {
            'betweenness': betweenness.get(node, 0.0),
            'closeness': closeness.get(node, 0.0),
            'pagerank': pagerank.get(node, 0.0)
        }

    return metrics

def identify_critical_nodes(
    G: nx.DiGraph,
    metrics: Dict[str, Dict[str, float]],
    top_n: int = 10
) -> Dict[str, List[str]]:
    """Identify architecturally significant nodes by metric.

    Returns:
        Dict with 'infrastructure', 'coordinators', 'bridges' lists
    """
    # Sort by each metric
    by_betweenness = sorted(
        metrics.items(),
        key=lambda x: x[1]['betweenness'],
        reverse=True
    )[:top_n]

    by_pagerank = sorted(
        metrics.items(),
        key=lambda x: x[1]['pagerank'],
        reverse=True
    )[:top_n]

    by_closeness = sorted(
        metrics.items(),
        key=lambda x: x[1]['closeness'],
        reverse=True
    )[:top_n]

    return {
        'bridges': [n[0] for n in by_betweenness],      # High betweenness
        'influential': [n[0] for n in by_pagerank],     # High pagerank
        'coordinators': [n[0] for n in by_closeness]    # High closeness
    }

def detect_clusters(G: nx.DiGraph) -> List[set]:
    """Detect cohesive modules via community detection.

    Clusters = groups with dense internal, sparse external connections.
    """
    # Convert to undirected for community detection
    G_undirected = G.to_undirected()

    try:
        from networkx.algorithms.community import louvain_communities
        communities = louvain_communities(G_undirected)
        return [set(c) for c in communities]
    except:
        # Fallback: connected components
        return [set(c) for c in nx.connected_components(G_undirected)]
```

---

## Phase 4: Multi-Modal Intent Extraction

### New File: `src/core/intent_extractor.py`

```python
"""Multi-modal intent extraction from code, commits, docs.

Research finding: "Developer intent is not fully captured by code alone"
(WSDM 2025). Must extract from multiple sources.
"""
import subprocess
import re
from pathlib import Path
from typing import Dict, List, Optional

def extract_readme_intent(repo_path: Path) -> Optional[str]:
    """Extract high-level purpose from README."""
    readme_patterns = ['README.md', 'README.rst', 'README.txt', 'README']

    for pattern in readme_patterns:
        readme_path = repo_path / pattern
        if readme_path.exists():
            content = readme_path.read_text()[:2000]  # First 2K chars
            return content
    return None

def extract_commit_intents(
    repo_path: Path,
    file_path: str,
    num_commits: int = 10
) -> List[Dict]:
    """Extract intent from commits affecting a specific file.

    Commit messages express: problem discovery, solution proposal,
    architectural intent, feature requests.
    """
    try:
        result = subprocess.run(
            ['git', 'log', '--oneline', '-n', str(num_commits), '--', file_path],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=10
        )

        commits = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split(' ', 1)
                if len(parts) == 2:
                    commits.append({
                        'hash': parts[0],
                        'message': parts[1]
                    })
        return commits
    except:
        return []

def extract_docstring_intent(source_code: str) -> Optional[str]:
    """Extract docstring from function/class source."""
    # Match triple-quoted docstrings
    match = re.search(r'"""(.*?)"""', source_code, re.DOTALL)
    if match:
        return match.group(1).strip()

    match = re.search(r"'''(.*?)'''", source_code, re.DOTALL)
    if match:
        return match.group(1).strip()

    return None

def classify_commit_intent(message: str) -> str:
    """Classify commit message intent category.

    Categories from research:
    - fix: Problem resolution
    - feat: New functionality
    - refactor: Architectural change
    - docs: Documentation
    - test: Test coverage
    - chore: Maintenance
    """
    message_lower = message.lower()

    if any(p in message_lower for p in ['fix', 'bug', 'issue', 'error']):
        return 'fix'
    elif any(p in message_lower for p in ['add', 'feat', 'implement', 'new']):
        return 'feature'
    elif any(p in message_lower for p in ['refactor', 'restructure', 'reorganize']):
        return 'refactor'
    elif any(p in message_lower for p in ['doc', 'readme', 'comment']):
        return 'docs'
    elif any(p in message_lower for p in ['test', 'spec', 'coverage']):
        return 'test'
    else:
        return 'chore'

def build_node_intent_profile(
    node_id: str,
    file_path: str,
    source_code: str,
    repo_path: Path
) -> Dict:
    """Build comprehensive intent profile for a node.

    Combines:
    - Docstring (explicit developer intent)
    - Commit history (evolution intent)
    - File context (module purpose)
    """
    profile = {
        'node_id': node_id,
        'file_path': file_path
    }

    # Docstring intent
    docstring = extract_docstring_intent(source_code)
    if docstring:
        profile['docstring'] = docstring[:500]  # Truncate

    # Commit intent
    commits = extract_commit_intents(repo_path, file_path, num_commits=5)
    if commits:
        profile['recent_commits'] = commits
        profile['commit_intents'] = [
            classify_commit_intent(c['message']) for c in commits
        ]

    return profile
```

---

## Integration into Collider Pipeline

### Modified `src/core/full_analysis.py` (Complete)

```python
# After existing Stage 3...

# ============================================================
# STAGE 3.1: SEMANTIC GRAPH ANALYSIS (PURPOSE = f(edges))
# ============================================================

from graph_framework import (
    build_nx_graph,
    compute_degree_metrics,
    classify_node_role,
    propagate_context,
    find_entry_points
)
from graph_metrics import (
    compute_centrality_metrics,
    identify_critical_nodes,
    detect_clusters
)

print("\n" + "="*60)
print("STAGE 3.1: SEMANTIC GRAPH ANALYSIS")
print("="*60)

# 3.1.1: Build graph
print("\nðŸ“Š Building NetworkX graph...")
G = build_nx_graph(nodes, edges)
print(f"   Nodes: {len(G.nodes)}, Edges: {len(G.edges)}")

# 3.1.2: Degree metrics (utility vs orchestrator)
print("\nðŸ“ˆ Computing degree metrics...")
degree_metrics = compute_degree_metrics(G)

# 3.1.3: Centrality metrics (bridges, coordinators)
print("\nðŸŽ¯ Computing centrality metrics...")
centrality_metrics = compute_centrality_metrics(G)

# 3.1.4: Identify critical nodes
print("\nâš¡ Identifying critical nodes...")
critical_nodes = identify_critical_nodes(G, centrality_metrics)
print(f"   Bridges: {len(critical_nodes['bridges'])}")
print(f"   Influential: {len(critical_nodes['influential'])}")
print(f"   Coordinators: {len(critical_nodes['coordinators'])}")

# 3.1.5: Detect clusters (modules)
print("\nðŸ”² Detecting clusters...")
clusters = detect_clusters(G)
print(f"   Found {len(clusters)} clusters")

# 3.1.6: Context propagation
print("\nðŸ”„ Propagating context from entry points...")
entry_points = find_entry_points(G)
print(f"   Entry points: {len(entry_points)}")
node_context = propagate_context(G, entry_points)

# 3.1.7: Enrich nodes
print("\nâœ¨ Enriching nodes with semantic data...")
for node in nodes:
    nid = node['id']

    # Degree metrics
    if nid in degree_metrics:
        node['in_degree'] = degree_metrics[nid]['in_degree']
        node['out_degree'] = degree_metrics[nid]['out_degree']
        node['semantic_role'] = classify_node_role(
            degree_metrics[nid]['in_degree'],
            degree_metrics[nid]['out_degree']
        )

    # Centrality metrics
    if nid in centrality_metrics:
        node['betweenness'] = centrality_metrics[nid]['betweenness']
        node['closeness'] = centrality_metrics[nid]['closeness']
        node['pagerank'] = centrality_metrics[nid]['pagerank']

    # Critical node flags
    node['is_bridge'] = nid in critical_nodes['bridges']
    node['is_influential'] = nid in critical_nodes['influential']
    node['is_coordinator'] = nid in critical_nodes['coordinators']

    # Context from propagation
    if nid in node_context:
        node['depth_from_entry'] = node_context[nid].get('depth_from_entry', -1)

print("âœ… Stage 3.1: Semantic analysis complete")
```

---

## Output Schema Enhancement

### New fields in `unified_analysis.json` nodes:

```json
{
  "id": "UserService.validate",
  "file_path": "src/services/user.py",
  "atom_type": "Function",

  "// === NEW: Semantic Indexer Fields ===": "",

  "// Degree metrics (Phase 1)": "",
  "in_degree": 47,
  "out_degree": 3,
  "semantic_role": "utility",

  "// Centrality metrics (Phase 3)": "",
  "betweenness": 0.023,
  "closeness": 0.156,
  "pagerank": 0.0089,

  "// Critical node flags (Phase 3)": "",
  "is_bridge": false,
  "is_influential": true,
  "is_coordinator": false,

  "// Context propagation (Phase 2)": "",
  "depth_from_entry": 3,

  "// Intent extraction (Phase 4)": "",
  "intent_profile": {
    "docstring": "Validates user data against schema",
    "recent_commits": ["Fix validation edge case", "Add email format check"],
    "commit_intents": ["fix", "feature"]
  }
}
```

---

## Local LLM Integration (Optional Enhancement)

### Using Ollama for semantic classification:

```python
# src/core/llm_classifier.py
import subprocess
import json

def classify_with_ollama(
    node_context: dict,
    model: str = "qwen2.5-coder:7b"
) -> str:
    """Use local LLM for nuanced semantic classification.

    Research finding: Fine-tuned small models outperform
    zero-shot large models on classification tasks.
    """
    prompt = f"""Classify this code element's architectural role.

Function: {node_context.get('id', 'unknown')}
Module: {node_context.get('file_path', 'unknown')}

Relationships:
- Called by {node_context.get('in_degree', 0)} functions
- Calls {node_context.get('out_degree', 0)} functions
- Betweenness centrality: {node_context.get('betweenness', 0):.3f}
- PageRank: {node_context.get('pagerank', 0):.4f}

Docstring: {node_context.get('docstring', 'None')}

Classify as ONE of:
- orchestrator (coordinates multiple components)
- infrastructure (foundational service)
- computation (domain logic)
- validation (checks constraints)
- utility (helper/transformation)

Role:"""

    try:
        result = subprocess.run(
            ['ollama', 'run', model, prompt],
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.stdout.strip().split('\n')[0].lower()
    except:
        return "unknown"
```

---

## Validation Checklist

- [ ] Phase 1: `graph_framework.py` passes unit tests
- [ ] Phase 1: Degree metrics appear in `unified_analysis.json`
- [ ] Phase 2: Context propagation reaches all reachable nodes
- [ ] Phase 3: Centrality metrics computed without errors
- [ ] Phase 3: Critical nodes identified match expert intuition
- [ ] Phase 4: Commit intents extracted for modified files
- [ ] Integration: Full pipeline runs end-to-end
- [ ] Visualization: HTML report shows semantic roles

---

## References

- PURPOSE_ONTOLOGY.md: Theoretical foundation
- PURPOSE_ONTOLOGY_VISUALS.md: ASCII diagrams
- Perplexity research: 60 academic citations
- Gemini analysis: Implementation guidance
- NetworkX docs: https://networkx.org/documentation/stable/
