# üöÄ Guia de Configura√ß√£o de LLM para Apple M4 Pro (24GB RAM)

## üíª Seu Hardware
- **Processador**: Apple M4 Pro (14 cores)
- **Mem√≥ria**: 24 GB RAM
- **Capacidade**: Excelente para modelos de at√© 34B par√¢metros!

---

## üèÜ Modelos Recomendados (do mais r√°pido ao mais capaz)

### 1. **Qwen2.5-Coder-14B-Instruct** ‚≠ê **RECOMENDADO**
- **Tamanho**: 14B par√¢metros
- **RAM necess√°ria**: ~10GB
- **Vantagens**: Especializado em c√≥digo, muito r√°pido
- **Download**: `ollama pull qwen2.5-coder:14b-instruct`

### 2. **DeepSeek-Coder-V2-Instruct** ‚ö° **MUITO R√ÅPIDO**
- **Tamanho**: 16B par√¢metros
- **RAM necess√°ria**: ~12GB
- **Vantagens**: √ìtimo para an√°lise de c√≥digo
- **Download**: `ollama pull deepseek-coder-v2:16b-instruct`

### 3. **Qwen2.5-32B-Instruct** üí™ **MAIOR CAPACIDADE**
- **Tamanho**: 32B par√¢metros
- **RAM necess√°ria**: ~20GB
- **Vantagens**: M√°ximo desempenho no seu hardware
- **Download**: `ollama pull qwen2.5:32b-instruct`

### 4. **Llama-3.1-8B-Instruct** üî• **EQUIL√çBRIO PERFEITO**
- **Tamanho**: 8B par√¢metros
- **RAM necess√°ria**: ~6GB
- **Vantagens**: R√°pido e eficiente
- **Download**: `ollama pull llama3.1:8b-instruct`

---

## ‚ö° Como Instalar e Usar

### Op√ß√£o A: Via Ollama (Recomendado)

```bash
# 1. Instalar o Ollama (se n√£o tiver)
brew install ollama

# 2. Iniciar o servi√ßo
ollama serve

# 3. Baixar o modelo recomendado
ollama pull qwen2.5-coder:14b-instruct

# 4. Configurar vari√°veis de ambiente
export SPECTROMETER_LLM_URL="http://localhost:11434/v1"
export SPECTROMETER_LLM_MODEL="qwen2.5-coder:14b-instruct"

# 5. Executar o Spectrometer
python3 spectrometer.py "the-code-elementals (2)" elements.json --enable-llm
```

### Op√ß√£o B: Via LM Studio (Interface Gr√°fica)

1. **Download**: https://lmstudio.ai/
2. **Instalar** e abrir o LM Studio
3. **Buscar modelo**: "Qwen2.5-Coder-14B-Instruct"
4. **Configurar**:
   - GPU Acceleration: Metal
   - Context Length: 4096
   - Server Port: 8080
5. **Start Server**
6. **Configurar Spectrometer**:
   ```bash
   export SPECTROMETER_LLM_URL="http://localhost:8080/v1"
   export SPECTROMETER_LLM_MODEL="qwen2.5-coder:14b-instruct"
   ```

---

## üìä Performance Esperada

| Modelo | Tempo Resposta | Qualidade | Uso RAM |
|--------|----------------|-----------|----------|
| Qwen2.5-14B | 2-3s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10GB |
| DeepSeek-16B | 2-4s | ‚≠ê‚≠ê‚≠ê‚≠ê | 12GB |
| Qwen2.5-32B | 4-6s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 20GB |
| Llama3.1-8B | 1-2s | ‚≠ê‚≠ê‚≠ê | 6GB |

---

## üéØ Configura√ß√£o Otimizada

Para obter o melhor desempenho no M4 Pro:

```bash
# Configurar para usar otimiza√ß√µes do Apple Silicon
export OLLAMA_GPU=nvidia  # Mesmo sendo Apple, for√ßa otimiza√ß√£o
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=2
```

---

## üîß Verifica√ß√£o

Para confirmar que o modelo est√° rodando:

```bash
# Verificar modelos dispon√≠veis
ollama list

# Testar o modelo
ollama run qwen2.5-coder:14b-instruct "Analise este c√≥digo: function hello() { return 'world'; }"
```

---

## ‚ö†Ô∏è Solu√ß√£o de Problemas

### Se o LLM retornar erro 404:
1. Verifique se o Ollama est√° rodando: `ps aux | grep ollama`
2. Verifique a porta: `lsof -i :11434`
3. Teste direto: `curl http://localhost:11434/api/generate`

### Se ficar lento demais:
1. Use um modelo menor (Llama3.1-8B)
2. Reduza o contexto no LLM
3. Feche outros aplicativos

---

## üí° Dica Pro

Para o **melhor equil√≠brio** entre velocidade e qualidade no seu hardware, use **Qwen2.5-Coder-14B-Instruct**:
- Especializado em an√°lise de c√≥digo
- Respostas r√°pidas no M4 Pro
- Alta precis√£o arquitetural
- Usa apenas 40% da sua RAM