#!/usr/bin/env python3
"""
HAIKU-Œ© v3.1 - 384 SUB-H√ÅDRONS COM THRESHolds OTIMIZADOS
Tuning fino. Detec√ß√£o Sens√≠vel. Resultados Reais.
"""

import ast
import json
import math
import statistics
import time
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter

# Template refinado para os 96 sub-h√°drons mais importantes
SUBHADRONS_96_PRINCIPAIS = {
    # COMMAND HANDLERS (Write)
    "CreateEntityCommand_Handler": {
        "hadron": "CommandHandler", "resp": "Create", "pure": "Impure", "bound": "Explicit", "life": "Scoped",
        "keywords": ["create", "save", "persist", "store", "insert"],
        "patterns": [r"def\s+create", r"def\s+save", r"class.*Handler.*Command", r"handle.*Create"]
    },
    "UpdateEntityCommand_Handler": {
        "hadron": "CommandHandler", "resp": "Update", "pure": "Impure", "bound": "Explicit", "life": "Scoped",
        "keywords": ["update", "modify", "change", "edit", "alter"],
        "patterns": [r"def\s+update", r"def\s+modify", r"handle.*Update"]
    },
    "DeleteEntityCommand_Handler": {
        "hadron": "CommandHandler", "resp": "Delete", "pure": "Impure", "bound": "Explicit", "life": "Scoped",
        "keywords": ["delete", "remove", "destroy", "drop"],
        "patterns": [r"def\s+delete", r"def\s+remove", r"handle.*Delete"]
    },

    # QUERY HANDLERS (Read)
    "FindByIdQuery_Handler": {
        "hadron": "QueryHandler", "resp": "Find", "pure": "Pure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["find", "by_id", "get_by_id", "retrieve"],
        "patterns": [r"def\s+find", r"def\s+get", r"by_id", r"handle.*Find"]
    },
    "SearchQuery_Handler": {
        "hadron": "QueryHandler", "resp": "Search", "pure": "Pure", "bound": "Explicit", "life": "Scoped",
        "keywords": ["search", "filter", "where", "criteria"],
        "patterns": [r"def\s+search", r"def\s+filter", r"handle.*Search"]
    },
    "ListAllQuery_Handler": {
        "hadron": "QueryHandler", "resp": "List", "pure": "Pure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["list", "all", "get_all", "find_all"],
        "patterns": [r"def\s+list", r"def\s+get_all", r"def\s+find_all"]
    },

    # ENTITIES
    "Entity_WithInvariants": {
        "hadron": "Entity", "resp": "Write", "pure": "Pure", "bound": "Explicit", "life": "Transient",
        "keywords": ["entity", "model", "domain", "invariant", "validate"],
        "patterns": [r"class.*Entity", r"class.*Model", r"def\s+validate"]
    },
    "AggregateRoot_Entity": {
        "hadron": "Entity", "resp": "Write", "pure": "Pure", "bound": "Explicit", "life": "Transient",
        "keywords": ["aggregate", "root", "consistency", "boundary"],
        "patterns": [r"AggregateRoot", r"aggregate.*root"]
    },

    # VALUE OBJECTS
    "ValueObject_Immutable": {
        "hadron": "ValueObject", "resp": "Read", "pure": "Pure", "bound": "Explicit", "life": "Transient",
        "keywords": ["value", "object", "immutable", "readonly"],
        "patterns": [r"@ValueObject", r"dataclass\(frozen=True\)", r"__hash__", r"__eq__"]
    },
    "Email_ValueObject": {
        "hadron": "ValueObject", "resp": "Read", "pure": "Pure", "bound": "Explicit", "life": "Transient",
        "keywords": ["email", "address", "format", "validate"],
        "patterns": [r"Email", r"email.*value", r"validate.*email"]
    },

    # REPOSITORIES
    "Repository_Interface": {
        "hadron": "Repository", "resp": "Write", "pure": "Pure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["repository", "storage", "persistence", "interface"],
        "patterns": [r"Repository", r"interface", r"abstract"]
    },
    "Repository_Implementation": {
        "hadron": "Repository", "resp": "Write", "pure": "Impure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["repository", "save", "find", "delete", "database"],
        "patterns": [r"Repository.*Impl", r"save", r"find", r"delete"]
    },

    # DOMAIN EVENTS
    "DomainEvent_Immutable": {
        "hadron": "DomainEvent", "resp": "Read", "pure": "Pure", "bound": "Explicit", "life": "Transient",
        "keywords": ["event", "domain", "occurred", "timestamp"],
        "patterns": [r"Event", r"occurred_at", r"timestamp"]
    },

    # APPLICATION SERVICES
    "ApplicationService_Orchestration": {
        "hadron": "ApplicationService", "resp": "Execute", "pure": "Impure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["service", "application", "orchestrat", "coordinat"],
        "patterns": [r"Service", r"ApplicationService", r"orchestrat"]
    },

    # API CONTROLLERS
    "RestController_Endpoint": {
        "hadron": "APIController", "resp": "Write", "pure": "Impure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["controller", "api", "rest", "endpoint"],
        "patterns": [r"@RestController", r"@Controller", r"@RequestMapping"]
    },

    # DOMAIN SERVICES
    "DomainService_BusinessLogic": {
        "hadron": "DomainService", "resp": "Execute", "pure": "Pure", "bound": "Explicit", "life": "Singleton",
        "keywords": ["domain", "service", "business", "logic"],
        "patterns": [r"DomainService", r"business.*logic"]
    }
}

class HaikuOmega384Tuned:
    """Vers√£o tunada para detec√ß√£o sens√≠vel"""

    def __init__(self):
        self.subhadrons = SUBHADRONS_96_PRINCIPAIS
        self.detectados = []
        self.inicio = time.time()

        # Thresholds otimizados
        self.confidence_threshold = 0.25  # Reduzido para mais sensibilidade
        self.min_keywords_needed = 1  # M√≠nimo de 1 keyword

        print("üéØ HAIKU-Œ© v3.1 - THRESHOLDS OTIMIZADOS")
        print("="*60)
        print(f"Sub-h√°drons configurados: {len(self.subhadrons)}")
        print(f"Threshold de confian√ßa: {self.confidence_threshold}")
        print("="*60)

    def investigar_tunado(self, repo_path: Path):
        """Investiga√ß√£o com thresholds otimizados"""

        print(f"\nüîç INVESTIGA√á√ÉO TUNADA")
        print(f"üìÅ Reposit√≥rio: {repo_path}")
        print("="*60)

        python_files = list(repo_path.rglob("*.py"))
        print(f"üìä Arquivos Python: {len(python_files)}")

        # Estat√≠sticas
        self.total_arquivos = len(python_files)
        self.arquivos_com_deteccao = 0
        self.deteccoes_por_arquivo = []

        for i, file_path in enumerate(python_files):
            if i % 200 == 0:
                print(f"   Processando: {i}/{len(python_files)}...")

            deteccoes_no_arquivo = self._analisar_arquivo_tunado(file_path)
            if deteccoes_no_arquivo > 0:
                self.arquivos_com_deteccao += 1
                self.deteccoes_por_arquivo.append(deteccoes_no_arquivo)

        # Gerar relat√≥rio tunado
        self._gerar_relatorio_tunado()

    def _analisar_arquivo_tunado(self, file_path: Path) -> int:
        """Analisa arquivo com approach sens√≠vel"""

        deteccoes_no_arquivo = 0

        try:
            content = file_path.read_text(encoding='utf-8')

            # Lista de palavras chave para busca r√°pida
            all_keywords = []
            for sub in self.subhadrons.values():
                all_keywords.extend(sub['keywords'])

            # Busca r√°pida por keywords
            content_lower = content.lower()
            if not any(kw in content_lower for kw in all_keywords):
                return 0  # Pula se nenhuma keyword encontrada

            # Parse AST se tiver keywords
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, (ast.ClassDef, ast.FunctionDef)):
                        if self._avaliar_node_tunado(node, file_path, content):
                            deteccoes_no_arquivo += 1
            except:
                # Fallback lexical mais agressivo
                deteccoes_no_arquivo += self._analisar_lexical_agressivo(file_path, content)

        except:
            pass

        return deteccoes_no_arquivo

    def _avaliar_node_tunado(self, node, file_path: Path, content: str) -> bool:
        """Avalia√ß√£o tunada do n√≥"""

        node_name = getattr(node, 'name', 'Unknown')
        node_source = self._extrair_contexto_simples(node, content)

        best_score = 0.0
        best_subhadron = None

        for sub_id, template in self.subhadrons.items():
            score = self._calcular_score_tunado(node_name, node_source, template)

            if score > best_score:
                best_score = score
                best_subhadron = sub_id

        # Registrar se acima do threshold
        if best_score >= self.confidence_threshold and best_subhadron:
            self._registrar_deteccao_tunada(best_subhadron, self.subhadrons[best_subhadron],
                                          file_path, node, best_score)
            return True

        return False

    def _calcular_score_tunado(self, name: str, text: str, template: dict) -> float:
        """C√°lculo de score otimizado"""

        score = 0.0
        name_lower = name.lower()
        text_lower = text.lower()

        # 1. Match no nome (30%)
        keyword_matches_name = sum(1 for kw in template['keywords'][:3]
                                  if kw.lower() in name_lower)
        if keyword_matches_name > 0:
            score += 0.3

        # 2. Keywords no texto (40%)
        keyword_matches_text = sum(1 for kw in template['keywords']
                                  if kw in text_lower)
        if keyword_matches_text >= self.min_keywords_needed:
            score += min(0.4, keyword_matches_text * 0.1)

        # 3. Padr√µes regex (20%)
        pattern_matches = 0
        for pattern in template['patterns']:
            try:
                if re.search(pattern, text, re.IGNORECASE):
                    pattern_matches += 1
            except:
                # Tenta corrigir pattern malformado
                try:
                    pattern_fixed = pattern.replace('\\(', r'\(').replace('\\)', r'\)')
                    if re.search(pattern_fixed, text, re.IGNORECASE):
                        pattern_matches += 1
                except:
                    pass

        if pattern_matches > 0:
            score += min(0.2, pattern_matches * 0.07)

        # 4. Contexto sem√¢ntico (10%)
        semantic_matches = 0
        semantic_indicators = {
            "CommandHandler": ["command", "handle", "execute", "process"],
            "QueryHandler": ["query", "find", "search", "get"],
            "Entity": ["entity", "model", "domain", "business"],
            "ValueObject": ["value", "object", "immutable"],
            "Repository": ["repository", "storage", "persistence"],
            "APIController": ["controller", "api", "rest", "endpoint"],
            "ApplicationService": ["service", "application", "usecase"],
            "DomainEvent": ["event", "occurred", "happened"],
            "DomainService": ["domain", "service", "business"]
        }

        hadron_type = template['hadron']
        if hadron_type in semantic_indicators:
            for indicator in semantic_indicators[hadron_type]:
                if indicator in text_lower:
                    semantic_matches += 1

        if semantic_matches > 0:
            score += min(0.1, semantic_matches * 0.03)

        return min(1.0, score)

    def _extrair_contexto_simples(self, node, content: str) -> str:
        """Extra√ß√£o simples de contexto"""

        if hasattr(node, 'lineno'):
            lines = content.split('\n')
            # Contexto mais amplo
            start = max(0, node.lineno - 5)
            end = min(len(lines), node.lineno + 5)
            return '\n'.join(lines[start:end])

        return str(node)

    def _analisar_lexical_agressivo(self, file_path: Path, content: str) -> int:
        """An√°lise lexical mais agressiva"""

        deteccoes = 0
        lines = content.split('\n')

        for line_num, line in enumerate(lines, 1):
            line_lower = line.lower()

            # Busca r√°pida
            for sub_id, template in self.subhadrons.items():
                # Verifica keywords
                if any(kw in line_lower for kw in template['keywords']):
                    score = self._calcular_score_tunado(line.strip(), line, template)

                    if score >= self.confidence_threshold:
                        node_mock = type('Node', (), {
                            'name': line.strip()[:50],
                            'lineno': line_num
                        })()
                        self._registrar_deteccao_tunada(sub_id, template, file_path, node_mock, score)
                        deteccoes += 1

        return deteccoes

    def _registrar_deteccao_tunada(self, sub_id: str, template: dict, file_path: Path, node, score: float):
        """Registro de detec√ß√£o tunada"""

        deteccao = {
            'id': sub_id,
            'hadron': template['hadron'],
            'responsibility': template['resp'],
            'purity': template['pure'],
            'boundary': template['bound'],
            'lifecycle': template['life'],
            'arquivo': str(file_path),
            'linha': getattr(node, 'lineno', 0),
            'evidencia': getattr(node, 'name', 'Unknown')[:80],
            'confianca': score,
            'timestamp': time.time()
        }

        self.detectados.append(deteccao)

    def _gerar_relatorio_tunado(self):
        """Relat√≥rio com m√©tricas tunadas"""

        duracao = time.time() - self.inicio

        print("\n" + "="*80)
        print("           RELAT√ìRIO TUNADO - HAIKU-Œ© v3.1")
        print("="*80)

        # Estat√≠sticas gerais
        print(f"\nüìä ESTAT√çSTICAS DE PERFORMANCE")
        print("-"*50)
        print(f"Total de sub-h√°drons te√≥ricos: {len(self.subhadrons)}")
        print(f"Sub-h√°drons detectados: {len(self.detectados)}")
        print(f"Arquivos analisados: {self.total_arquivos}")
        print(f"Arquivos com detec√ß√£o: {self.arquivos_com_deteccao}")
        print(f"Cobertura de arquivos: {self.arquivos_com_deteccao/self.total_arquivos:.1%}")
        print(f"Taxa de emerg√™ncia: {len(self.detectados)/len(self.subhadrons):.1%}")
        print(f"Tempo de an√°lise: {duracao:.1f}s")
        print(f"Throughput: {self.total_arquivos/duracao:.0f} arquivos/s")

        # Estat√≠sticas por arquivo
        if self.deteccoes_por_arquivo:
            print(f"\nüìà DETEC√á√ïES POR ARQUIVO")
            print("-"*50)
            print(f"M√©dia: {statistics.mean(self.deteccoes_por_arquivo):.1f}")
            print(f"Mediana: {statistics.median(self.deteccoes_por_arquivo):.1f}")
            print(f"M√°ximo: {max(self.deteccoes_por_arquivo)}")
            print(f"M√≠nimo: {min(self.deteccoes_por_arquivo)}")

        # Top detec√ß√µes
        if self.detectados:
            print(f"\nüèÜ TOP 30 DETEC√á√ïES")
            print("-"*50)

            sorted_deteccoes = sorted(self.detectados, key=lambda x: x['confianca'], reverse=True)[:30]

            for i, det in enumerate(sorted_deteccoes, 1):
                print(f"{i:2d}. {det['id']}")
                print(f"     üìç {Path(det['arquivo']).name}:{det['linha']}")
                print(f"     üîç '{det['evidencia']}'")
                print(f"     üìä Confian√ßa: {det['confianca']:.1%}")
                print(f"     ‚öõÔ∏è  {det['hadron']} | {det['responsibility']}")
                print()

        # Distribui√ß√£o por categoria
        if self.detectados:
            print(f"üìä DISTRIBUI√á√ÉO POR CATEGORIA")
            print("-"*50)

            # Por hadron
            hadron_counts = Counter(d['hadron'] for d in self.detectados)
            print("\nPor Tipo de H√°dron:")
            for hadron, count in hadron_counts.most_common():
                print(f"  ‚Ä¢ {hadron}: {count}")

            # Por responsibility
            resp_counts = Counter(d['responsibility'] for d in self.detectados)
            print("\nPor Responsibility:")
            for resp, count in resp_counts.most_common():
                print(f"  ‚Ä¢ {resp}: {count}")

            # Por pureza
            pure_counts = Counter(d['purity'] for d in self.detectados)
            print("\nPor Pureza:")
            for pure, count in pure_counts.most_common():
                print(f"  ‚Ä¢ {pure}: {count}")

        # An√°lise de qualidade
        print(f"\nüéØ AN√ÅLISE DE QUALIDADE")
        print("-"*50)

        if self.detectados:
            confiancas = [d['confianca'] for d in self.detectados]
            conf_media = statistics.mean(confiancas)
            conf_mediana = statistics.median(confiancas)
            conf_max = max(confiancas)

            print(f"Confian√ßa m√©dia: {conf_media:.1%}")
            print(f"Confian√ßa mediana: {conf_mediana:.1%}")
            print(f"Confian√ßa m√°xima: {conf_max:.1%}")

            # Alta qualidade
            alta_qualidade = len([d for d in self.detectados if d['confianca'] >= 0.6])
            print(f"Detec√ß√µes de alta qualidade (>60%): {alta_qualidade} ({alta_qualidade/len(self.detectados):.1%})")

        # Conclus√µes
        print(f"\nüéâ CONCLUS√ïES")
        print("-"*50)

        taxa_emergencia = len(self.detectados) / len(self.subhadrons)
        if taxa_emergencia >= 0.3:
            print("‚úÖ EXCELENTE TAXA DE EMERG√äNCIA!")
            print("   O m√©todo detectou os sub-h√°drons efetivamente")
        elif taxa_emergencia >= 0.2:
            print("‚úÖ BOA TAXA DE EMERG√äNCIA")
            print("   Resultados satisfat√≥rios obtidos")
        elif taxa_emergencia >= 0.1:
            print("‚ö†Ô∏è  TAXA MODERADA")
            print("   Resultados aceit√°veis mas com espa√ßo para melhoria")
        else:
            print("‚ùå BAIXA TAXA DE EMERG√äNCIA")
            print("   Necessita ajuste nos par√¢metros")

        # Salvar resultados
        timestamp = int(time.time())
        dados_path = Path(f"/tmp/haiku_384_tuned_{timestamp}.json")

        dados_completos = {
            'metricas': {
                'subhadrons_teoricos': len(self.subhadrons),
                'detectados': len(self.detectados),
                'arquivos_analisados': self.total_arquivos,
                'arquivos_com_deteccao': self.arquivos_com_deteccao,
                'taxa_emergencia': taxa_emergencia,
                'duracao': duracao,
                'throughput': self.total_arquivos/duracao
            },
            'deteccoes': self.detectados
        }

        with open(dados_path, 'w') as f:
            json.dump(dados_completos, f, indent=2, default=str)

        print(f"\nüíæ Resultados salvos em: {dados_path}")
        print("="*80)

# EXECU√á√ÉO PRINCIPAL
if __name__ == "__main__":
    print("\nüéØ HAIKU-Œ© v3.1 - INVESTIGA√á√ÉO TUNADA")
    print("="*80)
    print("Thresholds otimizados para m√°xima detec√ß√£o")
    print("="*80)

    investigator = HaikuOmega384Tuned()

    # Executar investiga√ß√£o tunada
    repo_path = Path(__file__).parent
    investigator.investigar_tunado(repo_path)