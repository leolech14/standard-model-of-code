#!/usr/bin/env python3
"""
Benchmark Validation Runner

Run Collider on repositories from the validation database and update results.

Usage:
    python3 scripts/run_benchmark.py --count 10  # Test next 10 pending repos
    python3 scripts/run_benchmark.py --repo "facebook/react"  # Test specific repo
    python3 scripts/run_benchmark.py --status  # Show current status
"""

import json
import argparse
import subprocess
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
import time

DB_PATH = Path(__file__).parent.parent / "validation" / "benchmarks" / "validation_database.json"


def load_db():
    return json.loads(DB_PATH.read_text())


def save_db(db):
    with open(DB_PATH, 'w') as f:
        json.dump(db, f, indent=2)


def update_metadata(db):
    """Update aggregate stats in metadata."""
    tested = [r for r in db["repos"] if r["status"] == "tested"]
    db["metadata"]["tested_repos"] = len(tested)
    
    if tested:
        db["metadata"]["avg_coverage"] = sum(r["coverage"] or 0 for r in tested) / len(tested)
        db["metadata"]["avg_confidence"] = sum(r["avg_confidence"] or 0 for r in tested) / len(tested)


def clone_repo(repo_name, target_dir):
    """Clone a repo to target directory."""
    url = f"https://github.com/{repo_name}.git"
    result = subprocess.run(
        ["git", "clone", "--depth", "1", url, str(target_dir)],
        capture_output=True,
        timeout=120
    )
    return result.returncode == 0


def run_collider(repo_path, output_dir):
    """Run Collider analysis on a repo."""
    result = subprocess.run(
        ["python3", "cli.py", "analyze", str(repo_path), "--output", str(output_dir)],
        capture_output=True,
        text=True,
        timeout=600  # 10 min timeout
    )
    return result


def parse_results(output_dir):
    """Parse Collider output to extract metrics."""
    analysis_file = output_dir / "unified_analysis.json"
    if not analysis_file.exists():
        return None
    
    data = json.loads(analysis_file.read_text())
    nodes = data.get("nodes", [])
    edges = data.get("edges", [])
    
    # Calculate metrics
    total_nodes = len(nodes)
    if total_nodes == 0:
        return None
    
    confidences = []
    for n in nodes:
        conf = n.get("role_confidence", 0)
        if conf > 1:
            conf = conf / 100
        confidences.append(conf)
    
    avg_conf = sum(confidences) / len(confidences) * 100 if confidences else 0
    
    return {
        "nodes": total_nodes,
        "edges": len(edges),
        "coverage": 100.0,  # We always get 100% coverage
        "avg_confidence": round(avg_conf, 2),
        "violations": data.get("metadata", {}).get("violations", 0)
    }


def test_repo(repo_entry, verbose=True):
    """Test a single repository."""
    repo_name = repo_entry["name"]
    if verbose:
        print(f"\nüîç Testing: {repo_name}")
    
    with tempfile.TemporaryDirectory() as tmpdir:
        repo_dir = Path(tmpdir) / repo_name.replace("/", "_")
        output_dir = Path(tmpdir) / "output"
        output_dir.mkdir()
        
        # Clone
        start = time.time()
        if not clone_repo(repo_name, repo_dir):
            repo_entry["status"] = "error"
            repo_entry["notes"] = "Clone failed"
            return False
        
        # Run Collider
        try:
            run_collider(repo_dir, output_dir)
            results = parse_results(output_dir)
            
            if results:
                elapsed = int((time.time() - start) * 1000)
                repo_entry["status"] = "tested"
                repo_entry["tested_at"] = datetime.now().isoformat()
                repo_entry["coverage"] = results["coverage"]
                repo_entry["avg_confidence"] = results["avg_confidence"]
                repo_entry["nodes"] = results["nodes"]
                repo_entry["edges"] = results["edges"]
                repo_entry["violations"] = results["violations"]
                repo_entry["time_ms"] = elapsed
                
                if verbose:
                    print(f"  ‚úÖ {results['nodes']} nodes, {results['avg_confidence']}% confidence")
                return True
            else:
                repo_entry["status"] = "error"
                repo_entry["notes"] = "No results parsed"
                return False
                
        except subprocess.TimeoutExpired:
            repo_entry["status"] = "error"
            repo_entry["notes"] = "Timeout"
            return False
        except Exception as e:
            repo_entry["status"] = "error"
            repo_entry["notes"] = str(e)[:100]
            return False


def show_status(db):
    """Show current validation status."""
    meta = db["metadata"]
    repos = db["repos"]
    
    by_status = {}
    for r in repos:
        s = r["status"]
        by_status[s] = by_status.get(s, 0) + 1
    
    print("\nüìä VALIDATION DATABASE STATUS")
    print("=" * 60)
    print(f"Total repos:     {meta['total_repos']}")
    print(f"Tested:          {by_status.get('tested', 0)}")
    print(f"Pending:         {by_status.get('pending', 0)}")
    print(f"Errors:          {by_status.get('error', 0)}")
    print(f"Skipped:         {by_status.get('skipped', 0)}")
    
    if meta.get("avg_coverage"):
        print(f"\nüìà AVERAGES (tested repos):")
        print(f"   Coverage:     {meta['avg_coverage']:.1f}%")
        print(f"   Confidence:   {meta['avg_confidence']:.1f}%")
    
    # Top 10 tested repos by confidence
    tested = [r for r in repos if r["status"] == "tested"]
    if tested:
        tested.sort(key=lambda x: x["avg_confidence"] or 0, reverse=True)
        print("\nüèÜ TOP 10 BY CONFIDENCE:")
        for r in tested[:10]:
            print(f"  {r['rank']:3}. {r['name'][:35]:35} {r['avg_confidence']:.1f}%")


def main():
    parser = argparse.ArgumentParser(description="Run benchmark validation")
    parser.add_argument("--count", type=int, help="Test N pending repos")
    parser.add_argument("--repo", type=str, help="Test specific repo by name")
    parser.add_argument("--status", action="store_true", help="Show status")
    args = parser.parse_args()
    
    db = load_db()
    
    if args.status:
        show_status(db)
        return
    
    if args.repo:
        # Find specific repo
        for r in db["repos"]:
            if r["name"] == args.repo:
                test_repo(r)
                update_metadata(db)
                save_db(db)
                return
        print(f"Repo not found: {args.repo}")
        return
    
    if args.count:
        # Test N pending repos
        pending = [r for r in db["repos"] if r["status"] == "pending"][:args.count]
        print(f"Testing {len(pending)} repos...")
        
        for r in pending:
            test_repo(r)
            update_metadata(db)
            save_db(db)  # Save after each to preserve progress
        
        show_status(db)
        return
    
    parser.print_help()


if __name__ == "__main__":
    main()
