{
  "exported_at": 1769237032.7597818,
  "node_count": 90,
  "total_tokens": 18889,
  "nodes": [
    {
      "content": "import subprocess\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "e9c7f6973728daee",
      "chunk_type": "imports",
      "relevance_score": 0.650430008588096,
      "start_line": 1,
      "end_line": 22,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.755957
    },
    {
      "content": "def get_repo_id(repo_path: Path) -> Dict:\n    \"\"\"Get repository identification info.\"\"\"\n    try:\n        commit = subprocess.run(\n            [\"git\", \"rev-parse\", \"HEAD\"],\n            capture_output=True, text=True, cwd=repo_path\n        ).stdout.strip()[:12]\n\n        branch = subprocess.run(\n            [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n            capture_output=True, text=True, cwd=repo_path\n        ).stdout.strip()\n\n        return {\n            \"commit\": commit,\n            \"branch\": branch,\n            \"timestamp\": datetime.now().isoformat(),\n            \"version\": REPOPACK_VERSION\n        }\n    except Exception:\n        return {\"commit\": \"unknown\", \"branch\": \"unknown\", \"timestamp\": datetime.now().isoformat(), \"version\": REPOPACK_VERSION}\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "d357be7083613fb8",
      "chunk_type": "function",
      "relevance_score": 0.9441830717576809,
      "start_line": 27,
      "end_line": 49,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.75597
    },
    {
      "content": "def get_file_tree(repo_path: Path, max_depth: int = 3, max_files: int = 200) -> str:\n    \"\"\"Get directory structure as tree string.\"\"\"\n    lines = []\n    count = 0\n\n    def walk(path: Path, prefix: str = \"\", depth: int = 0):\n        nonlocal count\n        if depth > max_depth or count > max_files:\n            return\n\n        try:\n            entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))\n        except PermissionError:\n            return\n\n        dirs = [e for e in entries if e.is_dir() and not e.name.startswith('.')]\n        files = [e for e in entries if e.is_file() and not e.name.startswith('.')]\n\n        for d in dirs[:20]:  # Limit dirs per level\n            lines.append(f\"{prefix}{d.name}/\")\n            walk(d, prefix + \"  \", depth + 1)\n\n        for f in files[:30]:  # Limit files per level\n            count += 1\n            if count <= max_files:\n                lines.append(f\"{prefix}{f.name}\")\n\n    walk(repo_path)\n    return \"\\n\".join(lines[:max_files])\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "50b759c1ba6e5287",
      "chunk_type": "function",
      "relevance_score": 0.9500433860765614,
      "start_line": 50,
      "end_line": 80,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.7559779
    },
    {
      "content": "def get_hot_files(repo_path: Path, limit: int = 10) -> List[Dict]:\n    \"\"\"Get recently modified files with content.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"git\", \"diff\", \"--name-only\", \"HEAD~5\", \"HEAD\"],\n            capture_output=True, text=True, cwd=repo_path\n        )\n        files = result.stdout.strip().split(\"\\n\")[:limit]\n\n        hot = []\n        for f in files:\n            if not f:\n                continue\n            path = repo_path / f\n            if path.exists() and path.stat().st_size < 50000:  # Max 50KB\n                try:\n                    content = path.read_text(errors='ignore')[:10000]  # Max 10K chars\n                    hot.append({\"path\": f, \"content\": content})\n                except Exception:\n                    pass\n        return hot\n    except Exception:\n        return []\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "afa994437dc9300f",
      "chunk_type": "function",
      "relevance_score": 0.946058302531887,
      "start_line": 81,
      "end_line": 105,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.7559838
    },
    {
      "content": "def format_repopack(\n    repo_path: Path,\n    question: str = \"\",\n    include_hot_code: bool = True,\n    max_tree_depth: int = 3",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "932b8b1b17907df3",
      "chunk_type": "function",
      "relevance_score": 0.8053604984823934,
      "start_line": 106,
      "end_line": 110,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.755989
    },
    {
      "content": "def get_cache_key(repo_path: Path) -> str:\n    \"\"\"Generate deterministic cache key for repo state.\"\"\"\n    repo_id = get_repo_id(Path(repo_path))\n\n    # Check for uncommitted changes\n    try:\n        result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            capture_output=True, text=True, cwd=repo_path\n        )\n        is_dirty = bool(result.stdout.strip())\n    except Exception:\n        is_dirty = True\n\n    return f\"{repo_id['commit']}_{'dirty' if is_dirty else 'clean'}\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/repopack.py",
      "chunk_id": "b29c8d285599f5e3",
      "chunk_type": "function",
      "relevance_score": 0.934948500216801,
      "start_line": 159,
      "end_line": 175,
      "metadata": {
        "file_type": ".py",
        "file_name": "repopack.py"
      },
      "created_at": 1769237032.7559948
    },
    {
      "content": "from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport re",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "14423f59b4df2042",
      "chunk_type": "imports",
      "relevance_score": 0.65,
      "start_line": 1,
      "end_line": 16,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7564669
    },
    {
      "content": "def _get_config() -> Dict[str, Any]:\n    \"\"\"Get ACI config, with lazy import to avoid circular dependency.\"\"\"\n    try:\n        from . import ACI_CONFIG\n        return ACI_CONFIG\n    except ImportError:\n        return {}\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "5c51e83cba8131a6",
      "chunk_type": "function",
      "relevance_score": 0.917022205742006,
      "start_line": 23,
      "end_line": 31,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756473
    },
    {
      "content": "class QueryIntent(Enum):\n    \"\"\"Classification of query intent.\"\"\"\n    ARCHITECTURE = \"architecture\"   # \"how does X work\", \"explain the design\"\n    DEBUG = \"debug\"                 # \"fix error\", \"issue with\", \"bug in\"\n    RESEARCH = \"research\"           # \"best practice\", \"compare\", \"latest\"\n    VALIDATE = \"validate\"           # \"check\", \"verify\", \"is this correct\"\n    TASK = \"task\"                   # \"what should I work on\", \"next task\"\n    COUNT = \"count\"                 # \"how many\", \"count of\", \"total\"\n    LOCATE = \"locate\"               # \"find\", \"where is\", \"which file\"\n    EXPLAIN = \"explain\"             # \"explain\", \"what is\", \"describe\"\n    IMPLEMENT = \"implement\"         # \"implement\", \"create\", \"build\"\n    UNKNOWN = \"unknown\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "46841ce490c127dd",
      "chunk_type": "class",
      "relevance_score": 0.943695079893223,
      "start_line": 32,
      "end_line": 45,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.75648
    },
    {
      "content": "class QueryComplexity(Enum):\n    \"\"\"Estimated complexity of the query.\"\"\"\n    SIMPLE = \"simple\"       # Single fact lookup, <1min expected\n    MODERATE = \"moderate\"   # Multi-file reasoning, 1-5min expected\n    COMPLEX = \"complex\"     # Deep analysis, >5min expected\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "46986e041328a71f",
      "chunk_type": "class",
      "relevance_score": 0.9212440818315533,
      "start_line": 46,
      "end_line": 52,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7564852
    },
    {
      "content": "class QueryScope(Enum):\n    \"\"\"Whether query targets internal codebase or external knowledge.\"\"\"\n    INTERNAL = \"internal\"   # Answer exists in codebase\n    EXTERNAL = \"external\"   # Requires web/external knowledge\n    HYBRID = \"hybrid\"       # Needs both internal and external\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "f4bb1d591e29812e",
      "chunk_type": "class",
      "relevance_score": 0.9221239884532224,
      "start_line": 53,
      "end_line": 59,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756491
    },
    {
      "content": "class QueryProfile:\n    \"\"\"Complete profile of an analyzed query.\"\"\"\n    query: str\n    intent: QueryIntent\n    complexity: QueryComplexity\n    scope: QueryScope\n    needs_agent_context: bool\n    suggested_sets: List[str]\n    keywords: List[str]\n    confidence: float  # 0.0-1.0, how confident is the analysis\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "60d8f56ac5388bff",
      "chunk_type": "class",
      "relevance_score": 0.9744979239712417,
      "start_line": 61,
      "end_line": 72,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756496
    },
    {
      "content": "def get_external_indicators() -> List[str]:\n    \"\"\"Get external scope indicators from config or defaults.\"\"\"\n    config = _get_config()\n    config_indicators = config.get(\"external_indicators\", [])\n    # Merge: config extends defaults\n    return list(set(_DEFAULT_EXTERNAL_INDICATORS + config_indicators))\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "1b98a9c010042c62",
      "chunk_type": "function",
      "relevance_score": 0.9242149919673394,
      "start_line": 142,
      "end_line": 149,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756501
    },
    {
      "content": "def get_agent_indicators() -> List[str]:\n    \"\"\"Get agent context indicators from config or defaults.\"\"\"\n    config = _get_config()\n    config_indicators = config.get(\"agent_context\", {}).get(\"trigger_keywords\", [])\n    return list(set(_DEFAULT_AGENT_INDICATORS + config_indicators))\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "ec39b5adaa391fa4",
      "chunk_type": "function",
      "relevance_score": 0.9225893217762146,
      "start_line": 150,
      "end_line": 156,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756505
    },
    {
      "content": "def get_complexity_indicators() -> Dict[str, List[str]]:\n    \"\"\"Get complexity indicators from config or defaults.\"\"\"\n    config = _get_config()\n    complexity_config = config.get(\"complexity\", {})\n    return {\n        \"simple\": complexity_config.get(\"simple\", _DEFAULT_SIMPLE_INDICATORS),\n        \"complex\": complexity_config.get(\"complex\", _DEFAULT_COMPLEX_INDICATORS),\n    }\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "b7d0f35725e13399",
      "chunk_type": "function",
      "relevance_score": 0.9288170675102897,
      "start_line": 157,
      "end_line": 166,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.75651
    },
    {
      "content": "def get_intent_keywords() -> Dict[QueryIntent, List[str]]:\n    \"\"\"Get intent keywords, merging config with defaults.\"\"\"\n    config = _get_config()\n    config_keywords = config.get(\"intent_keywords\", {})\n\n    # Start with defaults\n    merged = dict(INTENT_KEYWORDS)\n\n    # Merge in config keywords (extends, doesn't replace)\n    intent_map = {\n        \"architecture\": QueryIntent.ARCHITECTURE,\n        \"debug\": QueryIntent.DEBUG,\n        \"research\": QueryIntent.RESEARCH,\n        \"task\": QueryIntent.TASK,\n    }\n\n    for key, intent in intent_map.items():\n        if key in config_keywords:\n            existing = set(merged.get(intent, []))\n            existing.update(config_keywords[key])\n            merged[intent] = list(existing)\n\n    return merged\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "e2c8421d7ebd8233",
      "chunk_type": "function",
      "relevance_score": 0.9438397488100351,
      "start_line": 167,
      "end_line": 191,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756515
    },
    {
      "content": "def _extract_keywords(query: str) -> List[str]:\n    \"\"\"Extract significant keywords from query.\"\"\"\n    # Lowercase and remove punctuation\n    clean = re.sub(r'[^\\w\\s]', ' ', query.lower())\n    words = clean.split()\n\n    # Filter out common stop words\n    stop_words = {\n        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n        'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n        'would', 'could', 'should', 'may', 'might', 'must', 'shall',\n        'can', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by',\n        'from', 'as', 'into', 'through', 'during', 'before', 'after',\n        'above', 'below', 'between', 'under', 'again', 'further',\n        'then', 'once', 'here', 'there', 'when', 'where', 'why',\n        'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',\n        'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n        'than', 'too', 'very', 'just', 'and', 'but', 'if', 'or',\n        'because', 'until', 'while', 'about', 'against', 'this',\n        'that', 'these', 'those', 'it', 'its', 'i', 'me', 'my',\n        'you', 'your', 'we', 'our', 'they', 'their', 'what', 'which'\n    }\n\n    return [w for w in words if w not in stop_words and len(w) > 2]\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "f3f288dc523d2fe3",
      "chunk_type": "function",
      "relevance_score": 0.954210934336962,
      "start_line": 199,
      "end_line": 224,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.75652
    },
    {
      "content": "def _detect_intent(query: str, keywords: List[str]) -> tuple[QueryIntent, float]:\n    \"\"\"Detect query intent with confidence score.\"\"\"\n    query_lower = query.lower()\n\n    # Get intent keywords from config (merged with defaults)\n    intent_keywords = get_intent_keywords()\n\n    best_intent = QueryIntent.UNKNOWN\n    best_score = 0.0\n\n    for intent, indicators in intent_keywords.items():\n        score = 0.0\n        for indicator in indicators:\n            if indicator in query_lower:\n                # Weight by indicator length (longer = more specific)\n                score += len(indicator) / 10.0\n\n        if score > best_score:\n            best_score = score\n            best_intent = intent\n\n    # Normalize confidence to 0-1\n    confidence = min(best_score / 3.0, 1.0)\n\n    return best_intent, confidence\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "9bfeecba2d0a3f6d",
      "chunk_type": "function",
      "relevance_score": 0.9455312202444601,
      "start_line": 225,
      "end_line": 251,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756525
    },
    {
      "content": "def _detect_complexity(query: str, keywords: List[str]) -> QueryComplexity:\n    \"\"\"Estimate query complexity.\"\"\"\n    query_lower = query.lower()\n\n    # Get indicators from config\n    indicators = get_complexity_indicators()\n    simple_indicators = indicators[\"simple\"]\n    complex_indicators = indicators[\"complex\"]\n\n    # Check for simple indicators\n    for indicator in simple_indicators:\n        if indicator in query_lower:\n            return QueryComplexity.SIMPLE\n\n    # Check for complex indicators\n    complex_count = sum(1 for ind in complex_indicators if ind in query_lower)\n    if complex_count >= 2:\n        return QueryComplexity.COMPLEX\n\n    # Use keyword count as heuristic\n    if len(keywords) <= 3:\n        return QueryComplexity.SIMPLE\n    elif len(keywords) <= 6:\n        return QueryComplexity.MODERATE\n    else:\n        return QueryComplexity.COMPLEX\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "98c3041faa8d01d1",
      "chunk_type": "function",
      "relevance_score": 0.9470009077503833,
      "start_line": 252,
      "end_line": 279,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7565298
    },
    {
      "content": "def _detect_scope(query: str, keywords: List[str]) -> QueryScope:\n    \"\"\"Detect if query needs external knowledge.\"\"\"\n    query_lower = query.lower()\n\n    # Get indicators from config\n    external_indicators = get_external_indicators()\n    external_count = sum(1 for ind in external_indicators if ind in query_lower)\n\n    if external_count >= 2:\n        return QueryScope.EXTERNAL\n    elif external_count == 1:\n        return QueryScope.HYBRID\n    else:\n        return QueryScope.INTERNAL\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "0e94f8a1c9eec297",
      "chunk_type": "function",
      "relevance_score": 0.9344209911001355,
      "start_line": 280,
      "end_line": 295,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756535
    },
    {
      "content": "def _needs_agent_context(query: str, intent: QueryIntent) -> bool:\n    \"\"\"Determine if .agent/ context is relevant.\"\"\"\n    query_lower = query.lower()\n\n    # Task-related intents always need agent context\n    if intent == QueryIntent.TASK:\n        return True\n\n    # Check for agent-specific keywords from config\n    agent_indicators = get_agent_indicators()\n    for indicator in agent_indicators:\n        if indicator in query_lower:\n            return True\n\n    return False\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "903266c1bc42ae68",
      "chunk_type": "function",
      "relevance_score": 0.9338803476360247,
      "start_line": 296,
      "end_line": 312,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7565398
    },
    {
      "content": "def _suggest_sets(intent: QueryIntent, needs_agent: bool, keywords: List[str]) -> List[str]:\n    \"\"\"Suggest analysis sets based on query profile.\"\"\"\n    sets = []\n\n    # Intent-based suggestions\n    intent_sets = {\n        QueryIntent.ARCHITECTURE: [\"pipeline\", \"theory\", \"architecture_review\"],\n        QueryIntent.DEBUG: [\"pipeline\", \"classifiers\", \"tests\"],\n        QueryIntent.VALIDATE: [\"research_validation\", \"tests\"],\n        QueryIntent.TASK: [\"agent_tasks\", \"agent_kernel\"],\n        QueryIntent.COUNT: [],  # Use INSTANT tier (truths)\n        QueryIntent.LOCATE: [],  # Use RAG tier\n        QueryIntent.EXPLAIN: [\"theory\", \"docs_core\"],\n        QueryIntent.IMPLEMENT: [\"pipeline\", \"classifiers\"],\n        QueryIntent.RESEARCH: [],  # External research\n        QueryIntent.UNKNOWN: [\"theory\"],\n    }\n\n    sets.extend(intent_sets.get(intent, []))\n\n    # Add agent sets if needed\n    if needs_agent:\n        if \"agent_tasks\" not in sets:\n            sets.insert(0, \"agent_tasks\")\n        if \"agent_kernel\" not in sets:\n            sets.insert(0, \"agent_kernel\")\n\n    # Keyword-based additions\n    keyword_sets = {\n        \"viz\": \"viz_core\",\n        \"visualization\": \"viz_core\",\n        \"constraint\": \"constraints\",\n        \"role\": \"role_registry\",\n        \"atom\": \"research_atoms\",\n        \"classifier\": \"classifiers\",\n        \"schema\": \"schema\",\n    }\n\n    for kw in keywords:\n        if kw in keyword_sets and keyword_sets[kw] not in sets:\n            sets.append(keyword_sets[kw])\n\n    return sets[:5]  # Limit to 5 suggestions\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "cc481c6c319ee5b9",
      "chunk_type": "function",
      "relevance_score": 0.9593195607847748,
      "start_line": 313,
      "end_line": 357,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756545
    },
    {
      "content": "def analyze_query(query: str) -> QueryProfile:\n    \"\"\"\n    Analyze a query and return a complete profile.\n\n    Args:\n        query: The user's question or instruction\n\n    Returns:\n        QueryProfile with intent, complexity, scope, and suggestions\n    \"\"\"\n    keywords = _extract_keywords(query)\n    intent, confidence = _detect_intent(query, keywords)\n    complexity = _detect_complexity(query, keywords)\n    scope = _detect_scope(query, keywords)\n    needs_agent = _needs_agent_context(query, intent)\n    suggested_sets = _suggest_sets(intent, needs_agent, keywords)\n\n    return QueryProfile(\n        query=query,\n        intent=intent,\n        complexity=complexity,\n        scope=scope,\n        needs_agent_context=needs_agent,\n        suggested_sets=suggested_sets,\n        keywords=keywords,\n        confidence=confidence\n    )\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "4e580817e9734c16",
      "chunk_type": "function",
      "relevance_score": 0.9460843237741802,
      "start_line": 358,
      "end_line": 386,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7565498
    },
    {
      "content": "def is_agent_query(query: str) -> bool:\n    \"\"\"Quick check if query relates to agent/task management.\"\"\"\n    profile = analyze_query(query)\n    return profile.needs_agent_context\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "799b895c26dd44e7",
      "chunk_type": "function",
      "relevance_score": 0.9125210001154448,
      "start_line": 388,
      "end_line": 393,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.756555
    },
    {
      "content": "def is_external_query(query: str) -> bool:\n    \"\"\"Quick check if query needs external knowledge.\"\"\"\n    profile = analyze_query(query)\n    return profile.scope in (QueryScope.EXTERNAL, QueryScope.HYBRID)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/query_analyzer.py",
      "chunk_id": "e59b5cd3bf96913e",
      "chunk_type": "function",
      "relevance_score": 0.9153748018956607,
      "start_line": 394,
      "end_line": 398,
      "metadata": {
        "file_type": ".py",
        "file_name": "query_analyzer.py"
      },
      "created_at": 1769237032.7565591
    },
    {
      "content": "    from aci import analyze_and_route, ACI_CONFIG",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/__init__.py",
      "chunk_id": "bd970c478366fda1",
      "chunk_type": "imports",
      "relevance_score": 0.55,
      "start_line": 1,
      "end_line": 10,
      "metadata": {
        "file_type": ".py",
        "file_name": "__init__.py"
      },
      "created_at": 1769237032.7566762
    },
    {
      "content": "def _load_aci_config() -> Dict[str, Any]:\n    \"\"\"Load ACI configuration from YAML file.\"\"\"\n    # Navigate from aci/ -> ai/ -> tools/ -> context-management/ -> config/\n    config_path = Path(__file__).parent.parent.parent.parent / \"config\" / \"aci_config.yaml\"\n\n    if not config_path.exists():\n        print(f\"[ACI] Warning: Config not found at {config_path}, using defaults\")\n        return {}\n\n    try:\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f) or {}\n            return config\n    except Exception as e:\n        print(f\"[ACI] Warning: Failed to load config: {e}\")\n        return {}\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/__init__.py",
      "chunk_id": "5bd77e9e3550645e",
      "chunk_type": "function",
      "relevance_score": 0.9396545800088291,
      "start_line": 31,
      "end_line": 47,
      "metadata": {
        "file_type": ".py",
        "file_name": "__init__.py"
      },
      "created_at": 1769237032.756682
    },
    {
      "content": "def analyze_and_route(query: str, force_tier: str = \"\"):\n    \"\"\"\n    Convenience function: analyze query and route to appropriate tier.\n\n    Args:\n        query: The user's question\n        force_tier: Optional tier override (\"instant\", \"rag\", \"long_context\", \"perplexity\")\n\n    Returns:\n        RoutingDecision with tier, sets, and reasoning\n    \"\"\"\n    forced = tier_from_string(force_tier) if force_tier else None\n    return route_query(query, force_tier=forced)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/__init__.py",
      "chunk_id": "203cfa72016a33e6",
      "chunk_type": "function",
      "relevance_score": 0.9333726476444978,
      "start_line": 129,
      "end_line": 142,
      "metadata": {
        "file_type": ".py",
        "file_name": "__init__.py"
      },
      "created_at": 1769237032.7566872
    },
    {
      "content": "from enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom .query_analyzer import (",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "f8e53b195ece36ad",
      "chunk_type": "imports",
      "relevance_score": 0.6550185272558782,
      "start_line": 1,
      "end_line": 21,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757099
    },
    {
      "content": "def get_model_token_limit(model_name: str, default: int = 1_000_000) -> int:\n    \"\"\"\n    Get input token limit for a Gemini model.\n\n    Queries the Gemini API for the model's input_token_limit.\n    Falls back to sensible defaults if API call fails.\n\n    Args:\n        model_name: Model identifier (e.g., \"gemini-2.0-flash\")\n        default: Fallback limit if API unavailable\n\n    Returns:\n        Token limit as int, or default value on errors\n    \"\"\"\n    try:\n        from google import genai\n        client = genai.Client()\n        model_info = client.models.get(model=model_name)\n        return model_info.input_token_limit\n    except Exception:\n        # Fall back to known defaults\n        return _TOKEN_LIMIT_DEFAULTS.get(model_name, default)\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "d2c29347d7f034fb",
      "chunk_type": "function",
      "relevance_score": 0.9436950798932231,
      "start_line": 44,
      "end_line": 67,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757108
    },
    {
      "content": "def get_tier_token_limit(tier: \"Tier\") -> int:\n    \"\"\"\n    Get recommended token limit for a tier.\n\n    Dynamically queries the Gemini API to get the appropriate token limit\n    for a tier's primary model. Falls back to sensible defaults on errors.\n\n    Args:\n        tier: The execution tier\n\n    Returns:\n        Recommended token limit for that tier\n    \"\"\"\n    tier_models = {\n        \"instant\": \"gemini-3-pro\",\n        \"rag\": \"gemini-3-pro\",\n        \"long_context\": \"gemini-3-pro\",\n        \"perplexity\": None,  # External service, not applicable\n        \"flash_deep\": \"gemini-2.0-flash-thinking-exp\",\n        \"hybrid\": \"gemini-2.0-flash-thinking-exp\",\n    }\n\n    model = tier_models.get(tier.value)\n    if model is None:\n        return 1_000_000  # Default for external services\n\n    return get_model_token_limit(model)\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "3d4f213a986bee3f",
      "chunk_type": "function",
      "relevance_score": 0.9457963605848558,
      "start_line": 68,
      "end_line": 96,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.7571142
    },
    {
      "content": "class Tier(Enum):\n    \"\"\"Execution tiers for query handling.\"\"\"\n    INSTANT = \"instant\"           # Tier 0: Cached truths, no AI call\n    RAG = \"rag\"                   # Tier 1: File Search with citations\n    LONG_CONTEXT = \"long_context\" # Tier 2: Gemini 3 Pro (1M context)\n    PERPLEXITY = \"perplexity\"     # Tier 3: External web research\n    FLASH_DEEP = \"flash_deep\"     # Tier 4: Gemini 2.0 Flash (2M context) - massive analysis\n    HYBRID = \"hybrid\"             # Multi-tier execution\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "9098c37ca36bdb65",
      "chunk_type": "class",
      "relevance_score": 0.9845098040014256,
      "start_line": 97,
      "end_line": 106,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757119
    },
    {
      "content": "class RoutingDecision:\n    \"\"\"Complete routing decision for a query.\"\"\"\n    tier: Tier\n    primary_sets: List[str]      # Analysis sets to use\n    fallback_tier: Optional[Tier] # If primary fails\n    use_truths: bool             # Check repo_truths.yaml first\n    inject_agent: bool           # Auto-inject agent_* sets\n    reasoning: str               # Why this tier was chosen\n    # Semantic matching (graph-based context selection)\n    semantic: Optional[SemanticMatch] = None\n    context_flow: str = \"turbulent\"  # \"laminar\" (coherent) or \"turbulent\" (mixed)\n    traversal_direction: EdgeDirection = EdgeDirection.BOTH\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "0781c9f644ca2334",
      "chunk_type": "class",
      "relevance_score": 0.9897244023329584,
      "start_line": 108,
      "end_line": 121,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.7571251
    },
    {
      "content": "def _is_flash_deep_query(query: str) -> tuple[bool, str]:\n    \"\"\"\n    Detect if query needs FLASH_DEEP tier (2M context).\n\n    Returns:\n        (should_use_flash_deep, reason)\n    \"\"\"\n    query_lower = query.lower()\n\n    # Check for trigger keywords\n    for trigger in FLASH_DEEP_TRIGGERS:\n        if trigger in query_lower:\n            return True, f\"Keyword '{trigger}' detected - needs massive context\"\n\n    # Check for multiple explicit set requests (would exceed 1M)\n    set_mentions = sum(1 for word in [\"pipeline\", \"theory\", \"architecture\", \"agent\", \"visualization\"]\n                       if word in query_lower)\n    if set_mentions >= 3:\n        return True, f\"Multiple domains ({set_mentions}) requested - needs massive context\"\n\n    return False, \"\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "a32d116ab49736a9",
      "chunk_type": "function",
      "relevance_score": 0.9440406796140396,
      "start_line": 213,
      "end_line": 235,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.75713
    },
    {
      "content": "def _get_fallback_tier(tier: Tier) -> Optional[Tier]:\n    \"\"\"Get fallback tier if primary fails.\"\"\"\n    fallbacks = {\n        Tier.INSTANT: Tier.RAG,\n        Tier.RAG: Tier.LONG_CONTEXT,\n        Tier.LONG_CONTEXT: Tier.FLASH_DEEP,  # Escalate to larger context\n        Tier.FLASH_DEEP: None,  # No fallback - maximum capacity\n        Tier.PERPLEXITY: Tier.LONG_CONTEXT,  # Fall back to internal\n        Tier.HYBRID: Tier.LONG_CONTEXT,\n    }\n    return fallbacks.get(tier)\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "930c4d684a79103d",
      "chunk_type": "function",
      "relevance_score": 0.9336510453564448,
      "start_line": 236,
      "end_line": 248,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.7571788
    },
    {
      "content": "def _should_use_truths(intent: QueryIntent, complexity: QueryComplexity) -> bool:\n    \"\"\"Determine if repo_truths.yaml should be checked first.\"\"\"\n    # COUNT queries always check truths\n    if intent == QueryIntent.COUNT:\n        return True\n\n    # Simple queries might benefit from truths\n    if complexity == QueryComplexity.SIMPLE:\n        return intent in (QueryIntent.LOCATE, QueryIntent.EXPLAIN)\n\n    return False\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "54450a28865fb586",
      "chunk_type": "function",
      "relevance_score": 0.931162464519895,
      "start_line": 249,
      "end_line": 261,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.7572238
    },
    {
      "content": "def _determine_sets(profile: QueryProfile, tier: Tier) -> List[str]:\n    \"\"\"Determine which analysis sets to use.\"\"\"\n    sets = []\n\n    # Start with suggested sets from analyzer\n    sets.extend(profile.suggested_sets)\n\n    # Tier-specific adjustments\n    if tier == Tier.RAG:\n        # RAG works best with focused sets\n        if not sets:\n            sets = [\"pipeline\", \"classifiers\"]\n    elif tier == Tier.LONG_CONTEXT:\n        # Long context can handle larger sets\n        if profile.intent == QueryIntent.TASK:\n            # Task queries need full agent context\n            sets = [\"agent_full\"] + [s for s in sets if not s.startswith(\"agent_\")]\n        elif profile.intent == QueryIntent.ARCHITECTURE:\n            if \"architecture_review\" not in sets:\n                sets.append(\"architecture_review\")\n    elif tier == Tier.FLASH_DEEP:\n        # FLASH_DEEP: Load EVERYTHING - 2M context capacity\n        # Include all major sets for comprehensive analysis\n        comprehensive_sets = [\n            \"pipeline\", \"theory\", \"architecture_review\",\n            \"agent_full\", \"visualization\", \"classifiers\"\n        ]\n        # Start with comprehensive sets, add any suggested that aren't included\n        sets = comprehensive_sets + [s for s in sets if s not in comprehensive_sets]\n\n    # Ensure we have at least one set\n    if not sets:\n        sets = [\"theory\"]\n\n    # Limit based on tier capacity\n    max_sets = 10 if tier == Tier.FLASH_DEEP else 5\n    return sets[:max_sets]\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "64925a52a8be57d0",
      "chunk_type": "function",
      "relevance_score": 0.9584984086998447,
      "start_line": 262,
      "end_line": 300,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.7572422
    },
    {
      "content": "def route_query(query: str, force_tier: Optional[Tier] = None, use_semantic: bool = True) -> RoutingDecision:\n    \"\"\"\n    Route a query to the appropriate execution tier.\n\n    This is the AUTO CONTEXT LOADER - it activates automatically on query input\n    and uses the SMoC relationship graph for intelligent context selection.\n\n    Args:\n        query: The user's question or instruction\n        force_tier: Optional tier override\n        use_semantic: Whether to use semantic matching (default True)\n\n    Returns:\n        RoutingDecision with tier, sets, semantic match, and reasoning\n    \"\"\"\n    # Analyze the query\n    profile = analyze_query(query)\n\n    # Perform semantic matching for graph-based context selection\n    sem_match = semantic_match(query) if use_semantic else None\n\n    # Handle forced tier\n    if force_tier is not None:\n        sets = _determine_sets(profile, force_tier)\n        # Merge semantic suggestions if available\n        if sem_match and sem_match.suggested_sets:\n            sets = _merge_sets_ordered(sem_match.suggested_sets, sets)\n        return RoutingDecision(\n            tier=force_tier,\n            primary_sets=sets,\n            fallback_tier=_get_fallback_tier(force_tier),\n            use_truths=force_tier == Tier.INSTANT,\n            inject_agent=profile.needs_agent_context,\n            reasoning=f\"Forced to {force_tier.value} tier\",\n            semantic=sem_match,\n            context_flow=sem_match.context_flow if sem_match else \"turbulent\",\n            traversal_direction=sem_match.targets[0].direction if sem_match and sem_match.targets else EdgeDirection.BOTH,\n        )\n\n    # Check for FLASH_DEEP triggers FIRST (before routing matrix)\n    # This allows comprehensive queries to bypass the matrix\n    is_flash_deep, flash_reason = _is_flash_deep_query(query)\n    if is_flash_deep and profile.scope == QueryScope.INTERNAL:\n        tier = Tier.FLASH_DEEP\n        reasoning = f\"FLASH_DEEP: {flash_reason}\"\n    else:\n        # Look up in routing matrix\n        key = (profile.intent, profile.complexity, profile.scope)\n        if key in ROUTING_MATRIX:\n            tier, reasoning = ROUTING_MATRIX[key]\n        else:\n            # Default routing based on scope\n            if profile.scope == QueryScope.EXTERNAL:\n                tier = Tier.PERPLEXITY\n                reasoning = \"External scope - defaulting to Perplexity\"\n            elif profile.scope == QueryScope.HYBRID:\n                tier = Tier.HYBRID\n                reasoning = \"Hybrid scope - need internal and external\"\n            else:\n                # Default to LONG_CONTEXT for internal queries\n                tier = Tier.LONG_CONTEXT\n                reasoning = \"Default routing to long context\"\n\n    # Determine sets - use semantic matching first, then profile\n    primary_sets = _determine_sets(profile, tier)\n\n    # Merge semantic suggestions (semantic-first for laminar flow)\n    if sem_match and sem_match.suggested_sets:\n        if sem_match.context_flow == \"laminar\":\n            # Laminar flow: semantic sets take priority (coherent context)\n            primary_sets = _merge_sets_ordered(sem_match.suggested_sets, primary_sets)\n        else:\n            # Turbulent flow: profile sets take priority (broader context)\n            primary_sets = _merge_sets_ordered(primary_sets, sem_match.suggested_sets)\n\n    use_truths = _should_use_truths(profile.intent, profile.complexity)\n    inject_agent = profile.needs_agent_context\n\n    # If agent context needed, ensure agent sets are included\n    if inject_agent and tier in (Tier.LONG_CONTEXT, Tier.HYBRID, Tier.FLASH_DEEP):\n        agent_sets = [\"agent_kernel\", \"agent_tasks\"]\n        for s in agent_sets:\n            if s not in primary_sets:\n                primary_sets.insert(0, s)\n\n    # Enrich reasoning with semantic context\n    if sem_match and sem_match.reasoning:\n        reasoning = f\"{reasoning} | Semantic: {sem_match.reasoning}\"\n\n    return RoutingDecision(\n        tier=tier,\n        primary_sets=primary_sets,\n        fallback_tier=_get_fallback_tier(tier),\n        use_truths=use_truths,\n        inject_agent=inject_agent,\n        reasoning=reasoning,\n        semantic=sem_match,\n        context_flow=sem_match.context_flow if sem_match else \"turbulent\",\n        traversal_direction=sem_match.targets[0].direction if sem_match and sem_match.targets else EdgeDirection.BOTH,\n    )\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "1f443f52077b5c67",
      "chunk_type": "function",
      "relevance_score": 0.9820588273306557,
      "start_line": 301,
      "end_line": 402,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.75729
    },
    {
      "content": "def _merge_sets_ordered(priority_sets: List[str], secondary_sets: List[str]) -> List[str]:\n    \"\"\"Merge two set lists, preserving order with priority_sets first.\"\"\"\n    seen = set()\n    result = []\n    for s in priority_sets:\n        if s not in seen:\n            seen.add(s)\n            result.append(s)\n    for s in secondary_sets:\n        if s not in seen:\n            seen.add(s)\n            result.append(s)\n    return result[:5]  # Limit to 5 sets\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "2c46ad0867f603e7",
      "chunk_type": "function",
      "relevance_score": 0.9328049101006416,
      "start_line": 403,
      "end_line": 417,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757327
    },
    {
      "content": "def tier_from_string(tier_str: str) -> Optional[Tier]:\n    \"\"\"Convert string to Tier enum.\"\"\"\n    tier_map = {\n        \"instant\": Tier.INSTANT,\n        \"rag\": Tier.RAG,\n        \"long_context\": Tier.LONG_CONTEXT,\n        \"long-context\": Tier.LONG_CONTEXT,\n        \"longcontext\": Tier.LONG_CONTEXT,\n        \"perplexity\": Tier.PERPLEXITY,\n        \"flash_deep\": Tier.FLASH_DEEP,\n        \"flash-deep\": Tier.FLASH_DEEP,\n        \"flashdeep\": Tier.FLASH_DEEP,\n        \"deep\": Tier.FLASH_DEEP,  # Shorthand\n        \"hybrid\": Tier.HYBRID,\n    }\n    return tier_map.get(tier_str.lower())\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "21f5ecd99102d9db",
      "chunk_type": "function",
      "relevance_score": 0.9380211241711607,
      "start_line": 418,
      "end_line": 435,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757338
    },
    {
      "content": "def format_routing_decision(decision: RoutingDecision) -> str:\n    \"\"\"Format routing decision for display.\"\"\"\n    lines = [\n        f\"Tier: {decision.tier.value.upper()}\",\n        f\"Sets: {', '.join(decision.primary_sets)}\",\n        f\"Context Flow: {decision.context_flow}\",\n        f\"Reason: {decision.reasoning}\",\n    ]\n\n    if decision.fallback_tier:\n        lines.append(f\"Fallback: {decision.fallback_tier.value}\")\n    if decision.use_truths:\n        lines.append(\"Will check repo_truths.yaml first\")\n    if decision.inject_agent:\n        lines.append(\"Agent context will be auto-injected\")\n\n    # Semantic matching details\n    if decision.semantic:\n        lines.append(\"\")\n        lines.append(\"Semantic Match:\")\n        if decision.semantic.targets:\n            for t in decision.semantic.targets:\n                parts = []\n                if t.purpose:\n                    parts.append(f\"\u03c0\u2082={t.purpose}\")\n                if t.layer:\n                    parts.append(f\"layer={t.layer}\")\n                if t.roles:\n                    parts.append(f\"roles=[{','.join(t.roles[:2])}]\")\n                lines.append(f\"  Target: {' '.join(parts)}\")\n        lines.append(f\"  Traversal: {decision.traversal_direction.value}\")\n\n    return \"\\n\".join(lines)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/tier_router.py",
      "chunk_id": "c7eeedcd437ec758",
      "chunk_type": "function",
      "relevance_score": 0.9549667638842979,
      "start_line": 436,
      "end_line": 469,
      "metadata": {
        "file_type": ".py",
        "file_name": "tier_router.py"
      },
      "created_at": 1769237032.757348
    },
    {
      "content": "    from aci.refinery import Refinery",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "6fba1844e2ee1410",
      "chunk_type": "imports",
      "relevance_score": 0.55,
      "start_line": 1,
      "end_line": 14,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.7575831
    },
    {
      "content": "class RefineryNode:\n    \"\"\"Atomic chunk with full metadata.\"\"\"\n    content: str                    # The chunk text\n    source_file: str                # Origin file path\n    chunk_id: str                   # Unique ID (SHA256-based)\n    chunk_type: str                 # Type: function, class, section, config_block, etc.\n    relevance_score: float = 0.0   # 0.0-1.0 relevance score\n    start_line: int = 0            # Line number in source (if applicable)\n    end_line: int = 0              # End line number\n    metadata: Dict[str, Any] = field(default_factory=dict)  # Additional metadata\n    created_at: float = field(default_factory=time.time)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to JSON-serializable dict.\"\"\"\n        return asdict(self)\n\n    @property\n    def token_estimate(self) -> int:\n        \"\"\"Rough token count estimate (chars / 4).\"\"\"\n        return len(self.content) // 4\n\n    def __repr__(self) -> str:\n        return f\"RefineryNode({self.chunk_type}:{self.chunk_id[:8]}... {self.token_estimate}tok)\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "bae083a43fd8ee7d",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 46,
      "end_line": 70,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.75759
    },
    {
      "content": "class FileChunker(ABC):\n    \"\"\"Base class for file-specific chunking strategies.\"\"\"\n\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n        self.content = \"\"\n        self._load()\n\n    def _load(self):\n        \"\"\"Load file content.\"\"\"\n        try:\n            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n                self.content = f.read()\n        except Exception as e:\n            logger.error(f\"Failed to load {self.file_path}: {e}\")\n            self.content = \"\"\n\n    @abstractmethod\n    def chunk(self) -> List[Tuple[str, str, int, int]]:\n        \"\"\"\n        Split file into semantic chunks.\n\n        Returns:\n            List of (content, chunk_type, start_line, end_line) tuples\n        \"\"\"\n        pass\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "77b2bb51d82bcdb9",
      "chunk_type": "class",
      "relevance_score": 0.994324536258624,
      "start_line": 71,
      "end_line": 98,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.7575989
    },
    {
      "content": "class PythonChunker(FileChunker):\n    \"\"\"Chunks Python files by semantic units (functions, classes, imports).\"\"\"\n\n    # Patterns for Python constructs\n    PATTERNS = {\n        'class': re.compile(r'^class\\s+(\\w+)', re.MULTILINE),\n        'function': re.compile(r'^(?:async\\s+)?def\\s+(\\w+)', re.MULTILINE),\n        'import_block': re.compile(r'^(?:import|from)\\s+.+$', re.MULTILINE),\n    }\n\n    def chunk(self) -> List[Tuple[str, str, int, int]]:\n        \"\"\"Split Python file into functions, classes, and import blocks.\"\"\"\n        chunks = []\n        lines = self.content.split('\\n')\n\n        if not lines:\n            return chunks\n\n        # Extract imports at top\n        import_lines = []\n        import_end = 0\n        for i, line in enumerate(lines):\n            stripped = line.strip()\n            if stripped.startswith(('import ', 'from ')):\n                import_lines.append(line)\n                import_end = i + 1\n            elif stripped and not stripped.startswith('#') and import_lines:\n                break\n\n        if import_lines:\n            chunks.append(('\\n'.join(import_lines), 'imports', 1, import_end))\n\n        # Find class and function definitions\n        current_def = None\n        current_start = 0\n        current_type = None\n        buffer: List[str] = []\n\n        for i, line in enumerate(lines):\n            # Detect new definition at column 0\n            if line and not line[0].isspace():\n                # Save previous definition if exists\n                if current_def and buffer:\n                    content = '\\n'.join(buffer)\n                    chunks.append((content, current_type, current_start + 1, i))\n                    buffer = []\n\n                # Check for class\n                class_match = self.PATTERNS['class'].match(line)\n                if class_match:\n                    current_def = class_match.group(1)\n                    current_type = 'class'\n                    current_start = i\n                    buffer = [line]\n                    continue\n\n                # Check for function\n                func_match = self.PATTERNS['function'].match(line)\n                if func_match:\n                    current_def = func_match.group(1)\n                    current_type = 'function'\n                    current_start = i\n                    buffer = [line]\n                    continue\n\n                # Neither - reset\n                if current_def:\n                    content = '\\n'.join(buffer)\n                    chunks.append((content, current_type, current_start + 1, i))\n                current_def = None\n                buffer = []\n            elif current_def:\n                buffer.append(line)\n\n        # Don't forget last definition\n        if current_def and buffer:\n            content = '\\n'.join(buffer)\n            chunks.append((content, current_type, current_start + 1, len(lines)))\n\n        return chunks\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "4e1666748bada2e3",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 99,
      "end_line": 180,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.757605
    },
    {
      "content": "class MarkdownChunker(FileChunker):\n    \"\"\"Chunks Markdown files by sections (headers).\"\"\"\n\n    HEADER_PATTERN = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n\n    def chunk(self) -> List[Tuple[str, str, int, int]]:\n        \"\"\"Split Markdown into sections based on headers.\"\"\"\n        chunks = []\n        lines = self.content.split('\\n')\n\n        if not lines:\n            return chunks\n\n        current_section = []\n        current_level = 0\n        section_start = 0\n\n        for i, line in enumerate(lines):\n            header_match = self.HEADER_PATTERN.match(line)\n            if header_match:\n                # Save previous section\n                if current_section:\n                    content = '\\n'.join(current_section)\n                    chunk_type = f\"h{current_level}\" if current_level else \"preamble\"\n                    chunks.append((content, chunk_type, section_start + 1, i))\n\n                # Start new section\n                current_level = len(header_match.group(1))\n                current_section = [line]\n                section_start = i\n            else:\n                current_section.append(line)\n\n        # Last section\n        if current_section:\n            content = '\\n'.join(current_section)\n            chunk_type = f\"h{current_level}\" if current_level else \"preamble\"\n            chunks.append((content, chunk_type, section_start + 1, len(lines)))\n\n        return chunks\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "8ef29e283ae41369",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 181,
      "end_line": 222,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.757611
    },
    {
      "content": "class YamlChunker(FileChunker):\n    \"\"\"Chunks YAML files by top-level keys.\"\"\"\n\n    TOP_KEY_PATTERN = re.compile(r'^(\\w[\\w\\-]*):')\n\n    def chunk(self) -> List[Tuple[str, str, int, int]]:\n        \"\"\"Split YAML into top-level key blocks.\"\"\"\n        chunks = []\n        lines = self.content.split('\\n')\n\n        if not lines:\n            return chunks\n\n        current_key = None\n        current_block = []\n        block_start = 0\n\n        for i, line in enumerate(lines):\n            # Skip comments and empty at start\n            if not line.strip() or line.strip().startswith('#'):\n                if current_key:\n                    current_block.append(line)\n                continue\n\n            # Check for top-level key (no indent)\n            if line and not line[0].isspace():\n                key_match = self.TOP_KEY_PATTERN.match(line)\n                if key_match:\n                    # Save previous block\n                    if current_key and current_block:\n                        content = '\\n'.join(current_block)\n                        chunks.append((content, f\"yaml_key:{current_key}\", block_start + 1, i))\n\n                    current_key = key_match.group(1)\n                    current_block = [line]\n                    block_start = i\n                    continue\n\n            if current_key:\n                current_block.append(line)\n\n        # Last block\n        if current_key and current_block:\n            content = '\\n'.join(current_block)\n            chunks.append((content, f\"yaml_key:{current_key}\", block_start + 1, len(lines)))\n\n        return chunks\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "29d025ffd62074b7",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 223,
      "end_line": 271,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.757616
    },
    {
      "content": "class GenericChunker(FileChunker):\n    \"\"\"Generic chunker for unknown file types - splits by blank lines.\"\"\"\n\n    def chunk(self) -> List[Tuple[str, str, int, int]]:\n        \"\"\"Split by paragraph (double newlines).\"\"\"\n        chunks = []\n\n        paragraphs = re.split(r'\\n\\s*\\n', self.content)\n        line_num = 1\n\n        for para in paragraphs:\n            if para.strip():\n                para_lines = para.count('\\n') + 1\n                chunks.append((para, 'paragraph', line_num, line_num + para_lines - 1))\n                line_num += para_lines + 1  # +1 for blank line\n            else:\n                line_num += 1\n\n        return chunks\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "fd13a7adfb2e0057",
      "chunk_type": "class",
      "relevance_score": 0.9906456678321428,
      "start_line": 272,
      "end_line": 292,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.757621
    },
    {
      "content": "class Refinery:\n    \"\"\"\n    Main context atomization engine.\n\n    Orchestrates:\n    1. File type detection and chunking\n    2. Relevance scoring\n    3. Caching of processed chunks\n    4. Export to JSON\n    \"\"\"\n\n    def __init__(self, cache_dir: Path = CHUNK_REGISTRY_DIR):\n        self.cache_dir = cache_dir\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self._chunk_cache: Dict[str, List[RefineryNode]] = {}\n\n    def _get_chunker(self, file_path: str) -> FileChunker:\n        \"\"\"Get appropriate chunker for file type.\"\"\"\n        path = Path(file_path)\n        suffix = path.suffix.lower()\n\n        if suffix == '.py':\n            return PythonChunker(file_path)\n        elif suffix in ('.md', '.markdown'):\n            return MarkdownChunker(file_path)\n        elif suffix in ('.yaml', '.yml'):\n            return YamlChunker(file_path)\n        else:\n            return GenericChunker(file_path)\n\n    def _generate_chunk_id(self, file_path: str, content: str) -> str:\n        \"\"\"Generate unique ID for chunk based on content and source.\"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(file_path.encode('utf-8'))\n        hasher.update(content.encode('utf-8'))\n        return hasher.hexdigest()[:16]\n\n    def _score_relevance(self, content: str, chunk_type: str) -> float:\n        \"\"\"\n        Score chunk relevance (0.0 - 1.0).\n\n        Heuristics:\n        - Longer content = more relevant (up to a point)\n        - Certain types score higher (class > function > imports)\n        - Docstrings boost score\n        - Comments in code slightly reduce score (noise)\n        \"\"\"\n        score = 0.5  # Base score\n\n        # Type bonuses\n        type_weights = {\n            'class': 0.2,\n            'function': 0.15,\n            'h1': 0.2,\n            'h2': 0.15,\n            'h3': 0.1,\n            'imports': 0.05,\n            'yaml_key': 0.1,\n            'paragraph': 0.0,\n            'preamble': 0.05,\n        }\n\n        # Check for partial type matches (like yaml_key:xxx)\n        base_type = chunk_type.split(':')[0] if ':' in chunk_type else chunk_type\n        score += type_weights.get(base_type, 0.0)\n\n        # Length score (logarithmic, caps at ~1000 chars)\n        content_len = len(content.strip())\n        if content_len > 50:\n            import math\n            length_score = min(0.2, 0.05 * math.log10(content_len))\n            score += length_score\n\n        # Docstring bonus for Python\n        if '\"\"\"' in content or \"'''\" in content:\n            score += 0.1\n\n        # Type hint bonus\n        if '->' in content or ': ' in content:\n            score += 0.05\n\n        # Clamp to 0.0 - 1.0\n        return max(0.0, min(1.0, score))\n\n    def process_file(self, file_path: str, use_cache: bool = True) -> List[RefineryNode]:\n        \"\"\"\n        Atomize a file into RefineryNodes.\n\n        Args:\n            file_path: Path to file to process\n            use_cache: Whether to use/update cache\n\n        Returns:\n            List of RefineryNode objects\n        \"\"\"\n        file_path = str(Path(file_path).resolve())\n\n        # Check cache\n        if use_cache and file_path in self._chunk_cache:\n            logger.info(f\"Cache hit for {file_path}\")\n            return self._chunk_cache[file_path]\n\n        # Get appropriate chunker\n        chunker = self._get_chunker(file_path)\n\n        if not chunker.content:\n            logger.warning(f\"Empty or unreadable file: {file_path}\")\n            return []\n\n        # Chunk the file\n        raw_chunks = chunker.chunk()\n\n        # Build RefineryNodes\n        nodes = []\n        for content, chunk_type, start_line, end_line in raw_chunks:\n            if not content.strip():\n                continue\n\n            chunk_id = self._generate_chunk_id(file_path, content)\n            relevance = self._score_relevance(content, chunk_type)\n\n            node = RefineryNode(\n                content=content,\n                source_file=file_path,\n                chunk_id=chunk_id,\n                chunk_type=chunk_type,\n                relevance_score=relevance,\n                start_line=start_line,\n                end_line=end_line,\n                metadata={\n                    'file_type': Path(file_path).suffix,\n                    'file_name': Path(file_path).name,\n                }\n            )\n            nodes.append(node)\n\n        logger.info(f\"Processed {file_path}: {len(nodes)} chunks\")\n\n        # Update cache\n        if use_cache:\n            self._chunk_cache[file_path] = nodes\n\n        return nodes\n\n    def process_directory(\n        self,\n        dir_path: str,\n        extensions: List[str] = None,\n        max_files: int = 100\n    ) -> List[RefineryNode]:\n        \"\"\"\n        Process all matching files in a directory.\n\n        Args:\n            dir_path: Directory to scan\n            extensions: File extensions to include (default: .py, .md, .yaml, .yml)\n            max_files: Maximum files to process (safety limit)\n\n        Returns:\n            Combined list of RefineryNodes from all files\n        \"\"\"\n        if extensions is None:\n            extensions = ['.py', '.md', '.yaml', '.yml']\n\n        dir_path_obj = Path(dir_path)\n        all_nodes: List[RefineryNode] = []\n        file_count = 0\n\n        for ext in extensions:\n            for file_path in dir_path_obj.rglob(f'*{ext}'):\n                if file_count >= max_files:\n                    logger.warning(f\"Reached max_files limit ({max_files})\")\n                    break\n\n                # Skip hidden files and common excludes\n                if any(part.startswith('.') for part in file_path.parts):\n                    continue\n                if 'node_modules' in str(file_path) or '__pycache__' in str(file_path):\n                    continue\n\n                nodes = self.process_file(str(file_path))\n                all_nodes.extend(nodes)\n                file_count += 1\n\n        logger.info(f\"Processed {file_count} files, {len(all_nodes)} total chunks\")\n        return all_nodes\n\n    def export_to_json(self, nodes: List[RefineryNode], output_path: str):\n        \"\"\"Export nodes to JSON file.\"\"\"\n        data = {\n            'exported_at': time.time(),\n            'node_count': len(nodes),\n            'total_tokens': sum(n.token_estimate for n in nodes),\n            'nodes': [n.to_dict() for n in nodes]\n        }\n\n        output_path_obj = Path(output_path)\n        output_path_obj.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(output_path_obj, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2)\n\n        logger.info(f\"Exported {len(nodes)} nodes to {output_path}\")\n\n    def filter_by_relevance(\n        self,\n        nodes: List[RefineryNode],\n        min_score: float = 0.5\n    ) -> List[RefineryNode]:\n        \"\"\"Filter nodes by minimum relevance score.\"\"\"\n        return [n for n in nodes if n.relevance_score >= min_score]\n\n    def select_top_k(\n        self,\n        nodes: List[RefineryNode],\n        k: int = 10\n    ) -> List[RefineryNode]:\n        \"\"\"Select top K nodes by relevance.\"\"\"\n        return sorted(nodes, key=lambda n: n.relevance_score, reverse=True)[:k]\n\n    def compact_for_context(\n        self,\n        nodes: List[RefineryNode],\n        max_tokens: int = 50000\n    ) -> List[RefineryNode]:\n        \"\"\"\n        Select nodes to fit within token budget.\n\n        Strategy: Greedy selection by relevance until budget exhausted.\n        \"\"\"\n        sorted_nodes = sorted(nodes, key=lambda n: n.relevance_score, reverse=True)\n        selected = []\n        total_tokens = 0\n\n        for node in sorted_nodes:\n            if total_tokens + node.token_estimate <= max_tokens:\n                selected.append(node)\n                total_tokens += node.token_estimate\n            elif total_tokens >= max_tokens:\n                break\n\n        logger.info(f\"Compacted to {len(selected)} nodes, ~{total_tokens} tokens\")\n        return selected\n\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Get refinery statistics.\"\"\"\n        all_nodes = []\n        for nodes in self._chunk_cache.values():\n            all_nodes.extend(nodes)\n\n        if not all_nodes:\n            return {'cached_files': 0, 'cached_chunks': 0}\n\n        return {\n            'cached_files': len(self._chunk_cache),\n            'cached_chunks': len(all_nodes),\n            'total_tokens': sum(n.token_estimate for n in all_nodes),\n            'avg_relevance': sum(n.relevance_score for n in all_nodes) / len(all_nodes),\n            'chunk_types': list(set(n.chunk_type for n in all_nodes)),\n        }\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/refinery.py",
      "chunk_id": "83475e93a6b63488",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 293,
      "end_line": 554,
      "metadata": {
        "file_type": ".py",
        "file_name": "refinery.py"
      },
      "created_at": 1769237032.757629
    },
    {
      "content": "from dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import List, Dict, Optional, Tuple\nimport re",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "2acf215011654043",
      "chunk_type": "imports",
      "relevance_score": 0.6537773480696266,
      "start_line": 1,
      "end_line": 17,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.7580318
    },
    {
      "content": "class PurposeLevel(Enum):\n    \"\"\"Hierarchical emergence levels of PURPOSE field.\"\"\"\n    PI1_ATOMIC = \"pi1\"      # = Role (Query, Validator, Repository)\n    PI2_MOLECULAR = \"pi2\"   # Emergent from effect + boundary + topology\n    PI3_ORGANELLE = \"pi3\"   # Emergent from children's \u03c0\u2082 distribution\n    PI4_SYSTEM = \"pi4\"      # Emergent from file-level \u03c0\u2083 distribution\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "1c266e0c2ed5da74",
      "chunk_type": "class",
      "relevance_score": 0.9281740542697204,
      "start_line": 25,
      "end_line": 32,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758075
    },
    {
      "content": "class EdgeDirection(Enum):\n    \"\"\"Direction for graph traversal.\"\"\"\n    UPSTREAM = \"upstream\"     # Where it comes from (providers)\n    DOWNSTREAM = \"downstream\" # Where it goes (consumers)\n    BOTH = \"both\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "7157921340f0ad99",
      "chunk_type": "class",
      "relevance_score": 0.9157985172728459,
      "start_line": 93,
      "end_line": 99,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758083
    },
    {
      "content": "class SemanticTarget:\n    \"\"\"A semantically matched target for context loading.\"\"\"\n    purpose: Optional[str] = None           # \u03c0\u2082 purpose (Retrieve, Transform, etc.)\n    layer: Optional[str] = None             # Architecture layer\n    roles: List[str] = field(default_factory=list)  # DDD roles\n    direction: EdgeDirection = EdgeDirection.BOTH   # Graph traversal direction\n    scale_focus: Optional[str] = None       # L3 (NODE), L4 (CONTAINER), L5 (FILE)\n    confidence: float = 0.0                 # Match confidence\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "572d48e39f816d2b",
      "chunk_type": "class",
      "relevance_score": 0.9858835251501131,
      "start_line": 215,
      "end_line": 224,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758092
    },
    {
      "content": "class SemanticMatch:\n    \"\"\"Result of semantic query matching.\"\"\"\n    query: str\n    targets: List[SemanticTarget]           # Matched semantic targets\n    suggested_files: List[str]              # File patterns to include\n    suggested_sets: List[str]               # Analysis sets to use\n    traversal_strategy: str                 # \"focused\", \"exploratory\", \"hierarchical\"\n    context_flow: str                       # \"laminar\" (coherent) or \"turbulent\" (mixed)\n    reasoning: str                          # Explanation of match\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "0201bf9d24ff638c",
      "chunk_type": "class",
      "relevance_score": 0.9863363604513286,
      "start_line": 226,
      "end_line": 236,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758131
    },
    {
      "content": "def _extract_keywords(query: str) -> List[str]:\n    \"\"\"Extract significant keywords from query.\"\"\"\n    clean = re.sub(r'[^\\w\\s]', ' ', query.lower())\n    words = clean.split()\n    stop_words = {\n        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',\n        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n        'could', 'should', 'may', 'might', 'must', 'can', 'to', 'of',\n        'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as', 'into',\n        'how', 'what', 'where', 'when', 'why', 'which', 'who',\n        'this', 'that', 'these', 'those', 'it', 'its', 'i', 'me',\n        'you', 'we', 'they', 'and', 'or', 'but', 'if', 'then',\n    }\n    return [w for w in words if w not in stop_words and len(w) > 2]\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "fccbc4114b85a232",
      "chunk_type": "function",
      "relevance_score": 0.9433143669542098,
      "start_line": 237,
      "end_line": 252,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758146
    },
    {
      "content": "def _match_purpose(keywords: List[str]) -> Tuple[Optional[str], float]:\n    \"\"\"Match keywords to \u03c0\u2082 PURPOSE field.\"\"\"\n    for kw in keywords:\n        if kw in QUERY_PURPOSE_KEYWORDS:\n            return QUERY_PURPOSE_KEYWORDS[kw], 0.9\n\n    # Partial matching\n    for kw in keywords:\n        for pattern, purpose in QUERY_PURPOSE_KEYWORDS.items():\n            if pattern in kw or kw in pattern:\n                return purpose, 0.6\n\n    return None, 0.0\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "13f768cfba619aa7",
      "chunk_type": "function",
      "relevance_score": 0.9326606256887672,
      "start_line": 253,
      "end_line": 267,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758154
    },
    {
      "content": "def _match_layer(keywords: List[str]) -> Tuple[Optional[str], float]:\n    \"\"\"Match keywords to architecture layer.\"\"\"\n    for kw in keywords:\n        if kw in QUERY_LAYER_KEYWORDS:\n            return QUERY_LAYER_KEYWORDS[kw], 0.9\n\n    # Partial matching\n    for kw in keywords:\n        for pattern, layer in QUERY_LAYER_KEYWORDS.items():\n            if pattern in kw or kw in pattern:\n                return layer, 0.6\n\n    return None, 0.0\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "cb4cc473668e29de",
      "chunk_type": "function",
      "relevance_score": 0.9321726338243094,
      "start_line": 268,
      "end_line": 282,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.7581599
    },
    {
      "content": "def _match_direction(keywords: List[str]) -> EdgeDirection:\n    \"\"\"Match keywords to graph traversal direction.\"\"\"\n    for kw in keywords:\n        if kw in QUERY_DIRECTION_KEYWORDS:\n            return QUERY_DIRECTION_KEYWORDS[kw]\n    return EdgeDirection.BOTH\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "41c97b6867b9ed6e",
      "chunk_type": "function",
      "relevance_score": 0.9206649882040626,
      "start_line": 283,
      "end_line": 290,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758165
    },
    {
      "content": "def _match_roles(keywords: List[str]) -> List[str]:\n    \"\"\"Match keywords to DDD roles.\"\"\"\n    matched = []\n    for kw in keywords:\n        kw_lower = kw.lower()\n        for roles in ROLE_CLUSTERS.values():\n            for role in roles:\n                if kw_lower in role.lower() or role.lower() in kw_lower:\n                    if role not in matched:\n                        matched.append(role)\n    return matched\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "54cc9cc8df42042b",
      "chunk_type": "function",
      "relevance_score": 0.9310588140887518,
      "start_line": 291,
      "end_line": 303,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.7581701
    },
    {
      "content": "def _purpose_to_sets(purpose: str) -> List[str]:\n    \"\"\"Map PURPOSE to analysis sets.\"\"\"\n    mapping = {\n        \"Retrieve\": [\"pipeline\", \"data_access\"],\n        \"Persist\": [\"infrastructure\", \"data_access\"],\n        \"Transform\": [\"pipeline\", \"classifiers\"],\n        \"Compute\": [\"pipeline\", \"algorithms\"],\n        \"Validate\": [\"validators\", \"constraints\"],\n        \"Coordinate\": [\"services\", \"orchestration\"],\n        \"Transport\": [\"gateways\", \"infrastructure\"],\n        \"Present\": [\"presentation\", \"viz_core\"],\n    }\n    return mapping.get(purpose, [\"pipeline\"])\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "00a24fd379b5d667",
      "chunk_type": "function",
      "relevance_score": 0.9374868157784532,
      "start_line": 304,
      "end_line": 318,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758175
    },
    {
      "content": "def _layer_to_sets(layer: str) -> List[str]:\n    \"\"\"Map architecture layer to analysis sets.\"\"\"\n    mapping = {\n        \"Interface\": [\"controllers\", \"api\"],\n        \"Application\": [\"services\", \"pipeline\"],\n        \"Core\": [\"domain\", \"entities\", \"theory\"],\n        \"Infrastructure\": [\"infrastructure\", \"data_access\"],\n        \"Test\": [\"tests\"],\n    }\n    return mapping.get(layer, [\"pipeline\"])\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "7720452400febf24",
      "chunk_type": "function",
      "relevance_score": 0.9297196275187714,
      "start_line": 319,
      "end_line": 330,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.7581801
    },
    {
      "content": "def _determine_context_flow(targets: List[SemanticTarget]) -> str:\n    \"\"\"Determine if context flow is laminar (coherent) or turbulent (mixed).\"\"\"\n    if not targets:\n        return \"turbulent\"\n\n    # Check purpose coherence\n    purposes = [t.purpose for t in targets if t.purpose]\n    if len(set(purposes)) <= 1:\n        return \"laminar\"\n\n    # Check layer coherence\n    layers = [t.layer for t in targets if t.layer]\n    if len(set(layers)) <= 2:\n        # Adjacent layers are still laminar\n        if len(layers) >= 2:\n            layer_indices = [LAYER_ORDER.index(l) for l in layers if l in LAYER_ORDER]\n            if layer_indices and max(layer_indices) - min(layer_indices) <= 1:\n                return \"laminar\"\n\n    return \"turbulent\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "f253ce4b83dfe7bf",
      "chunk_type": "function",
      "relevance_score": 0.943578646777294,
      "start_line": 331,
      "end_line": 352,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758212
    },
    {
      "content": "def _determine_traversal_strategy(targets: List[SemanticTarget]) -> str:\n    \"\"\"Determine graph traversal strategy.\"\"\"\n    if not targets:\n        return \"exploratory\"\n\n    # If single focused target, use focused strategy\n    if len(targets) == 1 and targets[0].confidence > 0.7:\n        return \"focused\"\n\n    # If multiple layers involved, use hierarchical\n    layers = [t.layer for t in targets if t.layer]\n    if len(set(layers)) > 1:\n        return \"hierarchical\"\n\n    return \"exploratory\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "8aeb47302d46558c",
      "chunk_type": "function",
      "relevance_score": 0.9346423459638615,
      "start_line": 353,
      "end_line": 369,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758262
    },
    {
      "content": "def semantic_match(query: str) -> SemanticMatch:\n    \"\"\"\n    Match query to semantic space of Standard Model of Code.\n\n    This is the core \"Curriculum Compiler\" function that:\n    1. Extracts semantic signals from the query\n    2. Maps to PURPOSE field (\u03c0\u2081-\u03c0\u2084) hierarchy\n    3. Identifies relevant architecture layers\n    4. Determines graph traversal direction\n    5. Suggests optimal context sets\n\n    Args:\n        query: The user's question or instruction\n\n    Returns:\n        SemanticMatch with targets, suggested files/sets, and reasoning\n    \"\"\"\n    keywords = _extract_keywords(query)\n\n    # Match semantic dimensions\n    purpose, purpose_conf = _match_purpose(keywords)\n    layer, layer_conf = _match_layer(keywords)\n    direction = _match_direction(keywords)\n    roles = _match_roles(keywords)\n\n    # Build semantic targets\n    targets = []\n\n    if purpose or layer or roles:\n        target = SemanticTarget(\n            purpose=purpose,\n            layer=layer,\n            roles=roles,\n            direction=direction,\n            scale_focus=\"L3\" if purpose else \"L5\",  # NODE for purpose, FILE for layer\n            confidence=max(purpose_conf, layer_conf, 0.5 if roles else 0.0)\n        )\n        targets.append(target)\n\n    # Determine suggested sets\n    suggested_sets = []\n    if purpose:\n        suggested_sets.extend(_purpose_to_sets(purpose))\n    if layer:\n        suggested_sets.extend(_layer_to_sets(layer))\n    if not suggested_sets:\n        suggested_sets = [\"pipeline\", \"theory\"]  # Default\n\n    # Deduplicate while preserving order\n    seen = set()\n    unique_sets = []\n    for s in suggested_sets:\n        if s not in seen:\n            seen.add(s)\n            unique_sets.append(s)\n\n    # Build suggested file patterns\n    suggested_files = []\n    if layer:\n        layer_patterns = {\n            \"Interface\": [\"**/controllers/**\", \"**/api/**\", \"**/routes/**\"],\n            \"Application\": [\"**/services/**\", \"**/usecases/**\"],\n            \"Core\": [\"**/domain/**\", \"**/entities/**\", \"**/models/**\"],\n            \"Infrastructure\": [\"**/infrastructure/**\", \"**/repositories/**\"],\n            \"Test\": [\"**/tests/**\", \"**/*_test.*\"],\n        }\n        suggested_files.extend(layer_patterns.get(layer, []))\n\n    # Determine flow characteristics\n    context_flow = _determine_context_flow(targets)\n    traversal_strategy = _determine_traversal_strategy(targets)\n\n    # Build reasoning\n    reasoning_parts = []\n    if purpose:\n        reasoning_parts.append(f\"\u03c0\u2082 purpose: {purpose}\")\n    if layer:\n        reasoning_parts.append(f\"layer: {layer}\")\n    if roles:\n        reasoning_parts.append(f\"roles: {', '.join(roles)}\")\n    if direction != EdgeDirection.BOTH:\n        reasoning_parts.append(f\"traversal: {direction.value}\")\n\n    reasoning = \"; \".join(reasoning_parts) if reasoning_parts else \"No strong semantic match - using exploratory strategy\"\n\n    return SemanticMatch(\n        query=query,\n        targets=targets,\n        suggested_files=suggested_files,\n        suggested_sets=unique_sets[:5],\n        traversal_strategy=traversal_strategy,\n        context_flow=context_flow,\n        reasoning=reasoning,\n    )\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "2b36607ad77c0145",
      "chunk_type": "function",
      "relevance_score": 0.974867219050879,
      "start_line": 370,
      "end_line": 465,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758275
    },
    {
      "content": "def get_upstream_context(particle_id: str, graph_data: Dict) -> List[str]:\n    \"\"\"\n    Get upstream context (providers, dependencies) for a particle.\n\n    Traverses UPSTREAM_EDGES to find what this particle depends on.\n    \"\"\"\n    if \"edges\" not in graph_data:\n        return []\n\n    upstream = []\n    for edge in graph_data[\"edges\"]:\n        if edge.get(\"source\") == particle_id and edge.get(\"type\") in UPSTREAM_EDGES:\n            upstream.append(edge.get(\"target\"))\n        elif edge.get(\"target\") == particle_id and edge.get(\"type\") in DOWNSTREAM_EDGES:\n            upstream.append(edge.get(\"source\"))\n\n    return upstream\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "a3b286f686002ba0",
      "chunk_type": "function",
      "relevance_score": 0.9397940008672038,
      "start_line": 466,
      "end_line": 484,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758281
    },
    {
      "content": "def get_downstream_context(particle_id: str, graph_data: Dict) -> List[str]:\n    \"\"\"\n    Get downstream context (consumers, callers) for a particle.\n\n    Traverses DOWNSTREAM_EDGES to find what depends on this particle.\n    \"\"\"\n    if \"edges\" not in graph_data:\n        return []\n\n    downstream = []\n    for edge in graph_data[\"edges\"]:\n        if edge.get(\"source\") == particle_id and edge.get(\"type\") in DOWNSTREAM_EDGES:\n            downstream.append(edge.get(\"target\"))\n        elif edge.get(\"target\") == particle_id and edge.get(\"type\") in UPSTREAM_EDGES:\n            downstream.append(edge.get(\"source\"))\n\n    return downstream\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "d02fa4f1c6baee34",
      "chunk_type": "function",
      "relevance_score": 0.9401044628940867,
      "start_line": 485,
      "end_line": 503,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758286
    },
    {
      "content": "def compute_semantic_distance(particle_a: Dict, particle_b: Dict) -> float:\n    \"\"\"\n    Compute semantic distance between two particles in dimension space.\n\n    Uses weighted dimension comparison from DIMENSION_WEIGHTS.\n    Distance ranges from 0.0 (identical) to 1.0 (maximally different).\n    \"\"\"\n    dims_a = particle_a.get(\"dimensions\", {})\n    dims_b = particle_b.get(\"dimensions\", {})\n\n    total_weight = 0.0\n    weighted_diff = 0.0\n\n    for dim, weight in DIMENSION_WEIGHTS.items():\n        val_a = dims_a.get(dim)\n        val_b = dims_b.get(dim)\n\n        if val_a is None or val_b is None:\n            continue\n\n        total_weight += weight\n\n        if dim == \"D2_LAYER\":\n            # Layer distance is positional\n            if val_a in LAYER_ORDER and val_b in LAYER_ORDER:\n                idx_a = LAYER_ORDER.index(val_a)\n                idx_b = LAYER_ORDER.index(val_b)\n                diff = abs(idx_a - idx_b) / (len(LAYER_ORDER) - 1)\n            else:\n                diff = 1.0 if val_a != val_b else 0.0\n        elif dim == \"D3_ROLE\":\n            # Role distance uses clusters\n            diff = 0.0 if val_a == val_b else 1.0\n            for cluster_roles in ROLE_CLUSTERS.values():\n                if val_a in cluster_roles and val_b in cluster_roles:\n                    diff = 0.3  # Same cluster = close\n                    break\n        elif dim == \"D8_TRUST\":\n            # Trust is numeric\n            diff = abs(val_a - val_b) / 100.0\n        else:\n            # Categorical dimensions\n            diff = 0.0 if val_a == val_b else 1.0\n\n        weighted_diff += weight * diff\n\n    if total_weight == 0:\n        return 1.0\n\n    return weighted_diff / total_weight\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "ca190924c22e9d5f",
      "chunk_type": "function",
      "relevance_score": 0.9614071803798871,
      "start_line": 504,
      "end_line": 555,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.758292
    },
    {
      "content": "def format_semantic_match(match: SemanticMatch) -> str:\n    \"\"\"Format semantic match for display.\"\"\"\n    lines = [\n        f\"Query: {match.query[:80]}...\",\n        f\"Strategy: {match.traversal_strategy} ({match.context_flow} flow)\",\n        f\"Reasoning: {match.reasoning}\",\n    ]\n\n    if match.targets:\n        lines.append(\"Targets:\")\n        for t in match.targets:\n            parts = []\n            if t.purpose:\n                parts.append(f\"\u03c0\u2082={t.purpose}\")\n            if t.layer:\n                parts.append(f\"layer={t.layer}\")\n            if t.roles:\n                parts.append(f\"roles=[{','.join(t.roles[:3])}]\")\n            lines.append(f\"  - {' '.join(parts)} (conf={t.confidence:.2f})\")\n\n    lines.append(f\"Sets: {', '.join(match.suggested_sets)}\")\n\n    if match.suggested_files:\n        lines.append(f\"Files: {', '.join(match.suggested_files[:3])}\")\n\n    return \"\\n\".join(lines)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/semantic_matcher.py",
      "chunk_id": "8fe9fa2c38d914b1",
      "chunk_type": "function",
      "relevance_score": 0.9476154004831063,
      "start_line": 556,
      "end_line": 582,
      "metadata": {
        "file_type": ".py",
        "file_name": "semantic_matcher.py"
      },
      "created_at": 1769237032.7582982
    },
    {
      "content": "from dataclasses import dataclass\nfrom typing import List, Dict, Optional\nfrom pathlib import Path\nimport yaml",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "bb3462d1fb1053a5",
      "chunk_type": "imports",
      "relevance_score": 0.6520696342579113,
      "start_line": 1,
      "end_line": 14,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758613
    },
    {
      "content": "def _get_config():\n    \"\"\"Get ACI config, with lazy import to avoid circular dependency.\"\"\"\n    try:\n        # Import here to avoid circular import at module load time\n        from . import ACI_CONFIG\n        return ACI_CONFIG\n    except ImportError:\n        return {}\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "beafad77b78c1769",
      "chunk_type": "function",
      "relevance_score": 0.8714067397014394,
      "start_line": 31,
      "end_line": 39,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.7586188
    },
    {
      "content": "def get_token_budgets() -> Dict[str, int]:\n    \"\"\"Get token budgets from config or defaults.\"\"\"\n    config = _get_config()\n    config_budgets = config.get(\"token_budgets\", {})\n    # Merge config over defaults\n    budgets = _DEFAULT_TOKEN_BUDGETS.copy()\n    budgets.update(config_budgets)\n    return budgets\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "91d3ac1a46d9b5e0",
      "chunk_type": "function",
      "relevance_score": 0.924286071324079,
      "start_line": 40,
      "end_line": 48,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758624
    },
    {
      "content": "def get_hard_cap() -> int:\n    \"\"\"Get hard cap from config or default.\"\"\"\n    config = _get_config()\n    return config.get(\"token_budgets\", {}).get(\"hard_cap\", _DEFAULT_HARD_CAP)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "2a632140d8ac990a",
      "chunk_type": "function",
      "relevance_score": 0.9125210001154448,
      "start_line": 49,
      "end_line": 53,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758629
    },
    {
      "content": "class OptimizedContext:\n    \"\"\"Result of context optimization.\"\"\"\n    sets: List[str]              # Final list of sets to use\n    truths: Optional[Dict]       # Repo truths if loaded\n    positioning: str             # \"sandwich\" or \"front-load\"\n    critical_files: List[str]    # Files to position strategically\n    estimated_tokens: int        # Estimated total token count\n    budget_warning: bool         # True if exceeding recommended budget\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "efc190bc83eaba1d",
      "chunk_type": "class",
      "relevance_score": 0.9825153761565968,
      "start_line": 59,
      "end_line": 68,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758634
    },
    {
      "content": "def load_repo_truths(project_root: Path) -> Optional[Dict]:\n    \"\"\"Load cached truths from BARE TruthValidator.\"\"\"\n    truths_path = project_root / \".agent/intelligence/truths/repo_truths.yaml\"\n    if not truths_path.exists():\n        return None\n\n    try:\n        with open(truths_path, 'r') as f:\n            return yaml.safe_load(f)\n    except Exception:\n        return None\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "fbf4dbf3b1b25989",
      "chunk_type": "function",
      "relevance_score": 0.9288170675102897,
      "start_line": 69,
      "end_line": 81,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758642
    },
    {
      "content": "def answer_from_truths(query: str, truths: Dict) -> Optional[str]:\n    \"\"\"Try to answer a query using cached truths.\"\"\"\n    query_lower = query.lower()\n\n    # Pattern: \"how many X files\"\n    if \"how many\" in query_lower:\n        counts = truths.get(\"counts\", {})\n        files = counts.get(\"files\", {})\n\n        # Check for specific file type\n        for ext, count in files.items():\n            if ext in query_lower:\n                return f\"There are {count} {ext} files in the repository.\"\n\n        # General file count\n        if \"files\" in query_lower and files:\n            total = sum(files.values())\n            breakdown = \", \".join(f\"{c} {e}\" for e, c in files.items())\n            return f\"Total files: {total} ({breakdown})\"\n\n        # Lines of code\n        if \"lines\" in query_lower or \"loc\" in query_lower:\n            loc = counts.get(\"lines_of_code\")\n            if loc:\n                return f\"The repository contains approximately {loc:,} lines of code.\"\n\n        # Functions\n        if \"function\" in query_lower:\n            funcs = counts.get(\"functions\")\n            if funcs:\n                return f\"There are approximately {funcs} functions in the repository.\"\n\n        # Classes\n        if \"class\" in query_lower:\n            classes = counts.get(\"classes\")\n            if classes:\n                return f\"There are approximately {classes} classes in the repository.\"\n\n    # Pattern: \"what is the pipeline\" - check for known facts\n    if \"pipeline\" in query_lower and \"stages\" in query_lower:\n        pipeline = truths.get(\"pipeline\", {})\n        stages = pipeline.get(\"stages\")\n        if stages:\n            return f\"The analysis pipeline has {stages} stages.\"\n\n    return None\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "53250928704da20d",
      "chunk_type": "function",
      "relevance_score": 0.9616116760557367,
      "start_line": 82,
      "end_line": 129,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.7586482
    },
    {
      "content": "def get_critical_files_for_sets(\n    sets: List[str],\n    sets_config: Dict",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "7a294f4ab08821f9",
      "chunk_type": "function",
      "relevance_score": 0.793753063169585,
      "start_line": 130,
      "end_line": 132,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758652
    },
    {
      "content": "def get_positional_strategy(\n    sets: List[str],\n    sets_config: Dict",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "f44ec3a945d514ac",
      "chunk_type": "function",
      "relevance_score": 0.7925629174359539,
      "start_line": 149,
      "end_line": 151,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758657
    },
    {
      "content": "def estimate_tokens_for_sets(\n    sets: List[str],\n    sets_config: Dict",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "5cb373b63c8faaf8",
      "chunk_type": "function",
      "relevance_score": 0.7928666248215634,
      "start_line": 171,
      "end_line": 173,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758662
    },
    {
      "content": "def inject_agent_context(\n    sets: List[str],\n    inject_level: str = \"minimal\"",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "49fb35ef1e6f9126",
      "chunk_type": "function",
      "relevance_score": 0.7951544993495973,
      "start_line": 199,
      "end_line": 201,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758666
    },
    {
      "content": "def optimize_context(\n    sets: List[str],\n    sets_config: Dict,\n    project_root: Path,\n    use_truths: bool = True,\n    inject_agent: bool = False,\n    inject_level: str = \"standard\"",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "295abee63b66c4c9",
      "chunk_type": "function",
      "relevance_score": 0.8133585864201508,
      "start_line": 228,
      "end_line": 234,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758671
    },
    {
      "content": "def format_context_summary(ctx: OptimizedContext) -> str:\n    \"\"\"Format context optimization summary for display.\"\"\"\n    lines = [\n        f\"Sets: {', '.join(ctx.sets)}\",\n        f\"Estimated tokens: {ctx.estimated_tokens:,}\",\n        f\"Positioning: {ctx.positioning}\",\n    ]\n\n    if ctx.critical_files:\n        lines.append(f\"Critical files: {len(ctx.critical_files)}\")\n\n    if ctx.truths:\n        lines.append(\"Truths: loaded\")\n\n    if ctx.budget_warning:\n        lines.append(f\"WARNING: Exceeds {get_hard_cap():,} token budget!\")\n\n    return \"\\n\".join(lines)\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/context_optimizer.py",
      "chunk_id": "ab4f5876a8f91708",
      "chunk_type": "function",
      "relevance_score": 0.93740940135031,
      "start_line": 276,
      "end_line": 294,
      "metadata": {
        "file_type": ".py",
        "file_name": "context_optimizer.py"
      },
      "created_at": 1769237032.758676
    },
    {
      "content": "from dataclasses import dataclass, asdict\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Optional, List, Dict\nimport yaml\nimport os\nfrom .query_analyzer import QueryProfile\nfrom .tier_router import RoutingDecision",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/feedback_loop.py",
      "chunk_id": "f3729625a6a502ce",
      "chunk_type": "imports",
      "relevance_score": 0.6698970004336019,
      "start_line": 1,
      "end_line": 22,
      "metadata": {
        "file_type": ".py",
        "file_name": "feedback_loop.py"
      },
      "created_at": 1769237032.758943
    },
    {
      "content": "class FeedbackEntry:\n    \"\"\"A single feedback entry for an ACI query.\"\"\"\n    timestamp: str\n    query: str\n    intent: str\n    complexity: str\n    scope: str\n    tier: str\n    sets_used: List[str]\n    tokens_input: int\n    tokens_output: int\n    success: bool\n    retry_count: int\n    fallback_used: bool\n    duration_ms: int\n    error: Optional[str] = None\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/feedback_loop.py",
      "chunk_id": "6ff124cacba22228",
      "chunk_type": "class",
      "relevance_score": 0.9776334108056096,
      "start_line": 26,
      "end_line": 43,
      "metadata": {
        "file_type": ".py",
        "file_name": "feedback_loop.py"
      },
      "created_at": 1769237032.758949
    },
    {
      "content": "class FeedbackLoop:\n    \"\"\"\n    Manages feedback collection and storage for ACI.\n\n    Feedback is used to:\n    1. Track query patterns over time\n    2. Identify common failure modes\n    3. Tune routing decisions\n    4. Measure token efficiency\n    \"\"\"\n\n    def __init__(self, project_root: Path):\n        self.project_root = project_root\n        self.feedback_dir = project_root / \".agent/intelligence\"\n        self.feedback_file = self.feedback_dir / \"aci_feedback.yaml\"\n        self._ensure_dir()\n\n    def _ensure_dir(self):\n        \"\"\"Ensure feedback directory exists.\"\"\"\n        self.feedback_dir.mkdir(parents=True, exist_ok=True)\n\n    def _load_feedback(self) -> Dict:\n        \"\"\"Load existing feedback data.\"\"\"\n        if not self.feedback_file.exists():\n            return {\n                \"version\": \"1.0.0\",\n                \"created\": datetime.now(timezone.utc).isoformat(),\n                \"entries\": [],\n                \"stats\": {\n                    \"total_queries\": 0,\n                    \"by_tier\": {},\n                    \"by_intent\": {},\n                    \"success_rate\": 0.0,\n                    \"avg_tokens\": 0,\n                }\n            }\n\n        try:\n            with open(self.feedback_file, 'r') as f:\n                return yaml.safe_load(f) or {}\n        except Exception:\n            return {\"version\": \"1.0.0\", \"entries\": [], \"stats\": {}}\n\n    def _save_feedback(self, data: Dict):\n        \"\"\"Save feedback data.\"\"\"\n        with open(self.feedback_file, 'w') as f:\n            yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n\n    def _update_stats(self, data: Dict, entry: FeedbackEntry):\n        \"\"\"Update aggregate statistics.\"\"\"\n        stats = data.get(\"stats\", {})\n\n        # Total queries\n        stats[\"total_queries\"] = stats.get(\"total_queries\", 0) + 1\n\n        # By tier\n        by_tier = stats.get(\"by_tier\", {})\n        by_tier[entry.tier] = by_tier.get(entry.tier, 0) + 1\n        stats[\"by_tier\"] = by_tier\n\n        # By intent\n        by_intent = stats.get(\"by_intent\", {})\n        by_intent[entry.intent] = by_intent.get(entry.intent, 0) + 1\n        stats[\"by_intent\"] = by_intent\n\n        # Success rate (rolling)\n        total = stats[\"total_queries\"]\n        old_rate = stats.get(\"success_rate\", 0.0)\n        new_rate = ((old_rate * (total - 1)) + (1.0 if entry.success else 0.0)) / total\n        stats[\"success_rate\"] = round(new_rate, 4)\n\n        # Average tokens (rolling)\n        old_avg = stats.get(\"avg_tokens\", 0)\n        new_avg = ((old_avg * (total - 1)) + entry.tokens_input) / total\n        stats[\"avg_tokens\"] = int(new_avg)\n\n        data[\"stats\"] = stats\n\n    def log_query(\n        self,\n        profile: QueryProfile,\n        decision: RoutingDecision,\n        tokens_input: int,\n        tokens_output: int,\n        success: bool,\n        duration_ms: int,\n        retry_count: int = 0,\n        fallback_used: bool = False,\n        error: Optional[str] = None\n    ):\n        \"\"\"\n        Log a query execution to the feedback file.\n\n        Args:\n            profile: Query profile from analyzer\n            decision: Routing decision from router\n            tokens_input: Input tokens consumed\n            tokens_output: Output tokens generated\n            success: Whether query succeeded\n            duration_ms: Execution time in milliseconds\n            retry_count: Number of retries needed\n            fallback_used: Whether fallback tier was used\n            error: Error message if failed\n        \"\"\"\n        entry = FeedbackEntry(\n            timestamp=datetime.now(timezone.utc).isoformat(),\n            query=profile.query[:200],  # Truncate long queries\n            intent=profile.intent.value,\n            complexity=profile.complexity.value,\n            scope=profile.scope.value,\n            tier=decision.tier.value,\n            sets_used=decision.primary_sets,\n            tokens_input=tokens_input,\n            tokens_output=tokens_output,\n            success=success,\n            retry_count=retry_count,\n            fallback_used=fallback_used,\n            duration_ms=duration_ms,\n            error=error\n        )\n\n        data = self._load_feedback()\n\n        # Keep only last 1000 entries\n        entries = data.get(\"entries\", [])\n        if len(entries) >= 1000:\n            entries = entries[-999:]\n\n        entries.append(asdict(entry))\n        data[\"entries\"] = entries\n\n        self._update_stats(data, entry)\n        self._save_feedback(data)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Get current feedback statistics.\"\"\"\n        data = self._load_feedback()\n        return data.get(\"stats\", {})\n\n    def get_recent_entries(self, count: int = 10) -> List[Dict]:\n        \"\"\"Get most recent feedback entries.\"\"\"\n        data = self._load_feedback()\n        entries = data.get(\"entries\", [])\n        return entries[-count:]\n\n    def get_tier_recommendations(self) -> Dict[str, str]:\n        \"\"\"\n        Analyze feedback to suggest routing improvements.\n\n        Returns dict of suggestions based on patterns.\n        \"\"\"\n        data = self._load_feedback()\n        entries = data.get(\"entries\", [])\n\n        if len(entries) < 10:\n            return {\"status\": \"Need more data (10+ queries)\"}\n\n        suggestions = {}\n\n        # Check for high retry rates per tier\n        tier_retries = {}\n        tier_counts = {}\n        for e in entries:\n            tier = e.get(\"tier\", \"unknown\")\n            tier_counts[tier] = tier_counts.get(tier, 0) + 1\n            tier_retries[tier] = tier_retries.get(tier, 0) + e.get(\"retry_count\", 0)\n\n        for tier, count in tier_counts.items():\n            if count > 5:\n                retry_rate = tier_retries[tier] / count\n                if retry_rate > 0.3:\n                    suggestions[tier] = f\"High retry rate ({retry_rate:.1%}), consider fallback\"\n\n        # Check for token efficiency\n        stats = data.get(\"stats\", {})\n        avg_tokens = stats.get(\"avg_tokens\", 0)\n        if avg_tokens > 150_000:\n            suggestions[\"tokens\"] = f\"High avg tokens ({avg_tokens:,}), consider smaller sets\"\n\n        return suggestions if suggestions else {\"status\": \"No issues detected\"}\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/feedback_loop.py",
      "chunk_id": "b4a18d9bf6416bf9",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 44,
      "end_line": 225,
      "metadata": {
        "file_type": ".py",
        "file_name": "feedback_loop.py"
      },
      "created_at": 1769237032.7589602
    },
    {
      "content": "def get_feedback_loop(project_root: Optional[Path] = None) -> FeedbackLoop:\n    \"\"\"Get or create feedback loop instance.\"\"\"\n    global _feedback_instance\n\n    if _feedback_instance is None:\n        root = project_root\n        if root is None:\n            # Try to detect project root\n            env_root = os.environ.get(\"PROJECT_ROOT\")\n            if env_root:\n                root = Path(env_root)\n            else:\n                root = Path(__file__).parent.parent.parent.parent.parent\n        _feedback_instance = FeedbackLoop(root)\n\n    return _feedback_instance\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/feedback_loop.py",
      "chunk_id": "f73b2fbeaa81a574",
      "chunk_type": "function",
      "relevance_score": 0.9377937427836246,
      "start_line": 230,
      "end_line": 247,
      "metadata": {
        "file_type": ".py",
        "file_name": "feedback_loop.py"
      },
      "created_at": 1769237032.758966
    },
    {
      "content": "def log_aci_query(\n    profile: QueryProfile,\n    decision: RoutingDecision,\n    tokens_input: int,\n    tokens_output: int,\n    success: bool,\n    duration_ms: int,\n    **kwargs",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/feedback_loop.py",
      "chunk_id": "200bc5c902671d9b",
      "chunk_type": "function",
      "relevance_score": 0.8123986633180904,
      "start_line": 248,
      "end_line": 255,
      "metadata": {
        "file_type": ".py",
        "file_name": "feedback_loop.py"
      },
      "created_at": 1769237032.75897
    },
    {
      "content": "    from aci.cache_registry import CacheRegistry",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/cache_registry.py",
      "chunk_id": "83dc9a178f198941",
      "chunk_type": "imports",
      "relevance_score": 0.55,
      "start_line": 1,
      "end_line": 9,
      "metadata": {
        "file_type": ".py",
        "file_name": "cache_registry.py"
      },
      "created_at": 1769237032.759221
    },
    {
      "content": "class CacheEntry:\n    \"\"\"Single cached context entry.\"\"\"\n    cache_name: str           # Gemini cache resource name (cachedContents/xxx)\n    model: str                # Model the cache is for (gemini-2.0-flash, etc.)\n    workspace_key: str        # Hash of (commit SHA + dirty state)\n    created_at: float         # Unix timestamp\n    expires_at: float         # Unix timestamp when cache expires\n    token_count: int = 0      # Number of tokens in cached context\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\"Check if cache is still valid (not expired).\"\"\"\n        return time.time() < self.expires_at\n\n    @property\n    def ttl_remaining(self) -> int:\n        \"\"\"Seconds until expiration.\"\"\"\n        return max(0, int(self.expires_at - time.time()))\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/cache_registry.py",
      "chunk_id": "ccc65d0b2b31c6a4",
      "chunk_type": "class",
      "relevance_score": 0.994126226897744,
      "start_line": 37,
      "end_line": 56,
      "metadata": {
        "file_type": ".py",
        "file_name": "cache_registry.py"
      },
      "created_at": 1769237032.759228
    },
    {
      "content": "class CacheRegistry:\n    \"\"\"Registry for tracking Gemini context caches.\"\"\"\n\n    def __init__(self, registry_path: Path = REGISTRY_FILE):\n        self.registry_path = registry_path\n        self.entries: Dict[str, CacheEntry] = {}\n        self._load()\n\n    def _load(self):\n        \"\"\"Load registry from disk.\"\"\"\n        if self.registry_path.exists():\n            try:\n                data = json.loads(self.registry_path.read_text())\n                for key, entry_data in data.get(\"entries\", {}).items():\n                    self.entries[key] = CacheEntry(**entry_data)\n            except Exception:\n                self.entries = {}\n\n    def _save(self):\n        \"\"\"Persist registry to disk.\"\"\"\n        self.registry_path.parent.mkdir(parents=True, exist_ok=True)\n        data = {\n            \"updated_at\": datetime.now().isoformat(),\n            \"entries\": {k: asdict(v) for k, v in self.entries.items()}\n        }\n        self.registry_path.write_text(json.dumps(data, indent=2))\n\n    def _make_key(self, model: str, workspace_key: str) -> str:\n        \"\"\"Generate registry key for model+workspace combination.\"\"\"\n        return f\"{model}::{workspace_key}\"\n\n    def register(\n        self,\n        cache_name: str,\n        model: str,\n        workspace_key: str,\n        ttl_seconds: int = 3600,\n        token_count: int = 0\n    ) -> CacheEntry:\n        \"\"\"\n        Register a new cache entry.\n\n        Args:\n            cache_name: Gemini cache resource name (cachedContents/xxx)\n            model: Model name (gemini-2.0-flash, etc.)\n            workspace_key: Hash of workspace state (commit + dirty)\n            ttl_seconds: Time to live in seconds (default 1 hour)\n            token_count: Number of tokens in cached context\n\n        Returns:\n            The created CacheEntry\n        \"\"\"\n        now = time.time()\n        entry = CacheEntry(\n            cache_name=cache_name,\n            model=model,\n            workspace_key=workspace_key,\n            created_at=now,\n            expires_at=now + ttl_seconds,\n            token_count=token_count\n        )\n        key = self._make_key(model, workspace_key)\n        self.entries[key] = entry\n        self._save()\n        return entry\n\n    def get_valid_cache(self, model: str, workspace_key: str) -> Optional[CacheEntry]:\n        \"\"\"\n        Get a valid (non-expired) cache for the given model and workspace.\n\n        Args:\n            model: Model name\n            workspace_key: Workspace state hash\n\n        Returns:\n            CacheEntry if valid cache exists, None otherwise\n        \"\"\"\n        key = self._make_key(model, workspace_key)\n        entry = self.entries.get(key)\n\n        if entry and entry.is_valid:\n            return entry\n\n        # Clean up expired entry\n        if entry:\n            del self.entries[key]\n            self._save()\n\n        return None\n\n    def invalidate(self, model: str, workspace_key: str) -> bool:\n        \"\"\"\n        Invalidate (remove) a cache entry.\n\n        Returns:\n            True if entry was found and removed\n        \"\"\"\n        key = self._make_key(model, workspace_key)\n        if key in self.entries:\n            del self.entries[key]\n            self._save()\n            return True\n        return False\n\n    def cleanup_expired(self) -> int:\n        \"\"\"\n        Remove all expired entries.\n\n        Returns:\n            Number of entries removed\n        \"\"\"\n        expired_keys = [k for k, v in self.entries.items() if not v.is_valid]\n        for key in expired_keys:\n            del self.entries[key]\n        if expired_keys:\n            self._save()\n        return len(expired_keys)\n\n    def list_all(self) -> Dict[str, CacheEntry]:\n        \"\"\"Get all entries (including expired).\"\"\"\n        return dict(self.entries)\n\n    def list_valid(self) -> Dict[str, CacheEntry]:\n        \"\"\"Get only valid (non-expired) entries.\"\"\"\n        return {k: v for k, v in self.entries.items() if v.is_valid}\n\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Get registry statistics.\"\"\"\n        valid = self.list_valid()\n        expired = len(self.entries) - len(valid)\n        total_tokens = sum(e.token_count for e in valid.values())\n\n        return {\n            \"total_entries\": len(self.entries),\n            \"valid_entries\": len(valid),\n            \"expired_entries\": expired,\n            \"total_cached_tokens\": total_tokens,\n            \"models\": list(set(e.model for e in valid.values()))\n        }\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/cache_registry.py",
      "chunk_id": "041e7bff15c6f26a",
      "chunk_type": "class",
      "relevance_score": 1.0,
      "start_line": 57,
      "end_line": 197,
      "metadata": {
        "file_type": ".py",
        "file_name": "cache_registry.py"
      },
      "created_at": 1769237032.759239
    },
    {
      "content": "def get_workspace_key() -> str:\n    \"\"\"\n    Generate a workspace key based on git state.\n\n    Combines:\n    - Current commit SHA (HEAD)\n    - Dirty state hash (uncommitted changes)\n\n    Returns:\n        String key like \"abc1234_dirty\" or \"abc1234_clean\"\n    \"\"\"\n    import subprocess\n\n    try:\n        # Get current commit SHA (short)\n        result = subprocess.run(\n            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n            capture_output=True, text=True, timeout=5\n        )\n        commit = result.stdout.strip() if result.returncode == 0 else \"unknown\"\n\n        # Check for uncommitted changes\n        result = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"],\n            capture_output=True, text=True, timeout=5\n        )\n        is_dirty = bool(result.stdout.strip())\n\n        return f\"{commit}_{'dirty' if is_dirty else 'clean'}\"\n\n    except Exception:\n        return f\"unknown_{int(time.time())}\"\n\n",
      "source_file": "/Users/lech/PROJECTS_all/PROJECT_elements/context-management/tools/ai/aci/cache_registry.py",
      "chunk_id": "cda808bee8490273",
      "chunk_type": "function",
      "relevance_score": 0.9483305493340968,
      "start_line": 198,
      "end_line": 231,
      "metadata": {
        "file_type": ".py",
        "file_name": "cache_registry.py"
      },
      "created_at": 1769237032.759244
    }
  ]
}