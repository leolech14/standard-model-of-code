{
  "ref_id": "REF-025",
  "title": "A Mathematical Theory of Communication",
  "authors": [
    "Shannon, C."
  ],
  "year": 1948,
  "source_type": "paper",
  "category": "I.5 Information Theory",
  "original_file": "pdf/REF-025_Shannon_1948_MathematicalTheoryCommunication.pdf",
  "enhanced_txt": "txt/REF-025.txt",
  "markdown_file": "md/REF-025.md",
  "page_count": 55,
  "image_count": 0,
  "token_count_estimate": 37943,
  "summary": "Shannon's 1948 paper 'A Mathematical Theory of Communication' establishes information theory, defining information entropy and channel capacity. The paper introduces the bit as the fundamental unit of information and proves fundamental theorems on lossless and lossy compression, channel coding, and the relationship between information rate, noise, and capacity. Shannon demonstrates that information can be quantified independently of meaning, enabling rigorous engineering of communication systems.",
  "smoc_relevance_summary": "Shannon entropy provides the foundation for measuring information content in code. SMoC uses entropy to quantify complexity, redundancy, and information density across scales. High-entropy code (many unpredictable patterns) is harder to compress and analyze; low-entropy code (repetitive patterns) suggests opportunities for abstraction. Shannon's channel capacity theorem informs SMoC's context budget limits\u2014there's a maximum rate at which information can flow from code into LLM context windows, bounded by the 'channel' capacity of the API.",
  "key_smoc_concepts": [
    {
      "smoc_concept": "graph_invariants",
      "paper_concept": "Information Entropy H = -\u03a3 p(x) log p(x)",
      "mapping": "Shannon entropy measures information content. In SMoC, applied to code graphs: high-entropy distributions of atom types, edge types, or dependency patterns indicate complex, information-rich structure. Low entropy indicates redundancy and compression opportunities.",
      "quotes_or_pages": [
        "p.10: 'H = -\u03a3 p_i log p_i'"
      ],
      "strength": "strong"
    }
  ],
  "important_figures": [],
  "important_equations": [
    {
      "equation": "H = -\u03a3 p_i log p_i",
      "page": 10,
      "description": "Shannon entropy",
      "smoc_mapping": "Measures information content in code"
    }
  ],
  "cross_references": [],
  "gaps_or_extensions": "Shannon doesn't address semantic meaning or purpose. SMoC extends to purposeful information via incoherence measures.",
  "generated_metadata": {
    "date": "2026-01-27",
    "model": "manual_analysis",
    "analyst": "llm_enrichment"
  },
  "priority_tier": 1
}
