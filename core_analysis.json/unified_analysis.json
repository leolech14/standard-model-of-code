{
  "schema_version": "1.0.0",
  "spectrometer_version": "V12.2",
  "generated_at": "2025-12-24T17:31:11.033989",
  "analysis_time_ms": 344,
  "target_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core",
  "target_name": "core",
  "target_type": "directory",
  "nodes": [
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py:_enrich_with_how",
      "name": "_enrich_with_how",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py",
      "start_line": 7,
      "end_line": 45,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "semantic_ids",
          "type": "List"
        },
        {
          "name": "purity_data",
          "type": "Dict"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enrich semantic IDs with HOW dimension (purity/behavior).\n\nArgs:\n    semantic_ids: List of SemanticID objects\n    purity_data: Result from PurityDetector.analyze()",
      "signature": "def _enrich_with_how(self, semantic_ids: List, purity_data: Dict):",
      "body_source": "def _enrich_with_how(self, semantic_ids: List, purity_data: Dict):\n    \"\"\"\n    Enrich semantic IDs with HOW dimension (purity/behavior).\n    \n    Args:\n        semantic_ids: List of SemanticID objects\n        purity_data: Result from PurityDetector.analyze()\n    \"\"\"\n    if purity_data is None or not purity_data.get(\"available\"):\n        print(\"  \u26a0\ufe0f  Purity detection unavailable, using heuristics\")\n        purity_data = {}  # Initialize as empty dict to avoid further errors\n    \n    purity_map = purity_data.get(\"purity_map\", {})\n    issues = purity_data.get(\"issues\", [])\n    \n    # Build issue map: file -> list of issues\n    issues_by_file = {}\n    for issue in issues:\n        file_path = issue.file_path\n        if file_path not in issues_by_file:\n            issues_by_file[file_path] = []\n        issues_by_file[file_path].append(issue)\n    \n    for sid in semantic_ids:\n        # Extract file from module_path\n        file_path = sid.module_path.replace(\".\", \"/\") + \".py\"\n        \n        # Set purity\n        sid.is_pure = purity_map.get(file_path, None)\n        \n        # Check for async\n        sid.is_async = sid.properties.get(\"async\", False)\n        \n        # Check for side effects\n        file_issues = issues_by_file.get(file_path, [])\n        sid.has_side_effects = len(file_issues) > 0\n        \n        # Heuristic: mutating if not pure or has I/O\n        sid.is_mutating = not sid.is_pure if sid.is_pure is not None else None",
      "complexity": 0,
      "lines_of_code": 38,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py:_enrich_with_where",
      "name": "_enrich_with_where",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py",
      "start_line": 48,
      "end_line": 77,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "semantic_ids",
          "type": "List"
        },
        {
          "name": "boundary_data",
          "type": "Dict"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enrich semantic IDs with WHERE dimension (layer/boundary).\n\nArgs:\n    semantic_ids: List of SemanticID objects\n    boundary_data: Result from BoundaryDetector.analyze()",
      "signature": "def _enrich_with_where(self, semantic_ids: List, boundary_data: Dict):",
      "body_source": "def _enrich_with_where(self, semantic_ids: List, boundary_data: Dict):\n    \"\"\"\n    Enrich semantic IDs with WHERE dimension (layer/boundary).\n    \n    Args:\n        semantic_ids: List of SemanticID objects\n        boundary_data: Result from BoundaryDetector.analyze()\n    \"\"\"\n    if boundary_data is None or not boundary_data.get(\"available\"):\n        print(\"  \u26a0\ufe0f  Boundary detection unavailable, using path inference\")\n        boundary_data = {}\n    \n    layer_map = boundary_data.get(\"layer_map\", {})\n    violations = boundary_data.get(\"violations\", [])\n    \n    # Build violation map: source module -> violation\n    violations_by_module = {}\n    for v in violations:\n        violations_by_module[v.source_module] = v\n    \n    for sid in semantic_ids:\n        # Extract file from module_path\n        file_path = sid.module_path.replace(\".\", \"/\") + \".py\"\n        \n        # Set architectural layer\n        sid.architectural_layer = layer_map.get(file_path, \"unknown\")\n        \n        # Check if crosses boundary\n        module_key = sid.module_path\n        sid.crosses_boundary = module_key in violations_by_module",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py:_enrich_with_why",
      "name": "_enrich_with_why",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/enrichment_helpers.py",
      "start_line": 81,
      "end_line": 87,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "engine"
        },
        {
          "name": "semantic_ids",
          "type": "List"
        },
        {
          "name": "intent_data",
          "type": "Dict"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enrich semantic IDs with WHY dimension (intent/patterns).\nDelegated to intent_detector module.",
      "signature": "def _enrich_with_why(engine, semantic_ids: List, intent_data: Dict):",
      "body_source": "def _enrich_with_why(engine, semantic_ids: List, intent_data: Dict):\n    \"\"\"\n    Enrich semantic IDs with WHY dimension (intent/patterns).\n    Delegated to intent_detector module.\n    \"\"\"\n    from core.intent_detector import _enrich_with_why as enrich_why\n    enrich_why(engine, semantic_ids, intent_data)",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:normalize_type",
      "name": "normalize_type",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 24,
      "end_line": 24,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "t"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def normalize_type(t): return t",
      "body_source": "        def normalize_type(t): return t",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine",
      "name": "TreeSitterUniversalEngine",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 68,
      "end_line": 68,
      "role": "DTO",
      "role_confidence": 82.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class TreeSitterUniversalEngine:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine.__init__",
      "name": "TreeSitterUniversalEngine.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 71,
      "end_line": 108,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.supported_languages = {\n            '.py': 'python',\n            '.java': 'java',\n            '.ts': 'typescript',\n            '.tsx': 'typescript',\n            '.js': 'javascript',\n            '.jsx': 'javascript',\n            '.mjs': 'javascript',\n            '.cjs': 'javascript',\n            '.go': 'go',\n            '.rs': 'rust',\n            '.kt': 'kotlin',\n            '.cs': 'c_sharp',\n            '.rb': 'ruby',\n            '.php': 'php'\n        }\n\n        # Load universal patterns\n        patterns_file = Path(__file__).parent.parent / 'patterns' / 'universal_patterns.json'\n        with open(patterns_file) as f:\n            self.patterns = json.load(f)\n\n        # Load pattern repository for dynamic pattern matching\n        try:\n            from core.registry.pattern_repository import get_pattern_repository\n            self.pattern_repo = get_pattern_repository()\n        except ImportError:\n            try:\n                from registry.pattern_repository import get_pattern_repository\n                self.pattern_repo = get_pattern_repository()\n            except ImportError:\n                self.pattern_repo = None\n\n        self._ts_symbol_extractor = (Path(__file__).parent.parent / \"tools\" / \"ts_symbol_extractor.cjs\").resolve()\n        self.python_depth_margin = 50\n        self.python_recursion_hard_cap = 10000\n        self.depth_summary: Dict[str, Any] = {}",
      "complexity": 0,
      "lines_of_code": 37,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._tokenize_identifier",
      "name": "TreeSitterUniversalEngine._tokenize_identifier",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 110,
      "end_line": 128,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Tokenize identifier into lowercase tokens (snake_case + CamelCase).",
      "signature": "def _tokenize_identifier(self, name: str) -> List[str]:",
      "body_source": "    def _tokenize_identifier(self, name: str) -> List[str]:\n        \"\"\"Tokenize identifier into lowercase tokens (snake_case + CamelCase).\"\"\"\n        if not name:\n            return []\n\n        parts = re.split(r'[_\\-\\s]+', name)\n        tokens: List[str] = []\n        for part in parts:\n            if not part:\n                continue\n            tokens.extend(\n                t.lower()\n                for t in re.findall(\n                    r'[A-Z]+(?=[A-Z][a-z]|[0-9]|$)|[A-Z]?[a-z]+|[0-9]+',\n                    part,\n                )\n                if t\n            )\n        return tokens",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine.analyze_file",
      "name": "TreeSitterUniversalEngine.analyze_file",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 130,
      "end_line": 169,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a single file for universal patterns",
      "signature": "def analyze_file(self, file_path: str) -> Dict[str, Any]:",
      "body_source": "    def analyze_file(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Analyze a single file for universal patterns\"\"\"\n\n        # Determine language\n        ext = Path(file_path).suffix\n        language = self.supported_languages.get(ext)\n\n        if not language:\n            return self._fallback_analysis(file_path)\n\n        # Read file content\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n        except Exception:\n            return {'particles': [], 'touchpoints': [], 'language': 'unknown'}\n\n        # Parse (simplified for minimal version)\n        depth_metrics: Dict[str, Any] = {}\n        if language == 'python':\n            particles, depth_metrics = self._extract_python_particles_ast(\n                content, file_path, include_depth_metrics=True\n            )\n        else:\n            particles = self._extract_particles(content, language, file_path)\n        touchpoints = self._extract_touchpoints(content, particles)\n        raw_imports = self._extract_raw_imports(content, language)\n\n        result = {\n            'file_path': file_path,\n            'language': language,\n            'particles': particles,\n            'touchpoints': touchpoints,\n            'raw_imports': raw_imports,\n            'lines_analyzed': len(content.split('\\n')),\n            'chars_analyzed': len(content)\n        }\n        if depth_metrics:\n            result['depth_metrics'] = depth_metrics\n        return result",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_particles",
      "name": "TreeSitterUniversalEngine._extract_particles",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 171,
      "end_line": 215,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract particles using universal pattern matching",
      "signature": "def _extract_particles(self, content: str, language: str, file_path: str) -> List[Dict]:",
      "body_source": "    def _extract_particles(self, content: str, language: str, file_path: str) -> List[Dict]:\n        \"\"\"Extract particles using universal pattern matching\"\"\"\n        particles = []\n        lines = content.split('\\n')\n\n        for i, line in enumerate(lines, 1):\n            # JS/TS: keep to top-level declarations (no indentation) to avoid nested helpers and class methods.\n            if language in {'javascript', 'typescript'}:\n                if line != line.lstrip():\n                    continue\n\n                if re.match(r'^\\s*(export\\s+)?(default\\s+)?(abstract\\s+)?class\\s+\\w+', line):\n                    particle = self._classify_class_pattern(line, i, file_path)\n                    if particle:\n                        particles.append(particle)\n                    continue\n\n                if re.match(r'^\\s*interface\\s+\\w+', line) or re.match(r'^\\s*type\\s+\\w+\\s*=', line):\n                    particle = self._classify_class_pattern(line, i, file_path)\n                    if particle:\n                        particles.append(particle)\n                    continue\n\n                if re.match(r'^\\s*(export\\s+)?(default\\s+)?(async\\s+)?function\\s+\\w+', line) or re.match(\n                    r'^\\s*(export\\s+)?(const|let|var)\\s+\\w+\\s*=\\s*(async\\s*)?\\(?[^=]*=>',\n                    line,\n                ):\n                    particle = self._classify_function_pattern(line, i, file_path, language)\n                    if particle:\n                        particles.append(particle)\n                    continue\n\n            # Other languages: keep permissive matching (indentation is less meaningful)\n            if re.match(r'^\\s*(class|public class|private class|interface|type)\\s+\\w+', line):\n                particle = self._classify_class_pattern(line, i, file_path)\n                if particle:\n                    particles.append(particle)\n                continue\n\n            if re.match(r'^\\s*(async\\s+def|def|public|private|protected|static|func|fn)\\s+\\w+', line):\n                particle = self._classify_function_pattern(line, i, file_path, language)\n                if particle:\n                    particles.append(particle)\n\n        return particles",
      "complexity": 0,
      "lines_of_code": 44,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._classify_class_pattern",
      "name": "TreeSitterUniversalEngine._classify_class_pattern",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 217,
      "end_line": 271,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "line",
          "type": "str"
        },
        {
          "name": "line_num",
          "type": "int"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Optional[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify class-like patterns",
      "signature": "def _classify_class_pattern(self, line: str, line_num: int, file_path: str) -> Optional[Dict]:",
      "body_source": "    def _classify_class_pattern(self, line: str, line_num: int, file_path: str) -> Optional[Dict]:\n        \"\"\"Classify class-like patterns\"\"\"\n        line_stripped = line.strip()\n\n        # Extract name\n        name_match = re.search(r'(class|interface|type)\\s+(\\w+)', line_stripped)\n        if not name_match:\n            return None\n\n        class_name = name_match.group(2)\n\n        # Determine particle type by location (strong signal in real-world repos)\n        normalized_path = file_path.replace('\\\\', '/').lower()\n        particle_type = None\n\n        if '/domain/' in normalized_path and '/entities/' in normalized_path:\n            particle_type = 'Entity'\n        elif '/domain/' in normalized_path and '/value_objects/' in normalized_path:\n            particle_type = 'ValueObject'\n        elif '/usecase/' in normalized_path or '/use_case/' in normalized_path:\n            particle_type = 'UseCase'\n        elif '/domain/' in normalized_path and '/repositories/' in normalized_path:\n            particle_type = 'Repository'\n        elif '/infrastructure/' in normalized_path and 'repository' in class_name.lower():\n            particle_type = 'RepositoryImpl'\n        elif 'BaseModel' in line_stripped or '/schemas/' in normalized_path or '/error_messages/' in normalized_path:\n            particle_type = 'DTO'\n        elif '/presentation/' in normalized_path and ('/handlers/' in normalized_path or '/api/' in normalized_path):\n            particle_type = 'Controller'\n        elif '/tests/' in normalized_path or '/test/' in normalized_path:\n            particle_type = 'Test'\n        elif '/config/' in normalized_path or 'settings' in normalized_path:\n            particle_type = 'Configuration'\n        elif 'exception' in normalized_path or 'error' in normalized_path:\n            particle_type = 'Exception'\n\n        # Determine particle type by naming conventions\n        if not particle_type:\n            particle_type = self._get_particle_type_by_name(class_name)\n        if not particle_type:\n            # Try to detect by content patterns\n            particle_type = self._detect_by_keywords(line_stripped)\n\n        resolved_type = particle_type or 'Unknown'\n        confidence = self._calculate_confidence(class_name, line_stripped) if particle_type else 30.0\n\n        return {\n            'type': resolved_type,\n            'name': class_name,\n            'symbol_kind': 'class',\n            'file_path': file_path,\n            'line': line_num,\n            'confidence': confidence,\n            'evidence': line_stripped[:100]\n        }",
      "complexity": 0,
      "lines_of_code": 54,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._classify_function_pattern",
      "name": "TreeSitterUniversalEngine._classify_function_pattern",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 273,
      "end_line": 296,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "line",
          "type": "str"
        },
        {
          "name": "line_num",
          "type": "int"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "Optional[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify function-like patterns",
      "signature": "def _classify_function_pattern(self, line: str, line_num: int, file_path: str, language: str) -> Optional[Dict]:",
      "body_source": "    def _classify_function_pattern(self, line: str, line_num: int, file_path: str, language: str) -> Optional[Dict]:\n        \"\"\"Classify function-like patterns\"\"\"\n        line_stripped = line.strip()\n\n        # Extract name\n        func_name = self._extract_function_name(line_stripped, language)\n        if not func_name:\n            return None\n\n        # Determine particle type\n        particle_type = self._get_function_type_by_name(func_name)\n\n        resolved_type = particle_type or 'Unknown'\n        confidence = self._calculate_confidence(func_name, line_stripped) if particle_type else 30.0\n\n        return {\n            'type': resolved_type,\n            'name': func_name,\n            'symbol_kind': 'function',\n            'file_path': file_path,\n            'line': line_num,\n            'confidence': confidence,\n            'evidence': line_stripped[:100]\n        }",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._classify_extracted_symbol",
      "name": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 298,
      "end_line": 535,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _classify_extracted_symbol(",
      "body_source": "    def _classify_extracted_symbol(\n        self,\n        *,\n        name: str,\n        symbol_kind: str,\n        file_path: str,\n        line_num: int,\n        evidence: str = \"\",\n        parent: str = \"\",\n        base_classes: List[str] = None,\n        decorators: List[str] = None,\n        # NEW: Lossless capture fields\n        end_line: int = 0,\n        body_source: str = \"\",\n        params: List[Dict[str, str]] = None,  # [{\"name\": \"x\", \"type\": \"int\", \"default\": \"0\"}]\n        return_type: str = \"\",\n        docstring: str = \"\",\n    ) -> Dict[str, Any]:\n        evidence_line = (evidence or \"\").strip()\n        base_classes = base_classes or []\n        decorators = decorators or []\n\n        normalized_path = file_path.replace(\"\\\\\", \"/\").lower()\n        particle_type: Optional[str] = None\n        confidence = 30.0  # Default low confidence\n        \n        # =============================================================================\n        # TIER 0: FRAMEWORK-SPECIFIC OVERRIDES (99% confidence)\n        # =============================================================================\n        # Pytest / Conftest\n        if \"conftest.py\" in normalized_path:\n            if any(d for d in decorators if \"fixture\" in d) or name == \"conftest\":\n                particle_type = \"Configuration\"\n                confidence = 99.0\n            else:\n                particle_type = \"Test\" # Default for things in conftest\n                confidence = 80.0\n                \n        if particle_type is None:\n            for d in decorators:\n                if \"fixture\" in d: # pytest.fixture\n                    particle_type = \"Configuration\"\n                    confidence = 90.0\n                    break\n                if \"validator\" in d.lower(): # pydantic validators, marshmallow\n                    particle_type = \"Validator\"\n                    confidence = 90.0\n                    break\n                if \"command\" in d.lower(): # click/typer commands\n                    particle_type = \"Command\"\n                    confidence = 90.0\n                    break\n                if d.endswith(\".task\") or d == \"task\": # celery.task\n                    particle_type = \"Job\"\n                    confidence = 90.0\n                    break\n                if \"router\" in d: # fastapi router\n                    particle_type = \"Controller\"\n                    confidence = 90.0\n                    break\n\n        # =============================================================================\n        # TIER 0.5: STRUCTURAL ANCHORS (95% confidence) - \"Pseudo-Decorators\"\n        # These are definitive signals in Go/JS/Java that act like decorators\n        # =============================================================================\n        if particle_type is None and self.pattern_repo is not None:\n            # Extract parameter types from params\n            if params:\n                param_types = [p.get(\"type\", \"\") for p in params if p.get(\"type\")]\n                if param_types:\n                    result = self.pattern_repo.classify_by_param_type(param_types)\n                    if result and result[0] != \"Unknown\" and result[1] > 0:\n                        particle_type = result[0]\n                        confidence = float(result[1])\n            \n            # Check file path patterns\n            if particle_type is None:\n                result = self.pattern_repo.classify_by_path(file_path)\n                if result and result[0] != \"Unknown\" and result[1] > 80:\n                    particle_type = result[0]\n                    confidence = float(result[1])\n\n        # =============================================================================\n        # TIER 1: INHERITANCE-BASED DETECTION (99% confidence)\n        # =============================================================================\n        if symbol_kind in {\"class\", \"interface\", \"type\", \"enum\"} and base_classes:\n            for base in base_classes:\n                if base in DDD_BASE_CLASS_MAPPINGS:\n                    particle_type = DDD_BASE_CLASS_MAPPINGS[base]\n                    confidence = 99.0  # Inheritance = highest confidence\n                    break\n\n        # =============================================================================\n        # TIER 2: PATH-BASED DETECTION (90% confidence)\n        # =============================================================================\n        if particle_type is None and symbol_kind in {\"class\", \"interface\", \"type\", \"enum\"}:\n            # Strong location signals (DDD/Clean folders, or UI layers).\n            if \"/domain/\" in normalized_path and \"/entities/\" in normalized_path:\n                particle_type = \"Entity\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/aggregates/\" in normalized_path or \"/aggregate/\" in normalized_path):\n                particle_type = \"AggregateRoot\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/events/\" in normalized_path or \"/event/\" in normalized_path):\n                particle_type = \"DomainEvent\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/value_objects/\" in normalized_path or \"/valueobjects/\" in normalized_path):\n                particle_type = \"ValueObject\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/services/\" in normalized_path or \"/domain_services/\" in normalized_path):\n                particle_type = \"DomainService\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and \"/repositories/\" in normalized_path:\n                particle_type = \"Repository\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/commands/\" in normalized_path or \"/command/\" in normalized_path):\n                particle_type = \"Command\"\n                confidence = 90.0\n            elif \"/domain/\" in normalized_path and (\"/queries/\" in normalized_path or \"/query/\" in normalized_path):\n                particle_type = \"Query\"\n                confidence = 90.0\n            elif \"/infrastructure/\" in normalized_path and \"repository\" in name.lower():\n                particle_type = \"RepositoryImpl\"\n                confidence = 90.0\n            elif (\n                \"/presentation/\" in normalized_path\n                or \"/controllers/\" in normalized_path\n                or \"/api/\" in normalized_path\n                or \"/ro-finance/src/components/\" in normalized_path\n                or \"/ro-finance/src/pages/\" in normalized_path\n            ):\n                particle_type = \"Controller\"\n                confidence = 85.0\n            elif \"BaseModel\" in evidence_line or \"/schemas/\" in normalized_path or \"/error_messages/\" in normalized_path:\n                particle_type = \"DTO\"\n                confidence = 85.0\n            elif \"/tests/\" in normalized_path or \"/test/\" in normalized_path:\n                particle_type = \"Test\"\n                confidence = 80.0\n            elif \"/config/\" in normalized_path or \"settings\" in normalized_path:\n                particle_type = \"Configuration\"\n                confidence = 80.0\n            elif \"exception\" in normalized_path or \"error\" in normalized_path:\n                particle_type = \"Exception\"\n                confidence = 80.0\n            elif \"/utils/\" in normalized_path or \"/helpers/\" in normalized_path or \"/common/\" in normalized_path:\n                particle_type = \"Utility\"\n                confidence = 75.0\n            elif \"/models/\" in normalized_path and \"/domain/\" not in normalized_path:\n                particle_type = \"DTO\"\n                confidence = 75.0\n            elif \"/adapters/\" in normalized_path:\n                particle_type = \"Adapter\"\n                confidence = 75.0\n            elif \"/clients/\" in normalized_path or \"/external/\" in normalized_path:\n                particle_type = \"Client\"\n                confidence = 75.0\n            elif \"/gateways/\" in normalized_path:\n                particle_type = \"Gateway\"\n                confidence = 75.0\n\n        # =============================================================================\n        # TIER 2.5: LEARNED PATTERNS FROM patterns.json (OVERRIDE MODE)\n        # Now checks ALL symbols and can OVERRIDE lower-confidence classifications\n        # =============================================================================\n        if self.pattern_repo is not None:\n            short_name = name.split(\".\")[-1] if \".\" in name else name\n            \n            # Try prefix patterns\n            prefix_result = self.pattern_repo.classify_by_prefix(short_name)\n            if prefix_result and prefix_result[0] != \"Unknown\":\n                pattern_conf = float(prefix_result[1])\n                # Override if: no type yet OR pattern has higher confidence\n                if particle_type is None or pattern_conf > confidence:\n                    particle_type = prefix_result[0]\n                    confidence = pattern_conf\n            \n            # Try suffix patterns (may override further)\n            suffix_result = self.pattern_repo.classify_by_suffix(short_name)\n            if suffix_result and suffix_result[0] != \"Unknown\":\n                pattern_conf = float(suffix_result[1])\n                # Override if suffix has higher confidence than current\n                if particle_type is None or pattern_conf > confidence:\n                    particle_type = suffix_result[0]\n                    confidence = pattern_conf\n\n        # =============================================================================\n        # TIER 3: NAMING CONVENTIONS (70-80% confidence)\n        # =============================================================================\n        if particle_type is None and symbol_kind in {\"class\", \"interface\", \"type\", \"enum\"}:\n            particle_type = self._get_particle_type_by_name(name)\n            if particle_type:\n                confidence = 75.0\n\n        if symbol_kind in {\"function\", \"method\"}:\n            # If we get \"Class.method\", classify primarily by the last segment.\n            short_name = name.split(\".\")[-1] if \".\" in name else name\n            \n            if particle_type is None:\n                particle_type = self._get_function_type_by_name(short_name)\n                if particle_type:\n                    confidence = 70.0\n\n            # UI components: exported PascalCase functions/components\n            if particle_type is None and (\n                \"/ro-finance/src/components/\" in normalized_path or \"/ro-finance/src/pages/\" in normalized_path\n            ):\n                if short_name[:1].isupper():\n                    particle_type = \"Controller\"\n                    confidence = 70.0\n\n        resolved_type = normalize_type(particle_type or \"Unknown\")\n\n        particle: Dict[str, Any] = {\n            \"type\": resolved_type,\n            \"name\": name,\n            \"symbol_kind\": symbol_kind if symbol_kind else \"unknown\",\n            \"file_path\": file_path,\n            \"line\": line_num,\n            \"end_line\": end_line if end_line else line_num,\n            \"confidence\": confidence,\n            \"evidence\": evidence_line[:200],\n            # Lossless fields for code regeneration\n            \"body_source\": body_source,\n            \"docstring\": docstring,\n            \"return_type\": return_type,\n        }\n\n        if parent:\n            particle[\"parent\"] = parent\n        if base_classes:\n            particle[\"base_classes\"] = base_classes\n        if decorators:\n            particle[\"decorators\"] = decorators\n        if params:\n            particle[\"params\"] = params\n\n        return particle",
      "complexity": 0,
      "lines_of_code": 237,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_base_class_names",
      "name": "TreeSitterUniversalEngine._get_base_class_names",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 537,
      "end_line": 551,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.ClassDef"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract base class names from a Python class definition.",
      "signature": "def _get_base_class_names(self, node: ast.ClassDef) -> List[str]:",
      "body_source": "    def _get_base_class_names(self, node: ast.ClassDef) -> List[str]:\n        \"\"\"Extract base class names from a Python class definition.\"\"\"\n        bases = []\n        for base in node.bases:\n            if isinstance(base, ast.Name):\n                bases.append(base.id)\n            elif isinstance(base, ast.Attribute):\n                bases.append(base.attr)\n            elif isinstance(base, ast.Subscript):\n                # Generic types like Generic[T]\n                if isinstance(base.value, ast.Name):\n                    bases.append(base.value.id)\n                elif isinstance(base.value, ast.Attribute):\n                    bases.append(base.value.attr)\n        return bases",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_decorators",
      "name": "TreeSitterUniversalEngine._get_decorators",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 553,
      "end_line": 585,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Any"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract decorator names from a node.",
      "signature": "def _get_decorators(self, node: Any) -> List[str]:",
      "body_source": "    def _get_decorators(self, node: Any) -> List[str]:\n        \"\"\"Extract decorator names from a node.\"\"\"\n        decorators = []\n        for decorator in getattr(node, \"decorator_list\", []):\n            try:\n                if isinstance(decorator, ast.Name):\n                    decorators.append(decorator.id)\n                elif isinstance(decorator, ast.Attribute):\n                    # Handle @pytest.fixture where fixture is attribute\n                    parts = []\n                    while isinstance(decorator, ast.Attribute):\n                        parts.append(decorator.attr)\n                        decorator = decorator.value\n                    if isinstance(decorator, ast.Name):\n                        parts.append(decorator.id)\n                    decorators.append(\".\".join(reversed(parts)))\n                elif isinstance(decorator, ast.Call):\n                    # Handle @decorator(args)\n                    if isinstance(decorator.func, ast.Name):\n                        decorators.append(decorator.func.id)\n                    elif isinstance(decorator.func, ast.Attribute):\n                        # Handle @pytest.fixture(scope=\"module\")\n                        parts = []\n                        curr = decorator.func\n                        while isinstance(curr, ast.Attribute):\n                            parts.append(curr.attr)\n                            curr = curr.value\n                        if isinstance(curr, ast.Name):\n                            parts.append(curr.id)\n                        decorators.append(\".\".join(reversed(parts)))\n            except Exception:\n                pass\n        return decorators",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_function_body",
      "name": "TreeSitterUniversalEngine._get_function_body",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 587,
      "end_line": 596,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Any"
        },
        {
          "name": "lines",
          "type": "List[str]"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract function body source code for lossless regeneration.",
      "signature": "def _get_function_body(self, node: Any, lines: List[str]) -> str:",
      "body_source": "    def _get_function_body(self, node: Any, lines: List[str]) -> str:\n        \"\"\"Extract function body source code for lossless regeneration.\"\"\"\n        try:\n            start = getattr(node, \"lineno\", 0) - 1\n            end = getattr(node, \"end_lineno\", start + 1)\n            if start >= 0 and end <= len(lines):\n                return \"\\n\".join(lines[start:end])\n        except Exception:\n            pass\n        return \"\"",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_function_params",
      "name": "TreeSitterUniversalEngine._get_function_params",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 598,
      "end_line": 640,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Any"
        }
      ],
      "return_type": "List[Dict[str, str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract function parameters with types and defaults.",
      "signature": "def _get_function_params(self, node: Any) -> List[Dict[str, str]]:",
      "body_source": "    def _get_function_params(self, node: Any) -> List[Dict[str, str]]:\n        \"\"\"Extract function parameters with types and defaults.\"\"\"\n        params = []\n        try:\n            args = getattr(node, \"args\", None)\n            if not args:\n                return params\n            \n            # Get defaults (aligned from the end)\n            defaults = args.defaults or []\n            num_defaults = len(defaults)\n            num_args = len(args.args)\n            \n            for i, arg in enumerate(args.args):\n                param = {\"name\": arg.arg}\n                \n                # Type annotation\n                if arg.annotation:\n                    param[\"type\"] = ast.unparse(arg.annotation)\n                \n                # Default value\n                default_idx = i - (num_args - num_defaults)\n                if default_idx >= 0 and default_idx < len(defaults):\n                    param[\"default\"] = ast.unparse(defaults[default_idx])\n                \n                params.append(param)\n            \n            # *args\n            if args.vararg:\n                param = {\"name\": f\"*{args.vararg.arg}\"}\n                if args.vararg.annotation:\n                    param[\"type\"] = ast.unparse(args.vararg.annotation)\n                params.append(param)\n            \n            # **kwargs\n            if args.kwarg:\n                param = {\"name\": f\"**{args.kwarg.arg}\"}\n                if args.kwarg.annotation:\n                    param[\"type\"] = ast.unparse(args.kwarg.annotation)\n                params.append(param)\n        except Exception:\n            pass\n        return params",
      "complexity": 0,
      "lines_of_code": 42,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_docstring",
      "name": "TreeSitterUniversalEngine._get_docstring",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 642,
      "end_line": 647,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Any"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract docstring from a function or class.",
      "signature": "def _get_docstring(self, node: Any) -> str:",
      "body_source": "    def _get_docstring(self, node: Any) -> str:\n        \"\"\"Extract docstring from a function or class.\"\"\"\n        try:\n            return ast.get_docstring(node) or \"\"\n        except Exception:\n            return \"\"",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_return_type",
      "name": "TreeSitterUniversalEngine._get_return_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 649,
      "end_line": 656,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Any"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract return type annotation.",
      "signature": "def _get_return_type(self, node: Any) -> str:",
      "body_source": "    def _get_return_type(self, node: Any) -> str:\n        \"\"\"Extract return type annotation.\"\"\"\n        try:\n            if hasattr(node, \"returns\") and node.returns:\n                return ast.unparse(node.returns)\n        except Exception:\n            pass\n        return \"\"",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._measure_python_ast_depth",
      "name": "TreeSitterUniversalEngine._measure_python_ast_depth",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 659,
      "end_line": 671,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        }
      ],
      "return_type": "Dict[str, int]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Measure AST depth without recursion to avoid stack limits.",
      "signature": "def _measure_python_ast_depth(self, tree: ast.AST) -> Dict[str, int]:",
      "body_source": "    def _measure_python_ast_depth(self, tree: ast.AST) -> Dict[str, int]:\n        \"\"\"Measure AST depth without recursion to avoid stack limits.\"\"\"\n        max_depth = 0\n        node_count = 0\n        stack = [(tree, 1)]\n        while stack:\n            node, depth = stack.pop()\n            node_count += 1\n            if depth > max_depth:\n                max_depth = depth\n            for child in ast.iter_child_nodes(node):\n                stack.append((child, depth + 1))\n        return {\"node_count\": node_count, \"max_depth\": max_depth}",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "name": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 673,
      "end_line": 764,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "lines",
          "type": "List[str]"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Recursive AST traversal for Python symbols (fast path).",
      "signature": "def _extract_python_particles_ast_recursive(",
      "body_source": "    def _extract_python_particles_ast_recursive(\n        self, tree: ast.AST, file_path: str, lines: List[str]\n    ) -> List[Dict]:\n        \"\"\"Recursive AST traversal for Python symbols (fast path).\"\"\"\n        particles: List[Dict[str, Any]] = []\n        class_stack: List[str] = []\n        func_stack: List[str] = []\n\n        def evidence_for_line(line_no: int) -> str:\n            idx = max(0, int(line_no or 1) - 1)\n            if idx >= len(lines):\n                return \"\"\n            return (lines[idx] or \"\").strip()\n\n        class Visitor(ast.NodeVisitor):\n            def visit_ClassDef(self, node: ast.ClassDef):\n                class_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                parent = class_stack[-1] if class_stack else (func_stack[-1] if func_stack else \"\")\n\n                base_classes = self_outer._get_base_class_names(node)\n                decorators = self_outer._get_decorators(node)\n\n                particles.append(\n                    self_outer._classify_extracted_symbol(\n                        name=class_name,\n                        symbol_kind=\"class\",\n                        file_path=file_path,\n                        line_num=line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        base_classes=base_classes,\n                        decorators=decorators,\n                    )\n                )\n\n                class_stack.append(class_name)\n                self.generic_visit(node)\n                class_stack.pop()\n\n            def visit_FunctionDef(self, node: ast.FunctionDef):\n                func_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                end_line_no = getattr(node, \"end_lineno\", line_no) or line_no\n                if class_stack:\n                    full_name = f\"{class_stack[-1]}.{func_name}\"\n                    parent = class_stack[-1]\n                    kind = \"method\"\n                elif func_stack:\n                    full_name = f\"{func_stack[-1]}.{func_name}\"\n                    parent = func_stack[-1]\n                    kind = \"function\"\n                else:\n                    full_name = func_name\n                    parent = \"\"\n                    kind = \"function\"\n\n                decorators = self_outer._get_decorators(node)\n                \n                # NEW: Extract lossless fields\n                body_source = self_outer._get_function_body(node, lines)\n                params = self_outer._get_function_params(node)\n                docstring = self_outer._get_docstring(node)\n                return_type = self_outer._get_return_type(node)\n\n                particles.append(\n                    self_outer._classify_extracted_symbol(\n                        name=full_name,\n                        symbol_kind=kind,\n                        file_path=file_path,\n                        line_num=line_no,\n                        end_line=end_line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        decorators=decorators,\n                        body_source=body_source,\n                        params=params,\n                        docstring=docstring,\n                        return_type=return_type,\n                    )\n                )\n\n                func_stack.append(func_name)\n                self.generic_visit(node)\n                func_stack.pop()\n\n            def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n                self.visit_FunctionDef(node)  # type: ignore[arg-type]\n\n        self_outer = self\n        Visitor().visit(tree)\n        return particles",
      "complexity": 0,
      "lines_of_code": 91,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine.evidence_for_line",
      "name": "TreeSitterUniversalEngine.evidence_for_line",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 681,
      "end_line": 685,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "line_no",
          "type": "int"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def evidence_for_line(line_no: int) -> str:",
      "body_source": "        def evidence_for_line(line_no: int) -> str:\n            idx = max(0, int(line_no or 1) - 1)\n            if idx >= len(lines):\n                return \"\"\n            return (lines[idx] or \"\").strip()",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:Visitor",
      "name": "Visitor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 687,
      "end_line": 687,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "NodeVisitor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Visitor(ast.NodeVisitor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:Visitor.visit_ClassDef",
      "name": "Visitor.visit_ClassDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 688,
      "end_line": 711,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.ClassDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_ClassDef(self, node: ast.ClassDef):",
      "body_source": "            def visit_ClassDef(self, node: ast.ClassDef):\n                class_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                parent = class_stack[-1] if class_stack else (func_stack[-1] if func_stack else \"\")\n\n                base_classes = self_outer._get_base_class_names(node)\n                decorators = self_outer._get_decorators(node)\n\n                particles.append(\n                    self_outer._classify_extracted_symbol(\n                        name=class_name,\n                        symbol_kind=\"class\",\n                        file_path=file_path,\n                        line_num=line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        base_classes=base_classes,\n                        decorators=decorators,\n                    )\n                )\n\n                class_stack.append(class_name)\n                self.generic_visit(node)\n                class_stack.pop()",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:Visitor.visit_FunctionDef",
      "name": "Visitor.visit_FunctionDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 713,
      "end_line": 757,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.FunctionDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_FunctionDef(self, node: ast.FunctionDef):",
      "body_source": "            def visit_FunctionDef(self, node: ast.FunctionDef):\n                func_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                end_line_no = getattr(node, \"end_lineno\", line_no) or line_no\n                if class_stack:\n                    full_name = f\"{class_stack[-1]}.{func_name}\"\n                    parent = class_stack[-1]\n                    kind = \"method\"\n                elif func_stack:\n                    full_name = f\"{func_stack[-1]}.{func_name}\"\n                    parent = func_stack[-1]\n                    kind = \"function\"\n                else:\n                    full_name = func_name\n                    parent = \"\"\n                    kind = \"function\"\n\n                decorators = self_outer._get_decorators(node)\n                \n                # NEW: Extract lossless fields\n                body_source = self_outer._get_function_body(node, lines)\n                params = self_outer._get_function_params(node)\n                docstring = self_outer._get_docstring(node)\n                return_type = self_outer._get_return_type(node)\n\n                particles.append(\n                    self_outer._classify_extracted_symbol(\n                        name=full_name,\n                        symbol_kind=kind,\n                        file_path=file_path,\n                        line_num=line_no,\n                        end_line=end_line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        decorators=decorators,\n                        body_source=body_source,\n                        params=params,\n                        docstring=docstring,\n                        return_type=return_type,\n                    )\n                )\n\n                func_stack.append(func_name)\n                self.generic_visit(node)\n                func_stack.pop()",
      "complexity": 0,
      "lines_of_code": 44,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:Visitor.visit_AsyncFunctionDef",
      "name": "Visitor.visit_AsyncFunctionDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 759,
      "end_line": 760,
      "role": "ApplicationService",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.AsyncFunctionDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):",
      "body_source": "            def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n                self.visit_FunctionDef(node)  # type: ignore[arg-type]",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "name": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 766,
      "end_line": 874,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "lines",
          "type": "List[str]"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Iterative AST traversal for deep trees (avoids recursion limits).",
      "signature": "def _extract_python_particles_ast_iterative(",
      "body_source": "    def _extract_python_particles_ast_iterative(\n        self, tree: ast.AST, file_path: str, lines: List[str]\n    ) -> List[Dict]:\n        \"\"\"Iterative AST traversal for deep trees (avoids recursion limits).\"\"\"\n        particles: List[Dict[str, Any]] = []\n        class_stack: List[str] = []\n        func_stack: List[str] = []\n\n        def evidence_for_line(line_no: int) -> str:\n            idx = max(0, int(line_no or 1) - 1)\n            if idx >= len(lines):\n                return \"\"\n            return (lines[idx] or \"\").strip()\n\n        stack: List[Tuple[ast.AST, str]] = [(tree, \"enter\")]\n        while stack:\n            node, state = stack.pop()\n\n            if state == \"exit\":\n                if isinstance(node, ast.ClassDef):\n                    if class_stack:\n                        class_stack.pop()\n                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                    if func_stack:\n                        func_stack.pop()\n                continue\n\n            if isinstance(node, ast.ClassDef):\n                class_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                parent = class_stack[-1] if class_stack else (func_stack[-1] if func_stack else \"\")\n\n                base_classes = self._get_base_class_names(node)\n                decorators = self._get_decorators(node)\n\n                particles.append(\n                    self._classify_extracted_symbol(\n                        name=class_name,\n                        symbol_kind=\"class\",\n                        file_path=file_path,\n                        line_num=line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        base_classes=base_classes,\n                        decorators=decorators,\n                    )\n                )\n\n                class_stack.append(class_name)\n                stack.append((node, \"exit\"))\n                children = list(ast.iter_child_nodes(node))\n                for child in reversed(children):\n                    stack.append((child, \"enter\"))\n                continue\n\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                func_name = getattr(node, \"name\", \"\") or \"\"\n                line_no = getattr(node, \"lineno\", 0) or 0\n                end_line_no = getattr(node, \"end_lineno\", line_no) or line_no\n                if class_stack:\n                    full_name = f\"{class_stack[-1]}.{func_name}\"\n                    parent = class_stack[-1]\n                    kind = \"method\"\n                elif func_stack:\n                    full_name = f\"{func_stack[-1]}.{func_name}\"\n                    parent = func_stack[-1]\n                    kind = \"function\"\n                else:\n                    full_name = func_name\n                    parent = \"\"\n                    kind = \"function\"\n\n                decorators = self._get_decorators(node)\n                \n                # Lossless fields\n                body_source = self._get_function_body(node, lines)\n                params = self._get_function_params(node)\n                docstring = self._get_docstring(node)\n                return_type = self._get_return_type(node)\n\n                particles.append(\n                    self._classify_extracted_symbol(\n                        name=full_name,\n                        symbol_kind=kind,\n                        file_path=file_path,\n                        line_num=line_no,\n                        end_line=end_line_no,\n                        evidence=evidence_for_line(line_no),\n                        parent=parent,\n                        decorators=decorators,\n                        body_source=body_source,\n                        params=params,\n                        docstring=docstring,\n                        return_type=return_type,\n                    )\n                )\n\n                func_stack.append(func_name)\n                stack.append((node, \"exit\"))\n                children = list(ast.iter_child_nodes(node))\n                for child in reversed(children):\n                    stack.append((child, \"enter\"))\n                continue\n\n            children = list(ast.iter_child_nodes(node))\n            for child in reversed(children):\n                stack.append((child, \"enter\"))\n\n        return particles",
      "complexity": 0,
      "lines_of_code": 108,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine.evidence_for_line",
      "name": "TreeSitterUniversalEngine.evidence_for_line",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 774,
      "end_line": 778,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "line_no",
          "type": "int"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def evidence_for_line(line_no: int) -> str:",
      "body_source": "        def evidence_for_line(line_no: int) -> str:\n            idx = max(0, int(line_no or 1) - 1)\n            if idx >= len(lines):\n                return \"\"\n            return (lines[idx] or \"\").strip()",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_python_particles_ast",
      "name": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 876,
      "end_line": 938,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract Python particles using AST with depth-aware fallbacks.",
      "signature": "def _extract_python_particles_ast(",
      "body_source": "    def _extract_python_particles_ast(\n        self, content: str, file_path: str, *, include_depth_metrics: bool = False\n    ):\n        \"\"\"Extract Python particles using AST with depth-aware fallbacks.\"\"\"\n        try:\n            tree = ast.parse(content)\n        except Exception as exc:\n            particles = self._extract_particles(content, \"python\", file_path)\n            if include_depth_metrics:\n                return particles, {\n                    \"file_path\": file_path,\n                    \"language\": \"python\",\n                    \"strategy\": \"regex_fallback\",\n                    \"parse_error\": str(exc),\n                }\n            return particles\n\n        lines = content.splitlines()\n        depth_metrics = self._measure_python_ast_depth(tree)\n        depth_metrics.update(\n            {\n                \"file_path\": file_path,\n                \"language\": \"python\",\n                \"recursion_limit\": sys.getrecursionlimit(),\n            }\n        )\n\n        required_limit = depth_metrics[\"max_depth\"] + self.python_depth_margin\n        depth_metrics[\"required_recursion_limit\"] = required_limit\n\n        original_limit = sys.getrecursionlimit()\n        recursion_adjusted = None\n        use_iterative = False\n\n        if required_limit > original_limit:\n            if required_limit <= self.python_recursion_hard_cap:\n                sys.setrecursionlimit(required_limit)\n                recursion_adjusted = required_limit\n            else:\n                use_iterative = True\n                depth_metrics[\"fallback_reason\"] = \"max_depth_exceeds_cap\"\n\n        if recursion_adjusted:\n            depth_metrics[\"recursion_adjusted_to\"] = recursion_adjusted\n\n        try:\n            if use_iterative:\n                depth_metrics[\"strategy\"] = \"iterative\"\n                particles = self._extract_python_particles_ast_iterative(tree, file_path, lines)\n            else:\n                depth_metrics[\"strategy\"] = \"recursive\"\n                particles = self._extract_python_particles_ast_recursive(tree, file_path, lines)\n        except RecursionError:\n            depth_metrics[\"strategy\"] = \"iterative\"\n            depth_metrics[\"fallback_reason\"] = \"recursion_error\"\n            particles = self._extract_python_particles_ast_iterative(tree, file_path, lines)\n        finally:\n            if recursion_adjusted and sys.getrecursionlimit() != original_limit:\n                sys.setrecursionlimit(original_limit)\n\n        if include_depth_metrics:\n            return particles, depth_metrics\n        return particles",
      "complexity": 0,
      "lines_of_code": 62,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_function_name",
      "name": "TreeSitterUniversalEngine._extract_function_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 940,
      "end_line": 974,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "line",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract a function name from a declaration line (best-effort, regex-based).",
      "signature": "def _extract_function_name(self, line: str, language: str) -> Optional[str]:",
      "body_source": "    def _extract_function_name(self, line: str, language: str) -> Optional[str]:\n        \"\"\"Extract a function name from a declaration line (best-effort, regex-based).\"\"\"\n        if language == 'python':\n            m = re.search(r'^(?:async\\s+def|def)\\s+(\\w+)', line)\n            return m.group(1) if m else None\n\n        if language == 'go':\n            # func Name( or func (r Receiver) Name(\n            m = re.search(r'^func\\s+(?:\\([^)]*\\)\\s*)?(\\w+)\\s*\\(', line)\n            return m.group(1) if m else None\n\n        if language == 'rust':\n            # fn name( or pub fn name(\n            m = re.search(r'^(?:pub\\s+)?fn\\s+(\\w+)\\s*\\(', line)\n            return m.group(1) if m else None\n\n        if language in {'javascript', 'typescript'}:\n            # export default async function name(\n            m = re.search(r'^(?:export\\s+)?(?:default\\s+)?(?:async\\s+)?function\\s+(\\w+)\\s*\\(', line)\n            if m:\n                return m.group(1)\n\n            # export const name = (...) => / const name = async (...) =>\n            m = re.search(\n                r'^(?:export\\s+)?(?:const|let|var)\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\(?[^=]*=>',\n                line,\n            )\n            if m:\n                return m.group(1)\n\n            return None\n\n        # Java/C#/Kotlin/TS methods (best-effort): last identifier before \"(\"\n        m = re.search(r'(\\w+)\\s*\\(', line)\n        return m.group(1) if m else None",
      "complexity": 0,
      "lines_of_code": 34,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_particle_type_by_name",
      "name": "TreeSitterUniversalEngine._get_particle_type_by_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 976,
      "end_line": 1052,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Determine particle type by naming conventions",
      "signature": "def _get_particle_type_by_name(self, name: str) -> Optional[str]:",
      "body_source": "    def _get_particle_type_by_name(self, name: str) -> Optional[str]:\n        \"\"\"Determine particle type by naming conventions\"\"\"\n        tokens = set(self._tokenize_identifier(name))\n        name_lower = name.lower()\n\n        # AggregateRoot has highest priority in naming\n        if 'aggregate' in tokens and 'root' in tokens:\n            return 'AggregateRoot'\n        if name_lower.endswith('aggregate'):\n            return 'AggregateRoot'\n        \n        # Events - check for Event suffix or common event patterns\n        if name_lower.endswith('event'):\n            return 'Event'\n        if tokens & {'event', 'occurred', 'happened', 'created', 'updated', 'deleted', 'changed'}:\n            # Only if it looks like an event name (past tense verbs)\n            if any(name_lower.endswith(suffix) for suffix in ['created', 'updated', 'deleted', 'changed', 'placed', 'cancelled', 'completed', 'started', 'finished']):\n                return 'Event'\n        \n        if tokens & {'entity', 'model'}:\n            return 'Entity'\n        elif tokens & {'repository', 'repo'}:\n            return 'Repository'\n        elif tokens & {'controller', 'view', 'api'}:\n            return 'Controller'\n        elif tokens & {'service', 'handler', 'engine', 'extractor', 'generator', 'loader'}:\n            return 'Service'\n        elif tokens & {'value', 'vo'}:\n            return 'ValueObject'\n        elif 'factory' in tokens:\n            return 'Factory'\n        elif tokens & {'spec', 'specification'}:\n            return 'Specification'\n        elif 'command' in tokens:\n            return 'Command'\n        elif 'query' in tokens:\n            return 'Query'\n        elif 'usecase' in tokens or ('use' in tokens and 'case' in tokens) or 'use_case' in name_lower:\n            return 'UseCase'\n        elif tokens & {'dto', 'request', 'response', 'schema'}:\n            return 'DTO'\n        elif tokens & {'error', 'exception'}:\n            return 'Exception'\n        elif tokens & {'config', 'settings', 'env'}:\n            return 'Configuration'\n        elif tokens & {'provider', 'module'}:\n            return 'Provider'\n        elif tokens & {'test', 'tests', 'spec', 'suite'}:\n            return 'Test'\n        elif 'utils' in tokens or 'helper' in tokens:\n            return 'Utility'\n        elif 'builder' in tokens:\n            return 'Builder'\n        elif 'adapter' in tokens:\n            return 'Adapter'\n        \n        # NEW PATTERNS for improved coverage\n        elif tokens & {'parser', 'parse', 'deserialize', 'serialize'}:\n            return 'Utility'\n        elif tokens & {'mapper', 'mapping', 'converter', 'convert', 'transform', 'translator'}:\n            return 'Mapper'\n        elif tokens & {'client', 'consumer', 'caller'}:\n            return 'Client'\n        elif tokens & {'gateway', 'facade', 'proxy'}:\n            return 'Gateway'\n        elif tokens & {'middleware', 'interceptor', 'filter'}:\n            return 'Adapter'\n        elif tokens & {'listener', 'watcher', 'monitor'}:\n            return 'Observer'\n        elif tokens & {'strategy', 'policy', 'rule'}:\n            return 'Policy'\n        elif tokens & {'state', 'status', 'context'}:\n            return 'Entity'\n        elif tokens & {'result', 'outcome', 'output'}:\n            return 'DTO'\n\n        return None",
      "complexity": 0,
      "lines_of_code": 76,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._get_function_type_by_name",
      "name": "TreeSitterUniversalEngine._get_function_type_by_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1054,
      "end_line": 1146,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Determine function particle type by naming",
      "signature": "def _get_function_type_by_name(self, name: str) -> Optional[str]:",
      "body_source": "    def _get_function_type_by_name(self, name: str) -> Optional[str]:\n        \"\"\"Determine function particle type by naming\"\"\"\n        name_lower = name.lower()\n        tokens = set(self._tokenize_identifier(name))\n\n        if tokens & {'handle', 'handler'} or name_lower.endswith('_handler'):\n            return 'EventHandler'\n        if tokens & {'on', 'when', 'observe', 'observer', 'listener', 'subscribe', 'subscriber'}:\n            return 'Observer'\n\n        if tokens & {'create', 'make', 'build'}:\n            return 'Factory'\n\n        if tokens & {'validate', 'check', 'verify', 'ensure', 'require'}:\n            return 'Specification'\n\n        if tokens & {'get', 'fetch', 'find', 'list', 'read', 'load'}:\n            return 'Query'\n\n        if tokens & {'save', 'commit', 'upsert', 'delete', 'write', 'sync', 'import', 'export', 'connect', 'disconnect', 'purge'}:\n            return 'Command'\n\n        if tokens & {'execute', 'run', 'start', 'stop', 'restart'}:\n            return 'UseCase'\n\n        if 'apply' in tokens:\n            return 'Policy'\n\n        if tokens & {'process', 'orchestrate'}:\n            return 'DomainService'\n\n        if tokens & {'is', 'has', 'can', 'should'}:\n            return 'Specification'\n\n        if tokens & {'setup', 'configure', 'config', 'init', 'bootstrap'}:\n            return 'Service'\n\n        # ========================\n        # NEW PATTERNS FOR COVERAGE\n        # ========================\n        \n        # Test functions (test_*, *_test)\n        if name_lower.startswith('test_') or name_lower.endswith('_test'):\n            return 'Test'\n        if 'test' in tokens:\n            return 'Test'\n        \n        # Private/internal functions  \n        if name_lower.startswith('_') and not name_lower.startswith('__'):\n            # Could be internal helper - classify by context\n            pass  # Let other rules handle, but mark if nothing else matches\n            \n        # Validators\n        if tokens & {'validate', 'validator', 'assert', 'check'}:\n            return 'Validator'\n            \n        # Parsers and converters\n        if tokens & {'parse', 'parser', 'convert', 'converter', 'deserialize', 'serialize', 'format', 'formatter'}:\n            return 'Utility'\n            \n        # Generators and builders\n        if tokens & {'generate', 'generator', 'yield'}:\n            return 'Factory'\n            \n        # Error handling\n        if tokens & {'error', 'exception', 'raise', 'fail'}:\n            return 'Exception'\n            \n        # Callbacks and hooks\n        if tokens & {'callback', 'hook', 'trigger', 'emit'}:\n            return 'EventHandler'\n            \n        # Comparators\n        if tokens & {'compare', 'equals', 'equal', 'match', 'matches', 'diff'}:\n            return 'Specification'\n            \n        # Decorators (functions that are decorators)\n        if tokens & {'decorator', 'decorate', 'wrap', 'wrapper'}:\n            return 'Utility'\n        \n        # Async patterns\n        if tokens & {'async', 'await', 'coroutine'}:\n            return 'Service'\n            \n        # Render/display\n        if tokens & {'render', 'display', 'show', 'print', 'format'}:\n            return 'Utility'\n            \n        # Cleanup/teardown\n        if tokens & {'cleanup', 'teardown', 'dispose', 'close', 'shutdown'}:\n            return 'Service'\n\n        return None",
      "complexity": 0,
      "lines_of_code": 92,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._detect_by_keywords",
      "name": "TreeSitterUniversalEngine._detect_by_keywords",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1148,
      "end_line": 1157,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "line",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect pattern by keywords in line",
      "signature": "def _detect_by_keywords(self, line: str) -> Optional[str]:",
      "body_source": "    def _detect_by_keywords(self, line: str) -> Optional[str]:\n        \"\"\"Detect pattern by keywords in line\"\"\"\n        if any(keyword in line for keyword in ['@dataclass', 'frozen', 'immutable']):\n            return 'ValueObject'\n        elif any(keyword in line for keyword in ['interface', 'abstract', 'protocol']):\n            return 'Service'\n        elif '@' in line and any(keyword in line for keyword in ['route', 'get', 'post']):\n            return 'Controller'\n\n        return None",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._calculate_confidence",
      "name": "TreeSitterUniversalEngine._calculate_confidence",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1159,
      "end_line": 1173,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "line",
          "type": "str"
        }
      ],
      "return_type": "float",
      "base_classes": [],
      "decorators": [],
      "docstring": "Calculate confidence score for detection",
      "signature": "def _calculate_confidence(self, name: str, line: str) -> float:",
      "body_source": "    def _calculate_confidence(self, name: str, line: str) -> float:\n        \"\"\"Calculate confidence score for detection\"\"\"\n        confidence = 50.0  # Base confidence\n\n        name_lower = name.lower()\n\n        # Naming patterns\n        if any(pattern in name_lower for pattern in ['entity', 'repository', 'service', 'controller', 'value']):\n            confidence += 25.0\n\n        # Keywords\n        if any(keyword in line for keyword in ['@dataclass', 'frozen', 'immutable', 'interface', 'abstract']):\n            confidence += 20.0\n\n        return min(confidence, 100.0)",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_touchpoints",
      "name": "TreeSitterUniversalEngine._extract_touchpoints",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1175,
      "end_line": 1201,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract universal touchpoints from content",
      "signature": "def _extract_touchpoints(self, content: str, particles: List[Dict]) -> List[Dict]:",
      "body_source": "    def _extract_touchpoints(self, content: str, particles: List[Dict]) -> List[Dict]:\n        \"\"\"Extract universal touchpoints from content\"\"\"\n        touchpoints = []\n\n        # Define touchpoint indicators\n        touchpoint_patterns = {\n            'identity': [r'\\b(id|uuid|identifier|key|_id)\\b'],\n            'state': [r'\\b(property|attribute|field|member)\\b'],\n            'data_access': [r'\\b(save|find|delete|query|persist|retrieve)\\b'],\n            'immutability': [r'\\b(frozen|immutable|readonly|final|const)\\b'],\n            'validation': [r'\\b(validate|check|verify|ensure|require)\\b'],\n            'coordination': [r'\\b(coordinate|manage|orchestrate|mediate)\\b']\n        }\n\n        for touchpoint, patterns in touchpoint_patterns.items():\n            for pattern in patterns:\n                matches = re.finditer(pattern, content, re.IGNORECASE)\n                for match in matches:\n                    line_num = content[:match.start()].count('\\n') + 1\n                    touchpoints.append({\n                        'type': touchpoint,\n                        'evidence': match.group(),\n                        'line': line_num,\n                        'confidence': 75.0\n                    })\n\n        return touchpoints",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_raw_imports",
      "name": "TreeSitterUniversalEngine._extract_raw_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1203,
      "end_line": 1221,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract raw import statements for dependency analysis.",
      "signature": "def _extract_raw_imports(self, content: str, language: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_raw_imports(self, content: str, language: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract raw import statements for dependency analysis.\"\"\"\n        if language == 'python':\n            return self._extract_python_imports(content)\n        if language in {'javascript', 'typescript'}:\n            return self._extract_js_ts_imports(content)\n        if language in {'java', 'kotlin'}:\n            return self._extract_java_like_imports(content)\n        if language == 'c_sharp':\n            return self._extract_csharp_imports(content)\n        if language == 'go':\n            return self._extract_go_imports(content)\n        if language == 'rust':\n            return self._extract_rust_imports(content)\n        if language == 'ruby':\n            return self._extract_ruby_imports(content)\n        if language == 'php':\n            return self._extract_php_imports(content)\n        return []",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_python_imports",
      "name": "TreeSitterUniversalEngine._extract_python_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1223,
      "end_line": 1256,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_python_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_python_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        try:\n            tree = ast.parse(content)\n        except Exception:\n            return imports\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Import):\n                for alias in node.names:\n                    if alias.name:\n                        imports.append(\n                            {\n                                'kind': 'import',\n                                'target': alias.name,\n                                'line': getattr(node, 'lineno', 0) or 0,\n                                'is_relative': False,\n                                'level': 0,\n                            }\n                        )\n            elif isinstance(node, ast.ImportFrom):\n                module = node.module or ''\n                level = getattr(node, 'level', 0) or 0\n                imports.append(\n                    {\n                        'kind': 'from_import',\n                        'target': module,\n                        'line': getattr(node, 'lineno', 0) or 0,\n                        'is_relative': bool(level),\n                        'level': level,\n                    }\n                )\n\n        return imports",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_js_ts_imports",
      "name": "TreeSitterUniversalEngine._extract_js_ts_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1258,
      "end_line": 1317,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_js_ts_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_js_ts_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        in_block_comment = False\n\n        for i, raw in enumerate(content.splitlines(), 1):\n            line = raw\n            stripped = line.strip()\n\n            if in_block_comment:\n                if '*/' in stripped:\n                    in_block_comment = False\n                continue\n            if stripped.startswith('/*'):\n                in_block_comment = True\n                continue\n            if stripped.startswith('//'):\n                continue\n\n            # import ... from 'x'  | import 'x'\n            m = re.match(r'^\\s*import\\s+(?:type\\s+)?(?:.+?\\s+from\\s+)?[\\'\"]([^\\'\"]+)[\\'\"]', line)\n            if m:\n                target = m.group(1)\n                imports.append(\n                    {\n                        'kind': 'import',\n                        'target': target,\n                        'line': i,\n                        'is_relative': target.startswith(('.', '/')),\n                        'level': 0,\n                    }\n                )\n                continue\n\n            # require('x')\n            for m in re.finditer(r'\\brequire\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)', line):\n                target = m.group(1)\n                imports.append(\n                    {\n                        'kind': 'require',\n                        'target': target,\n                        'line': i,\n                        'is_relative': target.startswith(('.', '/')),\n                        'level': 0,\n                    }\n                )\n\n            # dynamic import('x')\n            for m in re.finditer(r'\\bimport\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)', line):\n                target = m.group(1)\n                imports.append(\n                    {\n                        'kind': 'dynamic_import',\n                        'target': target,\n                        'line': i,\n                        'is_relative': target.startswith(('.', '/')),\n                        'level': 0,\n                    }\n                )\n\n        return imports",
      "complexity": 0,
      "lines_of_code": 59,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_java_like_imports",
      "name": "TreeSitterUniversalEngine._extract_java_like_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1319,
      "end_line": 1327,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_java_like_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_java_like_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        for i, line in enumerate(content.splitlines(), 1):\n            m = re.match(r'^\\s*import\\s+([a-zA-Z0-9_.*]+)\\s*;', line)\n            if not m:\n                continue\n            target = m.group(1)\n            imports.append({'kind': 'import', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n        return imports",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_csharp_imports",
      "name": "TreeSitterUniversalEngine._extract_csharp_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1329,
      "end_line": 1339,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_csharp_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_csharp_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        for i, line in enumerate(content.splitlines(), 1):\n            if 'using (' in line or line.strip().startswith('using ('):\n                continue\n            m = re.match(r'^\\s*using\\s+([a-zA-Z0-9_.]+)\\s*;', line)\n            if not m:\n                continue\n            target = m.group(1)\n            imports.append({'kind': 'using', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n        return imports",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_go_imports",
      "name": "TreeSitterUniversalEngine._extract_go_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1341,
      "end_line": 1365,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_go_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_go_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        in_block = False\n\n        for i, line in enumerate(content.splitlines(), 1):\n            stripped = line.strip()\n            if stripped.startswith('import ('):\n                in_block = True\n                continue\n            if in_block:\n                if stripped.startswith(')'):\n                    in_block = False\n                    continue\n                m = re.search(r'\\\"([^\\\"]+)\\\"', stripped)\n                if m:\n                    target = m.group(1)\n                    imports.append({'kind': 'import', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n                continue\n\n            m = re.match(r'^\\s*import\\s+\\\"([^\\\"]+)\\\"', line)\n            if m:\n                target = m.group(1)\n                imports.append({'kind': 'import', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n\n        return imports",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_rust_imports",
      "name": "TreeSitterUniversalEngine._extract_rust_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1367,
      "end_line": 1375,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_rust_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_rust_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        for i, line in enumerate(content.splitlines(), 1):\n            m = re.match(r'^\\s*use\\s+([^;]+);', line)\n            if not m:\n                continue\n            target = m.group(1).strip()\n            imports.append({'kind': 'use', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n        return imports",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_ruby_imports",
      "name": "TreeSitterUniversalEngine._extract_ruby_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1377,
      "end_line": 1393,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_ruby_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_ruby_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        for i, line in enumerate(content.splitlines(), 1):\n            m = re.match(r'^\\s*require(_relative)?\\s+[\\'\"]([^\\'\"]+)[\\'\"]', line)\n            if not m:\n                continue\n            target = m.group(2)\n            imports.append(\n                {\n                    'kind': 'require_relative' if m.group(1) else 'require',\n                    'target': target,\n                    'line': i,\n                    'is_relative': bool(m.group(1)) or target.startswith(('.', '/')),\n                    'level': 0,\n                }\n            )\n        return imports",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_php_imports",
      "name": "TreeSitterUniversalEngine._extract_php_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1395,
      "end_line": 1407,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _extract_php_imports(self, content: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_php_imports(self, content: str) -> List[Dict[str, Any]]:\n        imports: List[Dict[str, Any]] = []\n        for i, line in enumerate(content.splitlines(), 1):\n            m = re.match(r'^\\s*use\\s+([^;]+);', line)\n            if m:\n                target = m.group(1).strip()\n                imports.append({'kind': 'use', 'target': target, 'line': i, 'is_relative': False, 'level': 0})\n                continue\n            m = re.match(r'^\\s*(require|include)(_once)?\\s*\\(?\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)?\\s*;', line)\n            if m:\n                target = m.group(3)\n                imports.append({'kind': m.group(1), 'target': target, 'line': i, 'is_relative': target.startswith(('.', '/')), 'level': 0})\n        return imports",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._fallback_analysis",
      "name": "TreeSitterUniversalEngine._fallback_analysis",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1409,
      "end_line": 1439,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Fallback analysis for unsupported languages",
      "signature": "def _fallback_analysis(self, file_path: str) -> Dict[str, Any]:",
      "body_source": "    def _fallback_analysis(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Fallback analysis for unsupported languages\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n        except Exception:\n            return {'particles': [], 'touchpoints': [], 'language': 'unknown'}\n\n        # Basic regex analysis\n        particles = []\n        touchpoints = []\n\n        # Simple class-like detection\n        for i, line in enumerate(content.split('\\n'), 1):\n            if re.search(r'\\b(class|interface|struct|type)\\s+\\w+', line):\n                particles.append({\n                    'type': 'Unknown',\n                    'name': 'detected_pattern',\n                    'line': i,\n                    'confidence': 30.0,\n                    'evidence': line.strip()\n                })\n\n        return {\n            'file_path': file_path,\n            'language': 'unknown',\n            'particles': particles,\n            'touchpoints': touchpoints,\n            'lines_analyzed': len(content.split('\\n')),\n            'chars_analyzed': len(content)\n        }",
      "complexity": 0,
      "lines_of_code": 30,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._summarize_depth_metrics",
      "name": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1441,
      "end_line": 1498,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "metrics",
          "type": "List[Dict[str, Any]]"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Aggregate depth metrics across files.",
      "signature": "def _summarize_depth_metrics(self, metrics: List[Dict[str, Any]]) -> Dict[str, Any]:",
      "body_source": "    def _summarize_depth_metrics(self, metrics: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Aggregate depth metrics across files.\"\"\"\n        if not metrics:\n            return {}\n\n        depths = [m.get(\"max_depth\", 0) for m in metrics if m.get(\"max_depth\") is not None]\n        nodes = [m.get(\"node_count\", 0) for m in metrics if m.get(\"node_count\") is not None]\n        required_limits = [\n            m.get(\"required_recursion_limit\", 0)\n            for m in metrics\n            if m.get(\"required_recursion_limit\") is not None\n        ]\n        adjusted_limits = [\n            m.get(\"recursion_adjusted_to\", 0)\n            for m in metrics\n            if m.get(\"recursion_adjusted_to\") is not None\n        ]\n\n        depths_sorted = sorted(depths)\n        p95_depth = 0\n        if depths_sorted:\n            idx = int(0.95 * (len(depths_sorted) - 1))\n            p95_depth = depths_sorted[idx]\n\n        strategy_counts = Counter(\n            m.get(\"strategy\", \"unknown\") for m in metrics if m.get(\"strategy\")\n        )\n        fallback_counts = Counter(\n            m.get(\"fallback_reason\", \"unknown\") for m in metrics if m.get(\"fallback_reason\")\n        )\n\n        deepest_files = sorted(\n            (\n                {\n                    \"file_path\": m.get(\"file_path\", \"\"),\n                    \"max_depth\": m.get(\"max_depth\", 0),\n                    \"node_count\": m.get(\"node_count\", 0),\n                }\n                for m in metrics\n            ),\n            key=lambda x: x[\"max_depth\"],\n            reverse=True,\n        )[:5]\n\n        return {\n            \"scope\": \"python_ast\",\n            \"files_measured\": len(depths),\n            \"max_ast_depth\": max(depths) if depths else 0,\n            \"avg_ast_depth\": sum(depths) / len(depths) if depths else 0,\n            \"p95_ast_depth\": p95_depth,\n            \"max_ast_nodes\": max(nodes) if nodes else 0,\n            \"avg_ast_nodes\": sum(nodes) / len(nodes) if nodes else 0,\n            \"max_required_recursion_limit\": max(required_limits) if required_limits else 0,\n            \"max_recursion_adjusted_to\": max(adjusted_limits) if adjusted_limits else 0,\n            \"strategy_counts\": dict(strategy_counts),\n            \"fallback_reasons\": dict(fallback_counts),\n            \"deepest_files\": deepest_files,\n        }",
      "complexity": 0,
      "lines_of_code": 57,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine.analyze_directory",
      "name": "TreeSitterUniversalEngine.analyze_directory",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1500,
      "end_line": 1543,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "directory_path",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze all supported files in directory",
      "signature": "def analyze_directory(self, directory_path: str) -> List[Dict[str, Any]]:",
      "body_source": "    def analyze_directory(self, directory_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Analyze all supported files in directory\"\"\"\n        results: List[Dict[str, Any]] = []\n        depth_metrics: List[Dict[str, Any]] = []\n\n        # Prefer TypeScript AST extraction for JS/TS when available (richer symbol mapping).\n        ts_results = self._extract_js_ts_directory_with_typescript(directory_path)\n        ts_files_abs = {Path(r.get(\"file_path\", \"\")).resolve() for r in ts_results if r.get(\"file_path\")}\n        results.extend(ts_results)\n\n        for root, dirs, files in os.walk(directory_path):\n            # Skip common ignore directories\n            dirs[:] = [\n                d\n                for d in dirs\n                if d\n                not in [\n                    '.git',\n                    '__pycache__',\n                    'node_modules',\n                    'venv',\n                    '.venv',\n                    'dist',\n                    'build',\n                    'coverage',\n                    '.next',\n                    '.turbo',\n                    '.cache',\n                ]\n            ]\n\n            for file in files:\n                if Path(file).suffix in self.supported_languages:\n                    file_path = os.path.join(root, file)\n                    # Skip JS/TS files already handled by TypeScript extractor.\n                    if Path(file_path).resolve() in ts_files_abs:\n                        continue\n                    result = self.analyze_file(file_path)\n                    if result.get(\"depth_metrics\"):\n                        depth_metrics.append(result[\"depth_metrics\"])\n                    results.append(result)\n\n        self.depth_summary = self._summarize_depth_metrics(depth_metrics)\n        return results",
      "complexity": 0,
      "lines_of_code": 43,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py:TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "name": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/tree_sitter_engine.py",
      "start_line": 1545,
      "end_line": 1608,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "directory_path",
          "type": "str"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract JS/TS symbols using the TypeScript compiler API (via Node), when available.",
      "signature": "def _extract_js_ts_directory_with_typescript(self, directory_path: str) -> List[Dict[str, Any]]:",
      "body_source": "    def _extract_js_ts_directory_with_typescript(self, directory_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract JS/TS symbols using the TypeScript compiler API (via Node), when available.\"\"\"\n        if not self._ts_symbol_extractor.exists():\n            return []\n\n        try:\n            proc = subprocess.run(\n                [\"node\", str(self._ts_symbol_extractor), directory_path],\n                capture_output=True,\n                text=True,\n                timeout=120,\n            )\n        except Exception:\n            return []\n\n        if proc.returncode != 0 or not proc.stdout:\n            return []\n\n        try:\n            payload = json.loads(proc.stdout)\n        except Exception:\n            return []\n\n        if not payload.get(\"ok\"):\n            return []\n\n        out: List[Dict[str, Any]] = []\n        for f in payload.get(\"files\", []) or []:\n            file_path = str(f.get(\"file_path\") or \"\")\n            language = str(f.get(\"language\") or \"unknown\")\n            if language not in {\"javascript\", \"typescript\"} or not file_path:\n                continue\n\n            particles: List[Dict[str, Any]] = []\n            for sym in f.get(\"particles\", []) or []:\n                name = str(sym.get(\"name\") or \"\")\n                symbol_kind = str(sym.get(\"symbol_kind\") or \"unknown\")\n                line_num = int(sym.get(\"line\") or 0)\n                evidence = str(sym.get(\"evidence\") or \"\")\n                parent = str(sym.get(\"parent\") or \"\")\n                particles.append(\n                    self._classify_extracted_symbol(\n                        name=name,\n                        symbol_kind=symbol_kind,\n                        file_path=file_path,\n                        line_num=line_num,\n                        evidence=evidence,\n                        parent=parent,\n                    )\n                )\n\n            out.append(\n                {\n                    \"file_path\": file_path,\n                    \"language\": language,\n                    \"particles\": particles,\n                    \"touchpoints\": f.get(\"touchpoints\") or [],\n                    \"raw_imports\": f.get(\"raw_imports\") or [],\n                    \"lines_analyzed\": int(f.get(\"lines_analyzed\") or 0),\n                    \"chars_analyzed\": int(f.get(\"chars_analyzed\") or 0),\n                }\n            )\n\n        return out",
      "complexity": 0,
      "lines_of_code": 63,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:Edge",
      "name": "Edge",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 23,
      "end_line": 23,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Edge:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:Node",
      "name": "Node",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 34,
      "end_line": 34,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Node:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:CodeGraph",
      "name": "CodeGraph",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 48,
      "end_line": 48,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class CodeGraph:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:CodeGraph.add_node",
      "name": "CodeGraph.add_node",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 60,
      "end_line": 61,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def add_node(self, node: Node):",
      "body_source": "    def add_node(self, node: Node):\n        self.nodes[node.id] = node",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:CodeGraph.add_edge",
      "name": "CodeGraph.add_edge",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 63,
      "end_line": 75,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "edge",
          "type": "Edge"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def add_edge(self, edge: Edge):",
      "body_source": "    def add_edge(self, edge: Edge):\n        self.edges.append(edge)\n        \n        # Index by type\n        if edge.edge_type == \"call\":\n            self.call_graph[edge.source].append(edge.target)\n            self.reverse_call_graph[edge.target].append(edge.source)\n        elif edge.edge_type == \"import\":\n            self.import_graph[edge.source].append(edge.target)\n        elif edge.edge_type == \"inherit\":\n            self.inheritance_graph[edge.source].append(edge.target)\n        elif edge.edge_type == \"data_flow\":\n            self.data_flow[edge.source].append(edge.target)",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:CodeGraph.get_stats",
      "name": "CodeGraph.get_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 77,
      "end_line": 85,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def get_stats(self) -> Dict:",
      "body_source": "    def get_stats(self) -> Dict:\n        return {\n            \"nodes\": len(self.nodes),\n            \"edges\": len(self.edges),\n            \"call_edges\": sum(len(v) for v in self.call_graph.values()),\n            \"import_edges\": sum(len(v) for v in self.import_graph.values()),\n            \"inherit_edges\": sum(len(v) for v in self.inheritance_graph.values()),\n            \"data_flow_edges\": sum(len(v) for v in self.data_flow.values()),\n        }",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor",
      "name": "GraphExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 88,
      "end_line": 88,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class GraphExtractor:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.__init__",
      "name": "GraphExtractor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 95,
      "end_line": 97,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.parsers = {}\n        self._init_parsers()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._init_parsers",
      "name": "GraphExtractor._init_parsers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 99,
      "end_line": 101,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _init_parsers(self):",
      "body_source": "    def _init_parsers(self):\n        from core.language_loader import LanguageLoader\n        self.parsers, self.languages, self.extensions = LanguageLoader.load_all()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.extract",
      "name": "GraphExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 103,
      "end_line": 160,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str",
          "default": "'python'"
        }
      ],
      "return_type": "CodeGraph",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract complete code graph from a repository.",
      "signature": "def extract(self, repo_path: str, language: str = \"python\") -> CodeGraph:",
      "body_source": "    def extract(self, repo_path: str, language: str = \"python\") -> CodeGraph:\n        \"\"\"\n        Extract complete code graph from a repository.\n        \"\"\"\n        path = Path(repo_path)\n        graph = CodeGraph()\n        \n        if language not in self.parsers:\n            # Try to auto-detect or fallback? For now raise.\n            # But wait, 'typescript' vs 'tsx'. User passes 'typescript'.\n            # We should probably check if language is valid.\n            if language not in self.parsers:\n                # Fallback for 'typescript' which might mean 'tsx' too?\n                # The loader returns 'typescript' and 'tsx' as separate keys.\n                raise ValueError(f\"Unsupported language: {language}\")\n        \n        parser = self.parsers[language]\n        \n        # First pass: collect all files and their structure\n        file_contents = {}\n        \n        # Use extensions from loader\n        patterns = self.extensions.get(language, [\"*.py\"])\n        found_files = []\n        for pattern in patterns:\n            # Ensure pattern is a glob (e.g. \".ts\" -> \"*.ts\")\n            glob_pat = pattern if pattern.startswith(\"*\") else f\"*{pattern}\"\n            found_files.extend(list(path.rglob(glob_pat)))\n            # Also check for src/ for TS if needed commonly?\n            # Loader puts simple exts. Let's add recursive search for them.\n            # actually rglob('*.ts') is recursive. 'src/**/*.ts' is redundant if we do pattern='*.ts'.\n            \n        for py_file in found_files:\n            if any(x in str(py_file) for x in [\"__pycache__\", \"node_modules\", \".git\", \".venv\", \"dddlint_env\", \"output/\"]):\n                continue\n            \n            try:\n                rel_path = str(py_file.relative_to(path))\n                code = py_file.read_bytes()\n                tree = parser.parse(code)\n                file_contents[rel_path] = (code, tree)\n                \n                # Add file node\n                graph.add_node(Node(\n                    id=f\"file:{rel_path}\",\n                    name=rel_path,\n                    node_type=\"file\",\n                    raw_type=\"file\",\n                    file=rel_path,\n                ))\n            except Exception as e:\n                pass\n        \n        # Second pass: extract structure and relationships\n        for rel_path, (code, tree) in file_contents.items():\n            self._extract_from_file(graph, rel_path, code, tree)\n        \n        return graph",
      "complexity": 0,
      "lines_of_code": 57,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._extract_from_file",
      "name": "GraphExtractor._extract_from_file",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 162,
      "end_line": 294,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "code",
          "type": "bytes"
        },
        {
          "name": "tree"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract all nodes and edges from a single file.",
      "signature": "def _extract_from_file(self, graph: CodeGraph, file_path: str, code: bytes, tree):",
      "body_source": "    def _extract_from_file(self, graph: CodeGraph, file_path: str, code: bytes, tree):\n        \"\"\"Extract all nodes and edges from a single file.\"\"\"\n        \n        # Track current scope for qualified names\n        scope_stack = [file_path]\n        \n        def get_qualified_name(name: str) -> str:\n            if len(scope_stack) > 1:\n                return f\"{scope_stack[-1]}.{name}\"\n            return f\"{file_path}:{name}\"\n        \n        def extract_name(node) -> str:\n            \"\"\"Extract identifier name from a node.\"\"\"\n            for child in node.children:\n                if child.type == \"identifier\":\n                    return child.text.decode()\n            return \"\"\n        \n        def visit(node, parent_class=None, parent_func=None):\n            node_type = node.type\n            \n            # \u2550\u2550\u2550 IMPORTS \u2550\u2550\u2550\n            if node_type == \"import_statement\":\n                # import x, y, z\n                for child in node.children:\n                    if child.type == \"dotted_name\":\n                        module_name = child.text.decode()\n                        graph.add_edge(Edge(\n                            source=f\"file:{file_path}\",\n                            target=f\"module:{module_name}\",\n                            edge_type=\"import\",\n                            file=file_path,\n                            line=node.start_point[0] + 1,\n                        ))\n            \n            elif node_type == \"import_from_statement\":\n                # from x import y\n                module_name = \"\"\n                for child in node.children:\n                    if child.type == \"dotted_name\":\n                        module_name = child.text.decode()\n                        break\n                    elif child.type == \"relative_import\":\n                        module_name = child.text.decode()\n                        break\n                \n                if module_name:\n                    graph.add_edge(Edge(\n                        source=f\"file:{file_path}\",\n                        target=f\"module:{module_name}\",\n                        edge_type=\"import\",\n                        file=file_path,\n                        line=node.start_point[0] + 1,\n                    ))\n            \n            # \u2550\u2550\u2550 CLASSES \u2550\u2550\u2550\n            elif node_type == \"class_definition\":\n                class_name = extract_name(node)\n                class_id = get_qualified_name(class_name)\n                \n                graph.add_node(Node(\n                    id=class_id,\n                    name=class_name,\n                    node_type=\"class\",\n                    raw_type=node.type,\n                    file=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=f\"file:{file_path}\",\n                ))\n                \n                # Extract inheritance (argument_list contains base classes)\n                for child in node.children:\n                    if child.type == \"argument_list\":\n                        for base in child.children:\n                            if base.type == \"identifier\":\n                                base_name = base.text.decode()\n                                graph.add_edge(Edge(\n                                    source=class_id,\n                                    target=f\"class:{base_name}\",\n                                    edge_type=\"inherit\",\n                                    file=file_path,\n                                    line=node.start_point[0] + 1,\n                                ))\n                            elif base.type == \"attribute\":\n                                base_name = base.text.decode()\n                                graph.add_edge(Edge(\n                                    source=class_id,\n                                    target=f\"class:{base_name}\",\n                                    edge_type=\"inherit\",\n                                    file=file_path,\n                                    line=node.start_point[0] + 1,\n                                ))\n                \n                # Process class body\n                scope_stack.append(class_id)\n                for child in node.children:\n                    if child.type == \"block\":\n                        for stmt in child.children:\n                            visit(stmt, parent_class=class_id)\n                scope_stack.pop()\n                return  # Don't recurse further\n            \n            # \u2550\u2550\u2550 FUNCTIONS \u2550\u2550\u2550\n            elif node_type in (\"function_definition\", \"async_function_definition\"):\n                func_name = extract_name(node)\n                func_id = get_qualified_name(func_name)\n                \n                graph.add_node(Node(\n                    id=func_id,\n                    name=func_name,\n                    node_type=\"function\",\n                    raw_type=node.type,\n                    file=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class or f\"file:{file_path}\",\n                ))\n                \n                # Process function body for calls\n                scope_stack.append(func_id)\n                for child in node.children:\n                    if child.type == \"block\":\n                        self._extract_calls(graph, child, func_id, file_path)\n                        self._extract_data_flow(graph, child, func_id, file_path)\n                scope_stack.pop()\n                return  # Don't recurse further\n            \n            # Recurse into children\n            for child in node.children:\n                visit(child, parent_class, parent_func)\n        \n        visit(tree.root_node)",
      "complexity": 0,
      "lines_of_code": 132,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.get_qualified_name",
      "name": "GraphExtractor.get_qualified_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 168,
      "end_line": 171,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def get_qualified_name(name: str) -> str:",
      "body_source": "        def get_qualified_name(name: str) -> str:\n            if len(scope_stack) > 1:\n                return f\"{scope_stack[-1]}.{name}\"\n            return f\"{file_path}:{name}\"",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.extract_name",
      "name": "GraphExtractor.extract_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 173,
      "end_line": 178,
      "role": "Transformer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract identifier name from a node.",
      "signature": "def extract_name(node) -> str:",
      "body_source": "        def extract_name(node) -> str:\n            \"\"\"Extract identifier name from a node.\"\"\"\n            for child in node.children:\n                if child.type == \"identifier\":\n                    return child.text.decode()\n            return \"\"",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.visit",
      "name": "GraphExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 180,
      "end_line": 292,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        },
        {
          "name": "parent_class",
          "default": "None"
        },
        {
          "name": "parent_func",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node, parent_class=None, parent_func=None):",
      "body_source": "        def visit(node, parent_class=None, parent_func=None):\n            node_type = node.type\n            \n            # \u2550\u2550\u2550 IMPORTS \u2550\u2550\u2550\n            if node_type == \"import_statement\":\n                # import x, y, z\n                for child in node.children:\n                    if child.type == \"dotted_name\":\n                        module_name = child.text.decode()\n                        graph.add_edge(Edge(\n                            source=f\"file:{file_path}\",\n                            target=f\"module:{module_name}\",\n                            edge_type=\"import\",\n                            file=file_path,\n                            line=node.start_point[0] + 1,\n                        ))\n            \n            elif node_type == \"import_from_statement\":\n                # from x import y\n                module_name = \"\"\n                for child in node.children:\n                    if child.type == \"dotted_name\":\n                        module_name = child.text.decode()\n                        break\n                    elif child.type == \"relative_import\":\n                        module_name = child.text.decode()\n                        break\n                \n                if module_name:\n                    graph.add_edge(Edge(\n                        source=f\"file:{file_path}\",\n                        target=f\"module:{module_name}\",\n                        edge_type=\"import\",\n                        file=file_path,\n                        line=node.start_point[0] + 1,\n                    ))\n            \n            # \u2550\u2550\u2550 CLASSES \u2550\u2550\u2550\n            elif node_type == \"class_definition\":\n                class_name = extract_name(node)\n                class_id = get_qualified_name(class_name)\n                \n                graph.add_node(Node(\n                    id=class_id,\n                    name=class_name,\n                    node_type=\"class\",\n                    raw_type=node.type,\n                    file=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=f\"file:{file_path}\",\n                ))\n                \n                # Extract inheritance (argument_list contains base classes)\n                for child in node.children:\n                    if child.type == \"argument_list\":\n                        for base in child.children:\n                            if base.type == \"identifier\":\n                                base_name = base.text.decode()\n                                graph.add_edge(Edge(\n                                    source=class_id,\n                                    target=f\"class:{base_name}\",\n                                    edge_type=\"inherit\",\n                                    file=file_path,\n                                    line=node.start_point[0] + 1,\n                                ))\n                            elif base.type == \"attribute\":\n                                base_name = base.text.decode()\n                                graph.add_edge(Edge(\n                                    source=class_id,\n                                    target=f\"class:{base_name}\",\n                                    edge_type=\"inherit\",\n                                    file=file_path,\n                                    line=node.start_point[0] + 1,\n                                ))\n                \n                # Process class body\n                scope_stack.append(class_id)\n                for child in node.children:\n                    if child.type == \"block\":\n                        for stmt in child.children:\n                            visit(stmt, parent_class=class_id)\n                scope_stack.pop()\n                return  # Don't recurse further\n            \n            # \u2550\u2550\u2550 FUNCTIONS \u2550\u2550\u2550\n            elif node_type in (\"function_definition\", \"async_function_definition\"):\n                func_name = extract_name(node)\n                func_id = get_qualified_name(func_name)\n                \n                graph.add_node(Node(\n                    id=func_id,\n                    name=func_name,\n                    node_type=\"function\",\n                    raw_type=node.type,\n                    file=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    parent=parent_class or f\"file:{file_path}\",\n                ))\n                \n                # Process function body for calls\n                scope_stack.append(func_id)\n                for child in node.children:\n                    if child.type == \"block\":\n                        self._extract_calls(graph, child, func_id, file_path)\n                        self._extract_data_flow(graph, child, func_id, file_path)\n                scope_stack.pop()\n                return  # Don't recurse further\n            \n            # Recurse into children\n            for child in node.children:\n                visit(child, parent_class, parent_func)",
      "complexity": 0,
      "lines_of_code": 112,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._extract_calls",
      "name": "GraphExtractor._extract_calls",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 296,
      "end_line": 315,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "block_node"
        },
        {
          "name": "caller_id",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract function calls from a code block.",
      "signature": "def _extract_calls(self, graph: CodeGraph, block_node, caller_id: str, file_path: str):",
      "body_source": "    def _extract_calls(self, graph: CodeGraph, block_node, caller_id: str, file_path: str):\n        \"\"\"Extract function calls from a code block.\"\"\"\n        \n        def visit(node):\n            if node.type == \"call\":\n                # Extract the function being called\n                callee = self._extract_callee(node)\n                if callee:\n                    graph.add_edge(Edge(\n                        source=caller_id,\n                        target=f\"func:{callee}\",\n                        edge_type=\"call\",\n                        file=file_path,\n                        line=node.start_point[0] + 1,\n                    ))\n            \n            for child in node.children:\n                visit(child)\n        \n        visit(block_node)",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.visit",
      "name": "GraphExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 299,
      "end_line": 313,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            if node.type == \"call\":\n                # Extract the function being called\n                callee = self._extract_callee(node)\n                if callee:\n                    graph.add_edge(Edge(\n                        source=caller_id,\n                        target=f\"func:{callee}\",\n                        edge_type=\"call\",\n                        file=file_path,\n                        line=node.start_point[0] + 1,\n                    ))\n            \n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._extract_callee",
      "name": "GraphExtractor._extract_callee",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 317,
      "end_line": 324,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "call_node"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract the name of the function being called.",
      "signature": "def _extract_callee(self, call_node) -> Optional[str]:",
      "body_source": "    def _extract_callee(self, call_node) -> Optional[str]:\n        \"\"\"Extract the name of the function being called.\"\"\"\n        for child in call_node.children:\n            if child.type == \"identifier\":\n                return child.text.decode()\n            elif child.type == \"attribute\":\n                return child.text.decode()\n        return None",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._extract_data_flow",
      "name": "GraphExtractor._extract_data_flow",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 326,
      "end_line": 358,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "block_node"
        },
        {
          "name": "scope_id",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract data flow (variable assignments and usage).",
      "signature": "def _extract_data_flow(self, graph: CodeGraph, block_node, scope_id: str, file_path: str):",
      "body_source": "    def _extract_data_flow(self, graph: CodeGraph, block_node, scope_id: str, file_path: str):\n        \"\"\"Extract data flow (variable assignments and usage).\"\"\"\n        \n        assignments = {}  # var_name -> line where assigned\n        \n        def visit(node):\n            if node.type == \"assignment\":\n                # Track what's being assigned\n                for child in node.children:\n                    if child.type == \"identifier\":\n                        var_name = child.text.decode()\n                        var_id = f\"{scope_id}::{var_name}\"\n                        \n                        graph.add_node(Node(\n                            id=var_id,\n                            name=var_name,\n                            node_type=\"variable\",\n                            file=file_path,\n                            start_line=node.start_point[0] + 1,\n                            parent=scope_id,\n                        ))\n                        \n                        assignments[var_name] = var_id\n                        break\n                \n                # Track what's being used (right side)\n                for child in node.children[1:]:\n                    self._track_variable_usage(graph, child, scope_id, assignments, file_path)\n            \n            for child in node.children:\n                visit(child)\n        \n        visit(block_node)",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.visit",
      "name": "GraphExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 331,
      "end_line": 356,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            if node.type == \"assignment\":\n                # Track what's being assigned\n                for child in node.children:\n                    if child.type == \"identifier\":\n                        var_name = child.text.decode()\n                        var_id = f\"{scope_id}::{var_name}\"\n                        \n                        graph.add_node(Node(\n                            id=var_id,\n                            name=var_name,\n                            node_type=\"variable\",\n                            file=file_path,\n                            start_line=node.start_point[0] + 1,\n                            parent=scope_id,\n                        ))\n                        \n                        assignments[var_name] = var_id\n                        break\n                \n                # Track what's being used (right side)\n                for child in node.children[1:]:\n                    self._track_variable_usage(graph, child, scope_id, assignments, file_path)\n            \n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor._track_variable_usage",
      "name": "GraphExtractor._track_variable_usage",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 360,
      "end_line": 376,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "node"
        },
        {
          "name": "scope_id",
          "type": "str"
        },
        {
          "name": "assignments",
          "type": "Dict"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Track variable usage to build data flow edges.",
      "signature": "def _track_variable_usage(self, graph: CodeGraph, node, scope_id: str,",
      "body_source": "    def _track_variable_usage(self, graph: CodeGraph, node, scope_id: str, \n                               assignments: Dict, file_path: str):\n        \"\"\"Track variable usage to build data flow edges.\"\"\"\n        if node.type == \"identifier\":\n            var_name = node.text.decode()\n            if var_name in assignments:\n                # Variable used from earlier assignment\n                graph.add_edge(Edge(\n                    source=assignments[var_name],\n                    target=f\"{scope_id}::usage:{var_name}\",\n                    edge_type=\"data_flow\",\n                    file=file_path,\n                    line=node.start_point[0] + 1,\n                ))\n        \n        for child in node.children:\n            self._track_variable_usage(graph, child, scope_id, assignments, file_path)",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.to_mermaid",
      "name": "GraphExtractor.to_mermaid",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 378,
      "end_line": 411,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "max_nodes",
          "type": "int",
          "default": "50"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate Mermaid diagram from graph.",
      "signature": "def to_mermaid(self, graph: CodeGraph, max_nodes: int = 50) -> str:",
      "body_source": "    def to_mermaid(self, graph: CodeGraph, max_nodes: int = 50) -> str:\n        \"\"\"Generate Mermaid diagram from graph.\"\"\"\n        lines = [\"graph TD\"]\n        \n        # Add nodes (limited for readability)\n        node_ids = set()\n        for i, (node_id, node) in enumerate(graph.nodes.items()):\n            if i >= max_nodes:\n                break\n            \n            safe_id = node_id.replace(\":\", \"_\").replace(\".\", \"_\").replace(\"/\", \"_\")\n            node_ids.add(node_id)\n            \n            if node.node_type == \"file\":\n                lines.append(f'    {safe_id}[\"{node.name}\"]')\n            elif node.node_type == \"class\":\n                lines.append(f'    {safe_id}[[\"\ud83c\udfdb\ufe0f {node.name}\"]]')\n            elif node.node_type == \"function\":\n                lines.append(f'    {safe_id}((\"\ud83d\udce6 {node.name}\"))')\n        \n        # Add edges\n        for edge in graph.edges[:100]:  # Limit edges\n            if edge.source in node_ids or edge.target in node_ids:\n                src = edge.source.replace(\":\", \"_\").replace(\".\", \"_\").replace(\"/\", \"_\")\n                tgt = edge.target.replace(\":\", \"_\").replace(\".\", \"_\").replace(\"/\", \"_\")\n                \n                if edge.edge_type == \"call\":\n                    lines.append(f'    {src} --> {tgt}')\n                elif edge.edge_type == \"import\":\n                    lines.append(f'    {src} -.-> {tgt}')\n                elif edge.edge_type == \"inherit\":\n                    lines.append(f'    {src} ==> {tgt}')\n        \n        return \"\\n\".join(lines)",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py:GraphExtractor.export_json",
      "name": "GraphExtractor.export_json",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_extractor.py",
      "start_line": 413,
      "end_line": 440,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph",
          "type": "CodeGraph"
        },
        {
          "name": "path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export graph to JSON for visualization.",
      "signature": "def export_json(self, graph: CodeGraph, path: str):",
      "body_source": "    def export_json(self, graph: CodeGraph, path: str):\n        \"\"\"Export graph to JSON for visualization.\"\"\"\n        data = {\n            \"nodes\": [\n                {\n                    \"id\": n.id,\n                    \"name\": n.name,\n                    \"type\": n.node_type,\n                    \"file\": n.file,\n                    \"start_line\": n.start_line,\n                    \"end_line\": n.end_line,\n                    \"parent\": n.parent,\n                } for n in graph.nodes.values()\n            ],\n            \"edges\": [\n                {\n                    \"source\": e.source,\n                    \"target\": e.target,\n                    \"type\": e.edge_type,\n                    \"file\": e.file,\n                    \"line\": e.line,\n                } for e in graph.edges\n            ],\n            \"stats\": graph.get_stats(),\n        }\n        \n        with open(path, 'w') as f:\n            json.dump(data, f, indent=2)",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py:AnalyzerConfig",
      "name": "AnalyzerConfig",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py",
      "start_line": 9,
      "end_line": 9,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class AnalyzerConfig:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py:AnalyzerConfig.config_hash",
      "name": "AnalyzerConfig.config_hash",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py",
      "start_line": 38,
      "end_line": 46,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [
        "property"
      ],
      "docstring": "Generate a stable SHA256 hash of the configuration.",
      "signature": "def config_hash(self) -> str:",
      "body_source": "    def config_hash(self) -> str:\n        \"\"\"Generate a stable SHA256 hash of the configuration.\"\"\"\n        # Convert to dict\n        data = asdict(self)\n        \n        # Create a canonical JSON representation (sorted keys are crucial)\n        # Tuple is naturally ordered in JSON as list, which works for stability\n        json_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py:AnalyzerConfig.save",
      "name": "AnalyzerConfig.save",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py",
      "start_line": 48,
      "end_line": 51,
      "role": "Command",
      "role_confidence": 82.0,
      "discovery_method": "structural:docstring_persist",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "path",
          "type": "Path"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Save config to a JSON file.",
      "signature": "def save(self, path: Path):",
      "body_source": "    def save(self, path: Path):\n        \"\"\"Save config to a JSON file.\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(self), f, indent=2, sort_keys=True)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py:AnalyzerConfig.__str__",
      "name": "AnalyzerConfig.__str__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/config.py",
      "start_line": 53,
      "end_line": 54,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __str__(self):",
      "body_source": "    def __str__(self):\n        return f\"AnalyzerConfig(ver={self.taxonomy_version}, rules={self.ruleset_version}, hash={self.config_hash[:8]})\"",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaConfig",
      "name": "OllamaConfig",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 17,
      "end_line": 17,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class OllamaConfig:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient",
      "name": "OllamaClient",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 27,
      "end_line": 27,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class OllamaClient:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient.__init__",
      "name": "OllamaClient.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 35,
      "end_line": 37,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "config",
          "type": "Optional[OllamaConfig]",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, config: Optional[OllamaConfig] = None):",
      "body_source": "    def __init__(self, config: Optional[OllamaConfig] = None):\n        self.config = config or OllamaConfig()\n        self._check_ollama_available()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient._check_ollama_available",
      "name": "OllamaClient._check_ollama_available",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 39,
      "end_line": 53,
      "role": "Validator",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if Ollama is running.",
      "signature": "def _check_ollama_available(self):",
      "body_source": "    def _check_ollama_available(self):\n        \"\"\"Check if Ollama is running.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"curl\", \"-s\", f\"{self.config.base_url}/api/tags\"],\n                capture_output=True,\n                timeout=5,\n            )\n            if result.returncode != 0:\n                raise RuntimeError(\"Ollama is not running. Start it with: ollama serve\")\n        except subprocess.TimeoutExpired:\n            raise RuntimeError(\"Ollama connection timed out\")\n        except FileNotFoundError:\n            # curl not available, try with requests\n            pass",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient._get_cache_key",
      "name": "OllamaClient._get_cache_key",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 55,
      "end_line": 58,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "prompt",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate cache key from prompt content hash.",
      "signature": "def _get_cache_key(self, prompt: str) -> str:",
      "body_source": "    def _get_cache_key(self, prompt: str) -> str:\n        \"\"\"Generate cache key from prompt content hash.\"\"\"\n        import hashlib\n        return hashlib.sha256(prompt.encode()).hexdigest()[:16]",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient.classify",
      "name": "OllamaClient.classify",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 60,
      "end_line": 115,
      "role": "Analyzer",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "system_prompt",
          "type": "str"
        },
        {
          "name": "user_prompt",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Call Ollama to classify a component.\n\nUses subprocess to call ollama CLI directly, which is more reliable\nthan the HTTP API when multiple instances may be running.\n\nReturns the raw JSON string response.\nImplements caching per paper \u00a75.2: \"LLM responses are cached by content hash.\"",
      "signature": "def classify(self, system_prompt: str, user_prompt: str) -> str:",
      "body_source": "    def classify(self, system_prompt: str, user_prompt: str) -> str:\n        \"\"\"\n        Call Ollama to classify a component.\n        \n        Uses subprocess to call ollama CLI directly, which is more reliable\n        than the HTTP API when multiple instances may be running.\n        \n        Returns the raw JSON string response.\n        Implements caching per paper \u00a75.2: \"LLM responses are cached by content hash.\"\n        \"\"\"\n        from pathlib import Path\n        \n        # Combine prompts for the CLI\n        full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\\n\\nRespond with valid JSON only.\"\n        \n        # Check cache first\n        cache_key = self._get_cache_key(full_prompt)\n        cache_dir = Path(self.config.cache_dir)\n        cache_path = cache_dir / f\"{cache_key}.json\"\n        \n        if cache_path.exists():\n            return cache_path.read_text()\n        \n        try:\n            result = subprocess.run(\n                [\"ollama\", \"run\", self.config.model, full_prompt],\n                capture_output=True,\n                text=True,\n                timeout=self.config.timeout,\n            )\n            \n            if result.returncode != 0:\n                raise RuntimeError(f\"Ollama failed: {result.stderr}\")\n            \n            content = result.stdout.strip()\n            \n            # Extract JSON from response (sometimes wrapped in markdown)\n            json_match = re.search(r'```json\\s*(.*?)\\s*```', content, re.DOTALL)\n            if json_match:\n                content = json_match.group(1)\n            else:\n                # Try to find raw JSON object (possibly nested)\n                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n                if json_match:\n                    content = json_match.group(0)\n            \n            # Cache the response\n            cache_dir.mkdir(exist_ok=True)\n            cache_path.write_text(content)\n            \n            return content\n            \n        except subprocess.TimeoutExpired:\n            raise RuntimeError(f\"Ollama timed out after {self.config.timeout}s\")\n        except FileNotFoundError:\n            raise RuntimeError(\"ollama CLI not found. Install from https://ollama.ai\")",
      "complexity": 0,
      "lines_of_code": 55,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:OllamaClient.is_available",
      "name": "OllamaClient.is_available",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 117,
      "end_line": 126,
      "role": "Specification",
      "role_confidence": 85.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if Ollama service is available.",
      "signature": "def is_available(self) -> bool:",
      "body_source": "    def is_available(self) -> bool:\n        \"\"\"Check if Ollama service is available.\"\"\"\n        try:\n            import urllib.request\n            # Just check if Ollama is responding, not specific models\n            req = urllib.request.Request(f\"{self.config.base_url}/api/version\")\n            with urllib.request.urlopen(req, timeout=5) as resp:\n                return resp.status == 200\n        except Exception:\n            return False",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py:classify_component_with_ollama",
      "name": "classify_component_with_ollama",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ollama_client.py",
      "start_line": 129,
      "end_line": 193,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "component_card",
          "type": "Dict[str, Any]"
        },
        {
          "name": "config",
          "type": "Optional[OllamaConfig]",
          "default": "None"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to classify a single component using Ollama.\n\nThis is a standalone function that can be used without the full\nLLMClassifier infrastructure.",
      "signature": "def classify_component_with_ollama(",
      "body_source": "def classify_component_with_ollama(\n    component_card: Dict[str, Any],\n    config: Optional[OllamaConfig] = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Convenience function to classify a single component using Ollama.\n    \n    This is a standalone function that can be used without the full\n    LLMClassifier infrastructure.\n    \"\"\"\n    from llm_classifier import (\n        ComponentCard,\n        format_system_prompt,\n        format_user_prompt,\n        EvidenceValidator,\n    )\n    \n    # Create client\n    client = OllamaClient(config)\n    \n    # Build component card if dict\n    if isinstance(component_card, dict):\n        card = ComponentCard(**component_card)\n    else:\n        card = component_card\n    \n    # Generate prompts\n    system_prompt = format_system_prompt()\n    user_prompt = format_user_prompt(card)\n    \n    # Call Ollama\n    response = client.classify(system_prompt, user_prompt)\n    \n    # Parse and validate\n    try:\n        result = json.loads(response)\n    except json.JSONDecodeError:\n        return {\n            \"node_id\": card.node_id,\n            \"role\": \"Unknown\",\n            \"confidence\": 0.1,\n            \"evidence\": [],\n            \"reasoning\": f\"Failed to parse LLM response: {response[:200]}\",\n        }\n    \n    # Validate evidence\n    validator = EvidenceValidator(strict=True)\n    validated = validator.validate(result, card)\n    \n    return {\n        \"node_id\": validated.node_id,\n        \"role\": validated.role,\n        \"confidence\": validated.confidence,\n        \"evidence\": [\n            {\n                \"type\": e.evidence_type,\n                \"quote\": e.quote,\n                \"file\": e.file,\n                \"line_start\": e.line_start,\n            }\n            for e in validated.evidence\n        ],\n        \"reasoning\": validated.reasoning,\n        \"validation_errors\": validator.validation_errors,\n    }",
      "complexity": 0,
      "lines_of_code": 64,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassMetrics",
      "name": "GodClassMetrics",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 18,
      "end_line": 18,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class GodClassMetrics:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite",
      "name": "GodClassDetectorLite",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 44,
      "end_line": 44,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class GodClassDetectorLite:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite.__init__",
      "name": "GodClassDetectorLite.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 47,
      "end_line": 108,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        # Touchpoint indicators for God Class detection\n        self.god_class_touchpoints = {\n            'coordination': [\n                r'\\b(manage|coordinate|orchestrate|control|supervise)\\b',\n                r'\\b(handle|process|execute|perform|run)\\b'\n            ],\n            'business_logic': [\n                r'\\b(calculate|compute|validate|verify|transform)\\b',\n                r'\\b(process|apply|enforce|implement)\\b'\n            ],\n            'data_access': [\n                r'\\b(save|load|find|delete|query|persist)\\b',\n                r'\\b(database|db|sql|repository|dao)\\b'\n            ],\n            'ui_interaction': [\n                r'\\b(render|display|show|present|view)\\b',\n                r'\\b(gui|ui|interface|component|widget)\\b'\n            ],\n            'infrastructure': [\n                r'\\b(network|http|api|rest|soap)\\b',\n                r'\\b(file|io|system|config|log)\\b'\n            ],\n            'validation': [\n                r'\\b(check|ensure|verify|validate)\\b',\n                r'\\b(assert|require|guard)\\b'\n            ]\n        }\n\n        # Language-specific patterns\n        self.language_patterns = {\n            'python': {\n                'class_pattern': r'^\\s*class\\s+(\\w+).*:',\n                'method_pattern': r'^\\s*def\\s+(\\w+)\\s*\\(',\n                'import_pattern': r'^\\s*(import|from)\\s+',\n                'comment_pattern': r'#.*$'\n            },\n            'java': {\n                'class_pattern': r'^\\s*(public\\s+)?(private\\s+)?(protected\\s+)?class\\s+(\\w+)',\n                'method_pattern': r'^\\s*(public\\s+)?(private\\s+)?(protected\\s+)?(static\\s+)?(\\w+)\\s*\\(',\n                'import_pattern': r'^\\s*import\\s+',\n                'comment_pattern': r'//.*$|/\\*.*?\\*/'\n            },\n            'javascript': {\n                'class_pattern': r'^\\s*(export\\s+)?(default\\s+)?class\\s+(\\w+)',\n                'method_pattern': r'^\\s*(async\\s+)?(\\w+)\\s*\\(',\n                'import_pattern': r'^\\s*(import\\s+.*from\\s+|const\\s+.*=\\s+require\\()',\n                'comment_pattern': r'//.*$|/\\*.*?\\*/'\n            },\n            'typescript': {\n                'class_pattern': r'^\\s*(export\\s+)?(abstract\\s+)?class\\s+(\\w+)',\n                'method_pattern': r'^\\s*(public\\s+)?(private\\s+)?(protected\\s+)?(async\\s+)?(\\w+)\\s*\\(',\n                'import_pattern': r'^\\s*import\\s+.*from\\s+',\n                'comment_pattern': r'//.*$|/\\*.*?\\*/'\n            },\n            'go': {\n                'class_pattern': r'^\\s*type\\s+(\\w+)\\s+struct\\s*{',\n                'method_pattern': r'^\\s*func\\s+\\([^)]*\\)\\s*(\\w+)\\s*\\(',\n                'import_pattern': r'^\\s*import\\s+',\n                'comment_pattern': r'//.*$'\n            }\n        }",
      "complexity": 0,
      "lines_of_code": 61,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite.analyze_repository",
      "name": "GodClassDetectorLite.analyze_repository",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 110,
      "end_line": 174,
      "role": "Repository",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze entire repository for God Classes",
      "signature": "def analyze_repository(self, repo_path: str) -> Dict[str, Any]:",
      "body_source": "    def analyze_repository(self, repo_path: str) -> Dict[str, Any]:\n        \"\"\"Analyze entire repository for God Classes\"\"\"\n        print(f\"\ud83d\udd0d Analyzing repository: {repo_path}\")\n\n        results = {\n            'metadata': {\n                'repository_path': repo_path,\n                'analysis_date': '2025-12-04',\n                'detector_version': 'V12.1-Lite'\n            },\n            'summary': {\n                'total_classes_analyzed': 0,\n                'god_classes_found': 0,\n                'antimatter_risk_classes': 0,\n                'languages_detected': set(),\n                'average_risk_score': 0.0\n            },\n            'god_classes': [],\n            'risk_distribution': {\n                'low_risk': 0,\n                'medium_risk': 0,\n                'high_risk': 0,\n                'critical_risk': 0\n            },\n            'recommendations': {}\n        }\n\n        # Find all source files\n        source_files = self._find_source_files(repo_path)\n        print(f\"\ud83d\udcc1 Found {len(source_files)} source files\")\n\n        # Analyze each file\n        for file_path in source_files:\n            language = self._detect_language(file_path)\n            if language:\n                results['summary']['languages_detected'].add(language)\n                god_classes = self._analyze_file(file_path, language)\n                results['god_classes'].extend(god_classes)\n\n        # Update summary statistics\n        results['summary']['total_classes_analyzed'] = len(results['god_classes'])\n        results['summary']['god_classes_found'] = len([gc for gc in results['god_classes'] if gc.is_god_class])\n        results['summary']['antimatter_risk_classes'] = len([gc for gc in results['god_classes'] if gc.antimatter_risk_score > 80])\n        results['summary']['languages_detected'] = list(results['summary']['languages_detected'])\n\n        if results['god_classes']:\n            avg_risk = sum(gc.antimatter_risk_score for gc in results['god_classes']) / len(results['god_classes'])\n            results['summary']['average_risk_score'] = avg_risk\n\n        # Categorize risk distribution\n        for gc in results['god_classes']:\n            if gc.antimatter_risk_score > 90:\n                results['risk_distribution']['critical_risk'] += 1\n            elif gc.antimatter_risk_score > 80:\n                results['risk_distribution']['high_risk'] += 1\n            elif gc.antimatter_risk_score > 60:\n                results['risk_distribution']['medium_risk'] += 1\n            else:\n                results['risk_distribution']['low_risk'] += 1\n\n        # Generate recommendations\n        results['recommendations'] = self._generate_recommendations(results['god_classes'])\n\n        print(f\"\ud83c\udfaf Analysis complete: {results['summary']['god_classes_found']} God Classes detected\")\n        return results",
      "complexity": 0,
      "lines_of_code": 64,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._find_source_files",
      "name": "GodClassDetectorLite._find_source_files",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 176,
      "end_line": 206,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find all relevant source files",
      "signature": "def _find_source_files(self, repo_path: str) -> List[str]:",
      "body_source": "    def _find_source_files(self, repo_path: str) -> List[str]:\n        \"\"\"Find all relevant source files\"\"\"\n        source_extensions = ['.py', '.java', '.ts', '.js', '.go', '.rs', '.kt', '.cs']\n        source_files = []\n\n        for root, dirs, files in os.walk(Path(repo_path)):\n            root_path = Path(root)\n            # Skip common directories\n            dirs[:] = [\n                d\n                for d in dirs\n                if d\n                not in [\n                    '.git',\n                    '__pycache__',\n                    'node_modules',\n                    'target',\n                    'build',\n                    'dist',\n                    'venv',\n                    '.venv',\n                    'coverage',\n                    '.next',\n                ]\n            ]\n\n            for file in files:\n                if any(file.endswith(ext) for ext in source_extensions):\n                    source_files.append(str(root_path / file))\n\n        return source_files",
      "complexity": 0,
      "lines_of_code": 30,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._detect_language",
      "name": "GodClassDetectorLite._detect_language",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 208,
      "end_line": 221,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect programming language from file extension",
      "signature": "def _detect_language(self, file_path: str) -> Optional[str]:",
      "body_source": "    def _detect_language(self, file_path: str) -> Optional[str]:\n        \"\"\"Detect programming language from file extension\"\"\"\n        ext = Path(file_path).suffix.lower()\n        language_map = {\n            '.py': 'python',\n            '.java': 'java',\n            '.ts': 'typescript',\n            '.js': 'javascript',\n            '.go': 'go',\n            '.rs': 'rust',\n            '.kt': 'kotlin',\n            '.cs': 'c_sharp'\n        }\n        return language_map.get(ext)",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._analyze_file",
      "name": "GodClassDetectorLite._analyze_file",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 223,
      "end_line": 277,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "List[GodClassMetrics]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a single file for God Classes",
      "signature": "def _analyze_file(self, file_path: str, language: str) -> List[GodClassMetrics]:",
      "body_source": "    def _analyze_file(self, file_path: str, language: str) -> List[GodClassMetrics]:\n        \"\"\"Analyze a single file for God Classes\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error reading {file_path}: {e}\")\n            return []\n\n        lines = content.split('\\n')\n        patterns = self.language_patterns.get(language, self.language_patterns['python'])\n\n        god_classes = []\n        current_class = None\n        class_start_line = 0\n        class_lines = []\n\n        for i, line in enumerate(lines, 1):\n            # Skip comments and empty lines\n            if not line.strip() or re.search(patterns['comment_pattern'], line):\n                continue\n\n            # Detect class definition\n            class_match = re.search(patterns['class_pattern'], line)\n            if class_match:\n                # Save previous class if exists\n                if current_class:\n                    metrics = self._analyze_class(\n                        current_class, '\\n'.join(class_lines),\n                        class_start_line, file_path, language\n                    )\n                    if metrics:\n                        god_classes.append(metrics)\n\n                # Start new class\n                class_name = class_match.groups()[-1]\n                current_class = class_name\n                class_start_line = i\n                class_lines = [line]\n                continue\n\n            # Add line to current class\n            if current_class:\n                class_lines.append(line)\n\n        # Don't forget the last class\n        if current_class:\n            metrics = self._analyze_class(\n                current_class, '\\n'.join(class_lines),\n                class_start_line, file_path, language\n            )\n            if metrics:\n                god_classes.append(metrics)\n\n        return god_classes",
      "complexity": 0,
      "lines_of_code": 54,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._analyze_class",
      "name": "GodClassDetectorLite._analyze_class",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 279,
      "end_line": 353,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "class_name",
          "type": "str"
        },
        {
          "name": "class_content",
          "type": "str"
        },
        {
          "name": "line_number",
          "type": "int"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "Optional[GodClassMetrics]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a single class for God Class characteristics",
      "signature": "def _analyze_class(self, class_name: str, class_content: str,",
      "body_source": "    def _analyze_class(self, class_name: str, class_content: str,\n                      line_number: int, file_path: str, language: str) -> Optional[GodClassMetrics]:\n        \"\"\"Analyze a single class for God Class characteristics\"\"\"\n        lines = class_content.split('\\n')\n        patterns = self.language_patterns.get(language, self.language_patterns['python'])\n\n        # Count methods\n        method_count = len(re.findall(patterns['method_pattern'], class_content, re.MULTILINE))\n\n        # Count imports/dependencies\n        dependency_count = len(re.findall(patterns['import_pattern'], class_content, re.MULTILINE))\n\n        # Count lines of code (excluding empty lines and comments)\n        loc_lines = [line for line in lines if line.strip() and not re.search(patterns['comment_pattern'], line)]\n        lines_of_code = len(loc_lines)\n\n        # Analyze touchpoints\n        touchpoint_counts = defaultdict(int)\n        class_content_lower = class_content.lower()\n\n        for touchpoint, patterns_list in self.god_class_touchpoints.items():\n            for pattern in patterns_list:\n                matches = re.findall(pattern, class_content_lower)\n                touchpoint_counts[touchpoint] += len(matches)\n\n        # Calculate responsibility overload\n        responsibility_count = sum(1 for count in touchpoint_counts.values() if count > 0)\n\n        # Calculate touchpoint overload score\n        total_touchpoints = sum(touchpoint_counts.values())\n        max_expected_touchpoints = max(10, method_count * 2)  # Reasonable baseline\n        touchpoint_overload = min(100, (total_touchpoints / max_expected_touchpoints) * 100)\n\n        # Calculate antimatter risk score\n        risk_factors = {\n            'size_overload': min(30, (lines_of_code / 200) * 10),  # 30 points max\n            'method_overload': min(25, (method_count / 20) * 10),  # 25 points max\n            'responsibility_overload': min(25, responsibility_count * 5),  # 25 points max\n            'touchpoint_overload': min(20, touchpoint_overload)  # 20 points max\n        }\n\n        antimatter_risk_score = sum(risk_factors.values())\n\n        # Determine if it's a God Class\n        is_god_class = (\n            lines_of_code > 200 and\n            method_count > 10 and\n            responsibility_count >= 3 and\n            antimatter_risk_score > 70\n        )\n\n        # Generate refactoring suggestions\n        suggested_refactors = self._generate_refactor_suggestions(\n            touchpoint_counts, method_count, lines_of_code\n        )\n\n        return GodClassMetrics(\n            class_name=class_name,\n            file_path=file_path,\n            line_number=line_number,\n            language=language,\n            lines_of_code=lines_of_code,\n            method_count=method_count,\n            responsibility_count=responsibility_count,\n            dependency_count=dependency_count,\n            touchpoint_overload=touchpoint_overload,\n            coordination_touchpoints=touchpoint_counts['coordination'],\n            business_logic_touchpoints=touchpoint_counts['business_logic'],\n            data_access_touchpoints=touchpoint_counts['data_access'],\n            ui_touchpoints=touchpoint_counts['ui_interaction'],\n            infrastructure_touchpoints=touchpoint_counts['infrastructure'],\n            antimatter_risk_score=antimatter_risk_score,\n            is_god_class=is_god_class,\n            suggested_refactors=suggested_refactors\n        )",
      "complexity": 0,
      "lines_of_code": 74,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._generate_refactor_suggestions",
      "name": "GodClassDetectorLite._generate_refactor_suggestions",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 355,
      "end_line": 384,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "touchpoint_counts",
          "type": "Dict"
        },
        {
          "name": "method_count",
          "type": "int"
        },
        {
          "name": "loc",
          "type": "int"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate refactoring suggestions based on touchpoint analysis",
      "signature": "def _generate_refactor_suggestions(self, touchpoint_counts: Dict, method_count: int, loc: int) -> List[str]:",
      "body_source": "    def _generate_refactor_suggestions(self, touchpoint_counts: Dict, method_count: int, loc: int) -> List[str]:\n        \"\"\"Generate refactoring suggestions based on touchpoint analysis\"\"\"\n        suggestions = []\n\n        if touchpoint_counts['data_access'] > 3:\n            suggestions.append(\"Extract Repository pattern for data access\")\n\n        if touchpoint_counts['business_logic'] > 5:\n            suggestions.append(\"Extract Domain Services for business logic\")\n\n        if touchpoint_counts['ui_interaction'] > 2:\n            suggestions.append(\"Separate UI Controller from business logic\")\n\n        if touchpoint_counts['coordination'] > 4:\n            suggestions.append(\"Extract Service Coordinator\")\n\n        if touchpoint_counts['infrastructure'] > 3:\n            suggestions.append(\"Extract Infrastructure Layer\")\n\n        if method_count > 20:\n            suggestions.append(f\"Split into {math.ceil(method_count / 10)} focused classes\")\n\n        if loc > 300:\n            suggestions.append(\"Consider Composite pattern for large class\")\n\n        # Domain-specific suggestions\n        if touchpoint_counts['data_access'] > 0 and touchpoint_counts['business_logic'] > 0:\n            suggestions.append(\"Apply DDD: Entity + Repository + Domain Service\")\n\n        return list(set(suggestions))  # Remove duplicates",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite._generate_recommendations",
      "name": "GodClassDetectorLite._generate_recommendations",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 386,
      "end_line": 435,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "god_classes",
          "type": "List[GodClassMetrics]"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate high-level recommendations",
      "signature": "def _generate_recommendations(self, god_classes: List[GodClassMetrics]) -> Dict[str, Any]:",
      "body_source": "    def _generate_recommendations(self, god_classes: List[GodClassMetrics]) -> Dict[str, Any]:\n        \"\"\"Generate high-level recommendations\"\"\"\n        recommendations = {\n            'priority_actions': [],\n            'architectural_patterns': [],\n            'risk_mitigation': [],\n            'refactoring_strategy': []\n        }\n\n        # Count high-risk classes\n        critical_classes = [gc for gc in god_classes if gc.antimatter_risk_score > 90]\n        high_risk_classes = [gc for gc in god_classes if 80 < gc.antimatter_risk_score <= 90]\n\n        if critical_classes:\n            recommendations['priority_actions'].append(\n                f\"IMMEDIATE: Refactor {len(critical_classes)} critical God Classes (>90% risk)\"\n            )\n\n        if high_risk_classes:\n            recommendations['priority_actions'].append(\n                f\"HIGH: Address {len(high_risk_classes)} high-risk God Classes (80-90% risk)\"\n            )\n\n        # Common patterns to apply\n        all_refactors = [refactor for gc in god_classes for refactor in gc.suggested_refactors]\n        refactor_counts = defaultdict(int)\n        for refactor in all_refactors:\n            refactor_counts[refactor] += 1\n\n        # Top recommendations\n        top_refactors = sorted(refactor_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n        recommendations['architectural_patterns'] = [refactor for refactor, count in top_refactors]\n\n        # Risk mitigation\n        recommendations['risk_mitigation'] = [\n            \"Implement code review thresholds for class size (>200 LOC)\",\n            \"Automated detection in CI/CD pipeline\",\n            \"Architectural fitness functions for single responsibility\",\n            \"Regular refactoring sprints for identified God Classes\"\n        ]\n\n        # Strategy\n        recommendations['refactoring_strategy'] = [\n            \"Start with highest risk classes first\",\n            \"Extract one responsibility at a time\",\n            \"Maintain test coverage during refactoring\",\n            \"Apply Strangler Fig pattern for gradual extraction\"\n        ]\n\n        return recommendations",
      "complexity": 0,
      "lines_of_code": 49,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py:GodClassDetectorLite.generate_ascii_visualization",
      "name": "GodClassDetectorLite.generate_ascii_visualization",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/god_class_detector_lite.py",
      "start_line": 437,
      "end_line": 464,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "god_classes",
          "type": "List[GodClassMetrics]"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate ASCII visualization of God Classes",
      "signature": "def generate_ascii_visualization(self, god_classes: List[GodClassMetrics]):",
      "body_source": "    def generate_ascii_visualization(self, god_classes: List[GodClassMetrics]):\n        \"\"\"Generate ASCII visualization of God Classes\"\"\"\n        print(f\"\\n\ud83d\udcca ASCII VISUALIZATION OF GOD CLASSES\")\n        print(\"=\"*100)\n\n        if not god_classes:\n            print(\"No God Classes detected! \ud83c\udf89\")\n            return\n\n        # Sort by risk score\n        sorted_classes = sorted(god_classes, key=lambda x: x.antimatter_risk_score, reverse=True)\n\n        for i, gc in enumerate(sorted_classes[:20], 1):  # Show top 20\n            # Create risk bar\n            bar_length = int(gc.antimatter_risk_score / 2)\n            if gc.antimatter_risk_score > 90:\n                bar = \"\ud83d\udd34\" * bar_length\n            elif gc.antimatter_risk_score > 80:\n                bar = \"\ud83d\udfe0\" * bar_length\n            elif gc.antimatter_risk_score > 60:\n                bar = \"\ud83d\udfe1\" * bar_length\n            else:\n                bar = \"\ud83d\udfe2\" * bar_length\n\n            print(f\"{i:2d}. {gc.class_name[:30]:<30} | {gc.language:<10} | \"\n                  f\"{gc.antimatter_risk_score:5.1f}% | {bar}\")\n\n        print(\"\\nLegend: \ud83d\udd34 Critical (>90%) | \ud83d\udfe0 High (80-90%) | \ud83d\udfe1 Medium (60-80%) | \ud83d\udfe2 Low (<60%)\")",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:UnifiedNode",
      "name": "UnifiedNode",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 18,
      "end_line": 18,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class UnifiedNode:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:UnifiedEdge",
      "name": "UnifiedEdge",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 61,
      "end_line": 61,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class UnifiedEdge:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:UnifiedAnalysisOutput",
      "name": "UnifiedAnalysisOutput",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 78,
      "end_line": 78,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class UnifiedAnalysisOutput:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:UnifiedAnalysisOutput.to_dict",
      "name": "UnifiedAnalysisOutput.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 166,
      "end_line": 168,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convert to dict for JSON serialization.",
      "signature": "def to_dict(self) -> Dict:",
      "body_source": "    def to_dict(self) -> Dict:\n        \"\"\"Convert to dict for JSON serialization.\"\"\"\n        return asdict(self)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:UnifiedAnalysisOutput.save",
      "name": "UnifiedAnalysisOutput.save",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 170,
      "end_line": 173,
      "role": "Command",
      "role_confidence": 82.0,
      "discovery_method": "structural:docstring_persist",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "output_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Save to JSON file.",
      "signature": "def save(self, output_path: str):",
      "body_source": "    def save(self, output_path: str):\n        \"\"\"Save to JSON file.\"\"\"\n        with open(output_path, 'w') as f:\n            json.dump(self.to_dict(), f, indent=2, default=str)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:create_unified_output",
      "name": "create_unified_output",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 176,
      "end_line": 293,
      "role": "Factory",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "target_path",
          "type": "str"
        },
        {
          "name": "nodes",
          "type": "List[Dict]"
        },
        {
          "name": "edges",
          "type": "List[Dict]"
        },
        {
          "name": "stats",
          "type": "Dict"
        },
        {
          "name": "auto_discovery_report",
          "type": "Dict",
          "default": "None"
        },
        {
          "name": "analysis_time_ms",
          "type": "int",
          "default": "0"
        }
      ],
      "return_type": "UnifiedAnalysisOutput",
      "base_classes": [],
      "decorators": [],
      "docstring": "Create a unified output from analysis results.\nEnsures consistent schema regardless of what analysis was performed.",
      "signature": "def create_unified_output(",
      "body_source": "def create_unified_output(\n    target_path: str,\n    nodes: List[Dict],\n    edges: List[Dict],\n    stats: Dict,\n    auto_discovery_report: Dict = None,\n    analysis_time_ms: int = 0,\n) -> UnifiedAnalysisOutput:\n    \"\"\"\n    Create a unified output from analysis results.\n    Ensures consistent schema regardless of what analysis was performed.\n    \"\"\"\n    \n    output = UnifiedAnalysisOutput(\n        generated_at=datetime.now().isoformat(),\n        analysis_time_ms=analysis_time_ms,\n        target_path=str(target_path),\n        target_name=Path(target_path).name,\n    )\n    \n    # Populate nodes (normalize to schema)\n    for node in nodes:\n        # Resolve ID: Use existing or generate formatted ID\n        node_id = node.get(\"id\")\n        if not node_id:\n            file = node.get(\"file_path\", node.get(\"file\", \"unknown\"))\n            name = node.get(\"name\", \"unknown\")\n            node_id = f\"{file}:{name}\"\n\n        unified_node = {\n            \"id\": node_id,\n            \"name\": node.get(\"name\", \"\"),\n            \"kind\": node.get(\"symbol_kind\", node.get(\"kind\", \"unknown\")),\n            \"file_path\": node.get(\"file_path\", node.get(\"file\", \"\")),\n            \"start_line\": node.get(\"line\", node.get(\"start_line\", 0)),\n            \"end_line\": node.get(\"end_line\", node.get(\"line\", 0)),\n            \"role\": node.get(\"type\", node.get(\"role\", \"Unknown\")),\n            \"role_confidence\": node.get(\"confidence\", node.get(\"role_confidence\", 0.0)),\n            \"discovery_method\": node.get(\"discovery_method\", \"pattern\"),\n            \"params\": node.get(\"params\", []),\n            \"return_type\": node.get(\"return_type\", \"\"),\n            \"base_classes\": node.get(\"base_classes\", []),\n            \"decorators\": node.get(\"decorators\", []),\n            \"docstring\": node.get(\"docstring\", \"\"),\n            \"signature\": node.get(\"evidence\", node.get(\"signature\", \"\")),\n            \"body_source\": node.get(\"body_source\", \"\"),\n            \"complexity\": node.get(\"complexity\", 0),\n            \"lines_of_code\": (node.get(\"end_line\", 0) - node.get(\"line\", 0)) or 0,\n            \"in_degree\": node.get(\"in_degree\", 0),\n            \"out_degree\": node.get(\"out_degree\", 0),\n            \"layer\": node.get(\"layer\"),\n            \"metadata\": node.get(\"metadata\", {}),\n        }\n        output.nodes.append(unified_node)\n    \n    # Populate edges\n    for edge in edges:\n        unified_edge = {\n            \"source\": edge.get(\"source\", \"\"),\n            \"target\": edge.get(\"target\", \"\"),\n            \"edge_type\": edge.get(\"edge_type\", \"unknown\"),\n            \"weight\": edge.get(\"weight\", 1.0),\n            \"confidence\": edge.get(\"confidence\", 1.0),\n            \"file_path\": edge.get(\"file\", \"\"),\n            \"line\": edge.get(\"line\", 0),\n            \"metadata\": edge.get(\"metadata\", {}),\n        }\n        output.edges.append(unified_edge)\n    \n    # Populate stats\n    output.stats = {\n        \"total_files\": stats.get(\"files_analyzed\", stats.get(\"total_files\", 0)),\n        \"total_lines\": stats.get(\"total_lines_analyzed\", stats.get(\"total_lines\", 0)),\n        \"total_nodes\": len(output.nodes),\n        \"total_edges\": len(output.edges),\n        \"languages\": list(stats.get(\"languages_detected\", stats.get(\"languages\", []))),\n        \"coverage_percentage\": stats.get(\"recognized_percentage\", stats.get(\"coverage_percentage\", 0.0)),\n        \"unknown_percentage\": 100.0 - stats.get(\"recognized_percentage\", stats.get(\"coverage_percentage\", 0.0)),\n    }\n    \n    # Classification breakdown\n    role_counts = {}\n    kind_counts = {}\n    confidence_dist = {\"high\": 0, \"medium\": 0, \"low\": 0}\n    \n    for node in output.nodes:\n        role = node.get(\"role\", \"Unknown\")\n        kind = node.get(\"kind\", \"unknown\")\n        conf = node.get(\"role_confidence\", 0)\n        \n        role_counts[role] = role_counts.get(role, 0) + 1\n        kind_counts[kind] = kind_counts.get(kind, 0) + 1\n        \n        if conf >= 80:\n            confidence_dist[\"high\"] += 1\n        elif conf >= 50:\n            confidence_dist[\"medium\"] += 1\n        else:\n            confidence_dist[\"low\"] += 1\n    \n    output.classification = {\n        \"by_role\": role_counts,\n        \"by_kind\": kind_counts,\n        \"by_layer\": {},  # TODO: populate from layer analysis\n        \"by_confidence\": confidence_dist,\n    }\n    \n    # Auto-discovery report\n    if auto_discovery_report:\n        output.auto_discovery = {\n            \"enabled\": True,\n            \"patterns_applied\": auto_discovery_report.get(\"total_classified\", 0),\n            \"particles_reclassified\": auto_discovery_report.get(\"particles_updated\", 0),\n            \"top_patterns\": auto_discovery_report.get(\"top_patterns\", []),\n            \"suggested_new_patterns\": auto_discovery_report.get(\"suggested_new_patterns\", []),\n        }\n    \n    return output",
      "complexity": 0,
      "lines_of_code": 117,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:analyze",
      "name": "analyze",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 298,
      "end_line": 481,
      "role": "Query",
      "role_confidence": 88.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "target_path",
          "type": "str"
        },
        {
          "name": "output_dir",
          "type": "str",
          "default": "None"
        },
        {
          "name": "**options"
        }
      ],
      "return_type": "UnifiedAnalysisOutput",
      "base_classes": [],
      "decorators": [],
      "docstring": "\ud83c\udfaf SINGLE ENTRY POINT for all Spectrometer analysis.\n\nPIPELINE ORDER:\n1. AST Parse \u2192 Raw particles\n2. RPBL Classification \u2192 Classified particles\n3. Auto Pattern Discovery \u2192 Reduced unknowns\n4. Edge Extraction \u2192 Call graph\n5. Graph Inference \u2192 Infer remaining unknowns from structure\n6. Output \u2192 Unified schema\n\nArgs:\n    target_path: Path to file, directory, or repository to analyze\n    output_dir: Optional output directory\n    **options: llm, llm_model, language\n\nReturns:\n    UnifiedAnalysisOutput with consistent schema",
      "signature": "def analyze(target_path: str, output_dir: str = None, **options) -> UnifiedAnalysisOutput:",
      "body_source": "def analyze(target_path: str, output_dir: str = None, **options) -> UnifiedAnalysisOutput:\n    \"\"\"\n    \ud83c\udfaf SINGLE ENTRY POINT for all Spectrometer analysis.\n    \n    PIPELINE ORDER:\n    1. AST Parse \u2192 Raw particles\n    2. RPBL Classification \u2192 Classified particles\n    3. Auto Pattern Discovery \u2192 Reduced unknowns\n    4. Edge Extraction \u2192 Call graph\n    5. Graph Inference \u2192 Infer remaining unknowns from structure\n    6. Output \u2192 Unified schema\n    \n    Args:\n        target_path: Path to file, directory, or repository to analyze\n        output_dir: Optional output directory\n        **options: llm, llm_model, language\n    \n    Returns:\n        UnifiedAnalysisOutput with consistent schema\n    \"\"\"\n    from tree_sitter_engine import TreeSitterUniversalEngine\n    from stats_generator import StatsGenerator\n    from particle_classifier import ParticleClassifier\n    \n    start_time = time.time()\n    target = Path(target_path).resolve()\n    \n    print(f\"\ud83d\udd2c SPECTROMETER UNIFIED ANALYSIS\")\n    print(f\"   Target: {target}\")\n    print(f\"=\" * 60)\n    \n    # =========================================================================\n    # STAGE 1: AST PARSE \u2192 Raw Particles\n    # =========================================================================\n    print(\"\\n\ud83d\udcc2 Stage 1: AST Parsing...\")\n    ts_engine = TreeSitterUniversalEngine()\n    \n    if target.is_file():\n        results = [ts_engine.analyze_file(str(target))]\n    else:\n        results = ts_engine.analyze_directory(str(target))\n    \n    raw_particle_count = sum(len(r.get('particles', [])) for r in results)\n    print(f\"   \u2192 {raw_particle_count} particles extracted\")\n    \n    # =========================================================================\n    # STAGE 2: RPBL CLASSIFICATION\n    # =========================================================================\n    print(\"\\n\ud83c\udff7\ufe0f  Stage 2: RPBL Classification...\")\n    classifier = ParticleClassifier()\n    \n    for result in results:\n        classified = []\n        for particle in result.get('particles', []):\n            classified.append(classifier.classify_particle(particle))\n        result['particles'] = classified\n    \n    # =========================================================================\n    # STAGE 3: AUTO PATTERN DISCOVERY\n    # =========================================================================\n    print(\"\\n\ud83d\udd0d Stage 3: Auto Pattern Discovery...\")\n    stats_gen = StatsGenerator()\n    comprehensive = stats_gen.generate_comprehensive_stats(results)\n    \n    particles = comprehensive.get('particles', [])\n    auto_discovery = comprehensive.get('auto_discovery', {})\n    \n    # =========================================================================\n    # STAGE 3.5: LLM ENRICHMENT (Optional)\n    # =========================================================================\n    if options.get('llm'):\n        print(\"\\n\ud83e\udd16 Stage 3.5: LLM Enrichment...\")\n        try:\n            from llm_classifier import LLMClassifier\n            model_name = options.get('llm_model', 'qwen2.5:7b-instruct')\n            print(f\"   \u2192 Using model: {model_name}\")\n            \n            enricher = LLMClassifier(model_name=model_name)\n            \n            # Identify candidates for enrichment (Unknowns or Low Confidence)\n            candidates = [p for p in particles if p.get('type') == 'Unknown' or p.get('confidence', 0) < 0.5]\n            print(f\"   \u2192 Refining {len(candidates)} low-confidence particles...\")\n            \n            refined_count = 0\n            for p in candidates:\n                # Basic context construction\n                context = f\"Name: {p.get('name')}\\nFile: {p.get('file_path')}\\nBody: {p.get('body_source', '')[:200]}\"\n                \n                new_role, confidence = enricher.classify_with_llm(context)\n                \n                if new_role and new_role != \"Unknown\":\n                    p['type'] = new_role\n                    p['confidence'] = confidence\n                    p['discovery_method'] = 'llm'\n                    refined_count += 1\n            \n            print(f\"   \u2192 {refined_count} particles refined by LLM\")\n            \n        except ImportError:\n            print(\"   \u26a0\ufe0f  LLM Classifier not found or dependencies missing\")\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f  LLM Enrichment failed: {e}\")\n            \n    # =========================================================================\n    # STAGE 4: EDGE EXTRACTION \u2192 Call Graph\n    # =========================================================================\n    print(\"\\n\ud83d\udd17 Stage 4: Edge Extraction...\")\n    edges = extract_call_edges(particles, results)\n    print(f\"   \u2192 {len(edges)} edges extracted\")\n    \n    # =========================================================================\n    # STAGE 5: GRAPH INFERENCE \u2192 Infer unknowns from structure\n    # =========================================================================\n    print(\"\\n\ud83e\udde0 Stage 5: Graph-Based Type Inference...\")\n    graph_inference_report = {\"total_inferred\": 0, \"analysis_status\": \"not_applied\"}\n    \n    if edges:\n        try:\n            from graph_type_inference import apply_graph_inference\n            particles, graph_inference_report = apply_graph_inference(particles, edges)\n            print(f\"   \u2192 {graph_inference_report.get('total_inferred', 0)} types inferred from graph\")\n        except ImportError as e:\n            print(f\"   \u26a0\ufe0f  Graph inference not available: {e}\")\n    else:\n        print(\"   \u26a0\ufe0f  No edges - skipping graph inference\")\n    \n    # =========================================================================\n    # STAGE 6: OUTPUT \u2192 Unified Schema\n    # =========================================================================\n    print(\"\\n\ud83d\udcca Stage 6: Building Unified Output...\")\n    \n    summary = comprehensive.get('summary', {})\n    detailed = comprehensive.get('detailed_stats', {})\n    \n    stats = {\n        **summary,\n        **detailed.get('file_analysis', {}),\n        'languages_detected': detailed.get('language_analysis', {}).get('languages_detected', []),\n    }\n    \n    # Recalculate coverage after inference\n    unknown_count = sum(1 for p in particles if p.get('type') == 'Unknown')\n    total_count = len(particles)\n    final_coverage = ((total_count - unknown_count) / total_count * 100) if total_count else 0\n    stats['recognized_percentage'] = final_coverage\n    \n    analysis_time_ms = int((time.time() - start_time) * 1000)\n    \n    output = create_unified_output(\n        target_path=str(target),\n        nodes=particles,\n        edges=edges,\n        stats=stats,\n        auto_discovery_report=auto_discovery,\n        analysis_time_ms=analysis_time_ms,\n    )\n    \n    # Add graph inference to output\n    output.architecture['graph_inference'] = graph_inference_report\n    if graph_inference_report.get('total_inferred', 0) > 0:\n        output.architecture['analysis_status'] = \"applied\"\n    \n    # Save output\n    if output_dir:\n        out_path = Path(output_dir)\n    else:\n        out_path = target / \"spectrometer_output\" if target.is_dir() else target.parent / \"spectrometer_output\"\n    \n    out_path.mkdir(parents=True, exist_ok=True)\n    output_file = out_path / \"unified_analysis.json\"\n    output.save(str(output_file))\n    \n    # =========================================================================\n    # SUMMARY\n    # =========================================================================\n    print(f\"\\n{'=' * 60}\")\n    print(f\"\u2705 ANALYSIS COMPLETE\")\n    print(f\"   Nodes: {output.stats['total_nodes']}\")\n    print(f\"   Edges: {output.stats['total_edges']}\")\n    print(f\"   Coverage: {output.stats['coverage_percentage']:.1f}%\")\n    print(f\"   Time: {analysis_time_ms}ms\")\n    print(f\"   Output: {output_file}\")\n    \n    return output",
      "complexity": 0,
      "lines_of_code": 183,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py:extract_call_edges",
      "name": "extract_call_edges",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/unified_analysis.py",
      "start_line": 484,
      "end_line": 564,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "particles",
          "type": "List[Dict]"
        },
        {
          "name": "results",
          "type": "List[Dict]"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract call relationships from particles and raw imports.\nCreates edges: {source, target, edge_type, file_path, line}",
      "signature": "def extract_call_edges(particles: List[Dict], results: List[Dict]) -> List[Dict]:",
      "body_source": "def extract_call_edges(particles: List[Dict], results: List[Dict]) -> List[Dict]:\n    \"\"\"\n    Extract call relationships from particles and raw imports.\n    Creates edges: {source, target, edge_type, file_path, line}\n    \"\"\"\n    edges = []\n    \n    # Build particle lookup by name\n    particle_by_name = {}\n    for p in particles:\n        name = p.get('name', '')\n        if name:\n            particle_by_name[name] = p\n            # Also register short name\n            short = name.split('.')[-1] if '.' in name else name\n            if short not in particle_by_name:\n                particle_by_name[short] = p\n    \n    # Extract imports from each file\n    for result in results:\n        file_path = result.get('file_path', '')\n        raw_imports = result.get('raw_imports', [])\n        \n        # Get file's particles\n        file_particles = [p for p in particles if p.get('file_path') == file_path]\n        \n        for imp in raw_imports:\n            # Create import edge - ensure target is always a string\n            source_module = Path(file_path).stem if file_path else 'unknown'\n            \n            if isinstance(imp, dict):\n                target_module = imp.get('module', '')\n                if isinstance(target_module, dict):\n                    target_module = target_module.get('name', str(target_module))\n                line = imp.get('line', 0)\n            else:\n                target_module = str(imp)\n                line = 0\n            \n            if target_module:  # Only add if we have a valid target\n                edges.append({\n                    'source': source_module,\n                    'target': str(target_module),  # Ensure string\n                    'edge_type': 'imports',\n                    'file_path': file_path,\n                    'line': line,\n                    'confidence': 1.0,\n                })\n    \n    # Extract containment edges (parent-child)\n    for p in particles:\n        parent = p.get('parent', '')\n        if parent:\n            edges.append({\n                'source': parent,\n                'target': p.get('name', ''),\n                'edge_type': 'contains',\n                'file_path': p.get('file_path', ''),\n                'line': p.get('line', 0),\n                'confidence': 1.0,\n            })\n    \n    # Extract call edges from body_source (heuristic)\n    for p in particles:\n        body = p.get('body_source', '')\n        if body:\n            # Look for function calls in body\n            import re\n            calls = re.findall(r'(?:self\\.)?(\\w+)\\s*\\(', body)\n            for call in calls:\n                if call in particle_by_name and call != p.get('name', '').split('.')[-1]:\n                    edges.append({\n                        'source': p.get('name', ''),\n                        'target': call,\n                        'edge_type': 'calls',\n                        'file_path': p.get('file_path', ''),\n                        'line': p.get('line', 0),\n                        'confidence': 0.7,  # Heuristic detection\n                    })\n    \n    return edges",
      "complexity": 0,
      "lines_of_code": 80,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:HadronLevel",
      "name": "HadronLevel",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 19,
      "end_line": 19,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class HadronLevel(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:Hadron",
      "name": "Hadron",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 27,
      "end_line": 27,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Hadron:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor",
      "name": "AtomExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 188,
      "end_line": 188,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class AtomExtractor:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.__init__",
      "name": "AtomExtractor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 197,
      "end_line": 199,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.parsers: Dict[str, any] = {}\n        self._init_parsers()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._init_parsers",
      "name": "AtomExtractor._init_parsers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 201,
      "end_line": 247,
      "role": "Query",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Initialize tree-sitter parsers for supported languages.",
      "signature": "def _init_parsers(self):",
      "body_source": "    def _init_parsers(self):\n        \"\"\"Initialize tree-sitter parsers for supported languages.\"\"\"\n        try:\n            from tree_sitter import Language, Parser\n            \n            # Python\n            try:\n                import tree_sitter_python as tspython\n                parser = Parser(Language(tspython.language()))\n                self.parsers[\"python\"] = parser\n            except ImportError:\n                pass\n            \n            # TypeScript\n            try:\n                import tree_sitter_typescript as tstypescript\n                parser = Parser(Language(tstypescript.language_typescript()))\n                self.parsers[\"typescript\"] = parser\n            except ImportError:\n                pass\n            \n            # JavaScript\n            try:\n                import tree_sitter_javascript as tsjavascript\n                parser = Parser(Language(tsjavascript.language()))\n                self.parsers[\"javascript\"] = parser\n            except ImportError:\n                pass\n            \n            # Go\n            try:\n                import tree_sitter_go as tsgo\n                parser = Parser(Language(tsgo.language()))\n                self.parsers[\"go\"] = parser\n            except ImportError:\n                pass\n                \n            # Java\n            try:\n                import tree_sitter_java as tsjava\n                parser = Parser(Language(tsjava.language()))\n                self.parsers[\"java\"] = parser\n            except ImportError:\n                pass\n                \n        except ImportError:\n            print(\"Warning: tree-sitter not installed. Run: pip install tree-sitter\")",
      "complexity": 0,
      "lines_of_code": 46,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.extract",
      "name": "AtomExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 249,
      "end_line": 281,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "code",
          "type": "bytes"
        },
        {
          "name": "language",
          "type": "str",
          "default": "'python'"
        },
        {
          "name": "file_path",
          "type": "str",
          "default": "''"
        }
      ],
      "return_type": "List[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract all hadrons (atoms, molecules, organelles) from source code.\n\nArgs:\n    code: Source code as bytes\n    language: Programming language (python, typescript, go, java)\n    file_path: Path to source file (for reporting)\n    \nReturns:\n    List of detected Hadron objects",
      "signature": "def extract(self, code: bytes, language: str = \"python\", file_path: str = \"\") -> List[Hadron]:",
      "body_source": "    def extract(self, code: bytes, language: str = \"python\", file_path: str = \"\") -> List[Hadron]:\n        \"\"\"\n        Extract all hadrons (atoms, molecules, organelles) from source code.\n        \n        Args:\n            code: Source code as bytes\n            language: Programming language (python, typescript, go, java)\n            file_path: Path to source file (for reporting)\n            \n        Returns:\n            List of detected Hadron objects\n        \"\"\"\n        if language not in self.parsers:\n            raise ValueError(f\"Unsupported language: {language}. Available: {list(self.parsers.keys())}\")\n        \n        parser = self.parsers[language]\n        tree = parser.parse(code)\n        \n        hadrons: List[Hadron] = []\n        \n        # Phase 1: Extract ATOMS (leaf nodes)\n        atoms = self._extract_atoms(tree.root_node, file_path)\n        hadrons.extend(atoms)\n        \n        # Phase 2: Compose MOLECULES (compound patterns)\n        molecules = self._extract_molecules(tree.root_node, file_path, atoms)\n        hadrons.extend(molecules)\n        \n        # Phase 3: Infer ORGANELLES (architecture roles)\n        organelles = self._infer_organelles(tree.root_node, file_path, atoms, molecules)\n        hadrons.extend(organelles)\n        \n        return hadrons",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._extract_atoms",
      "name": "AtomExtractor._extract_atoms",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 283,
      "end_line": 308,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "root_node"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "List[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract atomic syntax elements from AST.",
      "signature": "def _extract_atoms(self, root_node, file_path: str) -> List[Hadron]:",
      "body_source": "    def _extract_atoms(self, root_node, file_path: str) -> List[Hadron]:\n        \"\"\"Extract atomic syntax elements from AST.\"\"\"\n        atoms = []\n        \n        def visit(node):\n            if node.type in ATOM_MAP:\n                mapping = ATOM_MAP[node.type]\n                hadron = Hadron(\n                    id=mapping[\"id\"],\n                    name=mapping[\"name\"],\n                    level=HadronLevel.ATOM,\n                    continent=mapping[\"continent\"],\n                    fundamental=mapping[\"fundamental\"],\n                    file_path=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    text_snippet=node.text.decode()[:100] if node.text else \"\",\n                    detection_rule=f\"node_type={node.type}\",\n                )\n                atoms.append(hadron)\n            \n            for child in node.children:\n                visit(child)\n        \n        visit(root_node)\n        return atoms",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.visit",
      "name": "AtomExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 287,
      "end_line": 305,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            if node.type in ATOM_MAP:\n                mapping = ATOM_MAP[node.type]\n                hadron = Hadron(\n                    id=mapping[\"id\"],\n                    name=mapping[\"name\"],\n                    level=HadronLevel.ATOM,\n                    continent=mapping[\"continent\"],\n                    fundamental=mapping[\"fundamental\"],\n                    file_path=file_path,\n                    start_line=node.start_point[0] + 1,\n                    end_line=node.end_point[0] + 1,\n                    text_snippet=node.text.decode()[:100] if node.text else \"\",\n                    detection_rule=f\"node_type={node.type}\",\n                )\n                atoms.append(hadron)\n            \n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._extract_molecules",
      "name": "AtomExtractor._extract_molecules",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 310,
      "end_line": 331,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "root_node"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "atoms",
          "type": "List[Hadron]"
        }
      ],
      "return_type": "List[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract molecular patterns (classes, functions with context).",
      "signature": "def _extract_molecules(self, root_node, file_path: str, atoms: List[Hadron]) -> List[Hadron]:",
      "body_source": "    def _extract_molecules(self, root_node, file_path: str, atoms: List[Hadron]) -> List[Hadron]:\n        \"\"\"Extract molecular patterns (classes, functions with context).\"\"\"\n        molecules = []\n        \n        def visit(node):\n            # Detect classes\n            if node.type == \"class_definition\":\n                molecule = self._classify_class(node, file_path)\n                if molecule:\n                    molecules.append(molecule)\n            \n            # Detect functions with semantic analysis\n            if node.type == \"function_definition\":\n                molecule = self._classify_function(node, file_path)\n                if molecule:\n                    molecules.append(molecule)\n            \n            for child in node.children:\n                visit(child)\n        \n        visit(root_node)\n        return molecules",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.visit",
      "name": "AtomExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 314,
      "end_line": 328,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            # Detect classes\n            if node.type == \"class_definition\":\n                molecule = self._classify_class(node, file_path)\n                if molecule:\n                    molecules.append(molecule)\n            \n            # Detect functions with semantic analysis\n            if node.type == \"function_definition\":\n                molecule = self._classify_function(node, file_path)\n                if molecule:\n                    molecules.append(molecule)\n            \n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._classify_class",
      "name": "AtomExtractor._classify_class",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 333,
      "end_line": 396,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Optional[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a class node into the appropriate molecule/organelle.",
      "signature": "def _classify_class(self, node, file_path: str) -> Optional[Hadron]:",
      "body_source": "    def _classify_class(self, node, file_path: str) -> Optional[Hadron]:\n        \"\"\"Classify a class node into the appropriate molecule/organelle.\"\"\"\n        class_name = \"\"\n        has_id_field = False\n        has_save_method = False\n        has_find_method = False\n        is_immutable = True  # Assume immutable until proven otherwise\n        \n        for child in node.children:\n            if child.type == \"identifier\":\n                class_name = child.text.decode() if child.text else \"\"\n            elif child.type == \"block\":\n                for stmt in child.children:\n                    # Check for id field\n                    if \"id\" in (stmt.text.decode() if stmt.text else \"\").lower():\n                        if \"self.id\" in (stmt.text.decode() if stmt.text else \"\") or \"this.id\" in (stmt.text.decode() if stmt.text else \"\"):\n                            has_id_field = True\n                    # Check for mutation (setters)\n                    if stmt.type == \"function_definition\":\n                        func_name = \"\"\n                        for c in stmt.children:\n                            if c.type == \"identifier\":\n                                func_name = c.text.decode() if c.text else \"\"\n                                break\n                        if func_name.startswith(\"set\") or func_name.startswith(\"update\"):\n                            is_immutable = False\n                        if func_name in (\"save\", \"persist\", \"store\"):\n                            has_save_method = True\n                        if func_name in (\"find\", \"get\", \"load\", \"fetch\"):\n                            has_find_method = True\n        \n        # Classification logic\n        if has_save_method and has_find_method:\n            return Hadron(\n                id=52, name=\"Repository\", level=HadronLevel.ORGANELLE,\n                continent=\"Organization\", fundamental=\"Modules\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=class_name,\n                detection_rule=\"class with save+find methods\",\n            )\n        elif has_id_field and not is_immutable:\n            return Hadron(\n                id=44, name=\"Entity\", level=HadronLevel.MOLECULE,\n                continent=\"Organization\", fundamental=\"Aggregates\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=class_name,\n                detection_rule=\"class with id field + mutable\",\n            )\n        elif is_immutable and not has_id_field:\n            return Hadron(\n                id=43, name=\"ValueObject\", level=HadronLevel.MOLECULE,\n                continent=\"Organization\", fundamental=\"Aggregates\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=class_name,\n                detection_rule=\"class immutable + no id\",\n            )\n        \n        return None",
      "complexity": 0,
      "lines_of_code": 63,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._classify_function",
      "name": "AtomExtractor._classify_function",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 398,
      "end_line": 446,
      "role": "Transformer",
      "role_confidence": 78.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Optional[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a function into pure/impure/async/handler.",
      "signature": "def _classify_function(self, node, file_path: str) -> Optional[Hadron]:",
      "body_source": "    def _classify_function(self, node, file_path: str) -> Optional[Hadron]:\n        \"\"\"Classify a function into pure/impure/async/handler.\"\"\"\n        func_name = \"\"\n        is_async = False\n        is_generator = False\n        has_io = False\n        return_type = None\n        \n        for child in node.children:\n            if child.type == \"identifier\":\n                func_name = child.text.decode() if child.text else \"\"\n            elif child.type == \"async\":\n                is_async = True\n            elif child.type == \"block\":\n                # Scan for I/O calls\n                has_io = self._has_io_calls(child)\n        \n        # Classification\n        if is_async:\n            return Hadron(\n                id=32, name=\"AsyncFunction\", level=HadronLevel.MOLECULE,\n                continent=\"Logic & Flow\", fundamental=\"Functions\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=func_name,\n                detection_rule=\"async keyword\",\n            )\n        elif not has_io:\n            return Hadron(\n                id=30, name=\"PureFunction\", level=HadronLevel.MOLECULE,\n                continent=\"Logic & Flow\", fundamental=\"Functions\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=func_name,\n                detection_rule=\"no I/O calls detected\",\n                confidence=0.8,  # Not 100% sure without deeper analysis\n            )\n        else:\n            return Hadron(\n                id=31, name=\"ImpureFunction\", level=HadronLevel.MOLECULE,\n                continent=\"Logic & Flow\", fundamental=\"Functions\",\n                file_path=file_path,\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                text_snippet=func_name,\n                detection_rule=\"has I/O calls\",\n            )",
      "complexity": 0,
      "lines_of_code": 48,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._has_io_calls",
      "name": "AtomExtractor._has_io_calls",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 448,
      "end_line": 460,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if a node contains any I/O-related function calls.",
      "signature": "def _has_io_calls(self, node) -> bool:",
      "body_source": "    def _has_io_calls(self, node) -> bool:\n        \"\"\"Check if a node contains any I/O-related function calls.\"\"\"\n        if node.type == \"call\":\n            call_text = node.text.decode() if node.text else \"\"\n            for io_func in IO_INDICATORS:\n                if io_func in call_text.lower():\n                    return True\n        \n        for child in node.children:\n            if self._has_io_calls(child):\n                return True\n        \n        return False",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor._infer_organelles",
      "name": "AtomExtractor._infer_organelles",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 462,
      "end_line": 516,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "root_node"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "atoms",
          "type": "List[Hadron]"
        },
        {
          "name": "molecules",
          "type": "List[Hadron]"
        }
      ],
      "return_type": "List[Hadron]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Infer architecture-level organelles from patterns.",
      "signature": "def _infer_organelles(self, root_node, file_path: str,",
      "body_source": "    def _infer_organelles(self, root_node, file_path: str, \n                          atoms: List[Hadron], molecules: List[Hadron]) -> List[Hadron]:\n        \"\"\"Infer architecture-level organelles from patterns.\"\"\"\n        organelles = []\n        \n        # Look for handler patterns in function names\n        def visit(node):\n            if node.type == \"function_definition\":\n                func_name = \"\"\n                for child in node.children:\n                    if child.type == \"identifier\":\n                        func_name = child.text.decode() if child.text else \"\"\n                        break\n                \n                # Command handler detection\n                if \"command\" in func_name.lower() and \"handle\" in func_name.lower():\n                    organelles.append(Hadron(\n                        id=35, name=\"CommandHandler\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name contains command+handle\",\n                    ))\n                \n                # Query handler detection\n                elif \"query\" in func_name.lower() and \"handle\" in func_name.lower():\n                    organelles.append(Hadron(\n                        id=36, name=\"QueryHandler\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name contains query+handle\",\n                    ))\n                \n                # Validator detection\n                elif func_name.lower().startswith(\"validate\"):\n                    organelles.append(Hadron(\n                        id=40, name=\"Validator\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name starts with validate\",\n                    ))\n            \n            for child in node.children:\n                visit(child)\n        \n        visit(root_node)\n        return organelles",
      "complexity": 0,
      "lines_of_code": 54,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.visit",
      "name": "AtomExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 468,
      "end_line": 513,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            if node.type == \"function_definition\":\n                func_name = \"\"\n                for child in node.children:\n                    if child.type == \"identifier\":\n                        func_name = child.text.decode() if child.text else \"\"\n                        break\n                \n                # Command handler detection\n                if \"command\" in func_name.lower() and \"handle\" in func_name.lower():\n                    organelles.append(Hadron(\n                        id=35, name=\"CommandHandler\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name contains command+handle\",\n                    ))\n                \n                # Query handler detection\n                elif \"query\" in func_name.lower() and \"handle\" in func_name.lower():\n                    organelles.append(Hadron(\n                        id=36, name=\"QueryHandler\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name contains query+handle\",\n                    ))\n                \n                # Validator detection\n                elif func_name.lower().startswith(\"validate\"):\n                    organelles.append(Hadron(\n                        id=40, name=\"Validator\", level=HadronLevel.ORGANELLE,\n                        continent=\"Logic & Flow\", fundamental=\"Functions\",\n                        file_path=file_path,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        text_snippet=func_name,\n                        detection_rule=\"name starts with validate\",\n                    ))\n            \n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 45,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.to_json",
      "name": "AtomExtractor.to_json",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 518,
      "end_line": 532,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "hadrons",
          "type": "List[Hadron]"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convert hadrons to JSON for output.",
      "signature": "def to_json(self, hadrons: List[Hadron]) -> str:",
      "body_source": "    def to_json(self, hadrons: List[Hadron]) -> str:\n        \"\"\"Convert hadrons to JSON for output.\"\"\"\n        return json.dumps([{\n            \"id\": h.id,\n            \"name\": h.name,\n            \"level\": h.level.value,\n            \"continent\": h.continent,\n            \"fundamental\": h.fundamental,\n            \"file_path\": h.file_path,\n            \"start_line\": h.start_line,\n            \"end_line\": h.end_line,\n            \"text_snippet\": h.text_snippet,\n            \"detection_rule\": h.detection_rule,\n            \"confidence\": h.confidence,\n        } for h in hadrons], indent=2)",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py:AtomExtractor.summary",
      "name": "AtomExtractor.summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_extractor.py",
      "start_line": 534,
      "end_line": 553,
      "role": "Factory",
      "role_confidence": 75.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "hadrons",
          "type": "List[Hadron]"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate summary statistics.",
      "signature": "def summary(self, hadrons: List[Hadron]) -> Dict:",
      "body_source": "    def summary(self, hadrons: List[Hadron]) -> Dict:\n        \"\"\"Generate summary statistics.\"\"\"\n        by_level = {\"atom\": 0, \"molecule\": 0, \"organelle\": 0}\n        by_continent = {}\n        by_fundamental = {}\n        by_name = {}\n        \n        for h in hadrons:\n            by_level[h.level.value] += 1\n            by_continent[h.continent] = by_continent.get(h.continent, 0) + 1\n            by_fundamental[h.fundamental] = by_fundamental.get(h.fundamental, 0) + 1\n            by_name[h.name] = by_name.get(h.name, 0) + 1\n        \n        return {\n            \"total\": len(hadrons),\n            \"by_level\": by_level,\n            \"by_continent\": by_continent,\n            \"by_fundamental\": by_fundamental,\n            \"top_10_hadrons\": sorted(by_name.items(), key=lambda x: -x[1])[:10],\n        }",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator",
      "name": "StatsGenerator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 16,
      "end_line": 16,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class StatsGenerator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator.__init__",
      "name": "StatsGenerator.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 19,
      "end_line": 30,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.results = {\n            'metadata': {\n                'generated_at': datetime.now().isoformat(),\n                'spectrometer_version': 'V12.1',\n                'mode': 'comprehensive_analysis'\n            },\n            'summary': {},\n            'detailed_stats': {},\n            'particles': [],\n            'performance': {}\n        }",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator.generate_comprehensive_stats",
      "name": "StatsGenerator.generate_comprehensive_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 32,
      "end_line": 81,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_results",
          "type": "List[Dict]"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate comprehensive statistics from analysis results",
      "signature": "def generate_comprehensive_stats(self, analysis_results: List[Dict]) -> Dict[str, Any]:",
      "body_source": "    def generate_comprehensive_stats(self, analysis_results: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive statistics from analysis results\"\"\"\n        start_time = time.time()\n\n        # Collect all particles\n        all_particles = []\n        all_touchpoints = []\n        files_analyzed = 0\n        files_with_particles = 0\n        total_lines = 0\n        total_chars = 0\n        languages = set()\n\n        for result in analysis_results:\n            if 'particles' in result:\n                files_analyzed += 1\n                languages.add(result.get('language', 'unknown'))\n                total_lines += result.get('lines_analyzed', 0)\n                total_chars += result.get('chars_analyzed', 0)\n\n                if result['particles']:\n                    files_with_particles += 1\n                    all_particles.extend(result['particles'])\n\n                if 'touchpoints' in result:\n                    all_touchpoints.extend(result['touchpoints'])\n\n        processing_time = time.time() - start_time\n\n        # Apply auto pattern discovery to reduce unknowns\n        try:\n            from core.heuristic_classifier import apply_heuristics\n            all_particles, discovery_report = apply_heuristics(all_particles)\n            print(f\"  \ud83d\udd2c Auto-discovery: {discovery_report.get('particles_updated', 0)} particles reclassified\")\n        except ImportError:\n            discovery_report = {}\n\n        # Generate statistics\n        self.results['summary'] = self._generate_summary(all_particles, files_analyzed, files_with_particles)\n        self.results['detailed_stats'] = self._generate_detailed_stats(\n            all_particles, all_touchpoints, files_analyzed, files_with_particles,\n            total_lines, total_chars, languages, processing_time\n        )\n        self.results['particles'] = all_particles\n        self.results['performance'] = self._calculate_performance(\n            files_analyzed, total_lines, total_chars, processing_time\n        )\n        self.results['auto_discovery'] = discovery_report\n\n        return self.results",
      "complexity": 0,
      "lines_of_code": 49,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator._generate_summary",
      "name": "StatsGenerator._generate_summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 83,
      "end_line": 106,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        },
        {
          "name": "total_files",
          "type": "int"
        },
        {
          "name": "files_with_particles",
          "type": "int"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate summary statistics",
      "signature": "def _generate_summary(self, particles: List[Dict], total_files: int, files_with_particles: int) -> Dict:",
      "body_source": "    def _generate_summary(self, particles: List[Dict], total_files: int, files_with_particles: int) -> Dict:\n        \"\"\"Generate summary statistics\"\"\"\n        type_counts = Counter(p['type'] for p in particles)\n        confidence_scores = [p.get('confidence', 0) for p in particles]\n        rpbl_scores = [p.get('rpbl_score', 0) for p in particles]\n        unknown_count = type_counts.get('Unknown', 0)\n        recognized_count = len(particles) - unknown_count\n        recognized_pct = (recognized_count / len(particles) * 100) if particles else 0.0\n\n        return {\n            'total_particles_found': len(particles),\n            'files_analyzed': total_files,\n            'files_with_particles': files_with_particles,\n            'coverage_percentage': (files_with_particles / total_files * 100) if total_files > 0 else 0,\n            'unique_particle_types': len(type_counts),\n            'top_particle_types': type_counts.most_common(5),\n            'unknown_particles': unknown_count,\n            'recognized_particles': recognized_count,\n            'recognized_percentage': recognized_pct,\n            'average_confidence': sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0,\n            'average_rpbl_score': sum(rpbl_scores) / len(rpbl_scores) if rpbl_scores else 0,\n            'high_confidence_particles': len([c for c in confidence_scores if c >= 80]),\n            'medium_confidence_particles': len([c for c in confidence_scores if 60 <= c < 80])\n        }",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator._generate_detailed_stats",
      "name": "StatsGenerator._generate_detailed_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 108,
      "end_line": 177,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        },
        {
          "name": "touchpoints",
          "type": "List[Dict]"
        },
        {
          "name": "files_analyzed",
          "type": "int"
        },
        {
          "name": "files_with_particles",
          "type": "int"
        },
        {
          "name": "total_lines",
          "type": "int"
        },
        {
          "name": "total_chars",
          "type": "int"
        },
        {
          "name": "languages",
          "type": "set"
        },
        {
          "name": "processing_time",
          "type": "float"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate detailed statistics",
      "signature": "def _generate_detailed_stats(self, particles: List[Dict], touchpoints: List[Dict],",
      "body_source": "    def _generate_detailed_stats(self, particles: List[Dict], touchpoints: List[Dict],\n                               files_analyzed: int, files_with_particles: int,\n                               total_lines: int, total_chars: int, languages: set,\n                               processing_time: float) -> Dict:\n        \"\"\"Generate detailed statistics\"\"\"\n        # Particle type analysis\n        type_analysis = {}\n        for particle in particles:\n            ptype = particle.get('type', 'Unknown')\n            if ptype not in type_analysis:\n                type_analysis[ptype] = {\n                    'count': 0,\n                    'total_confidence': 0,\n                    'total_rpbl': 0,\n                    'responsibility_sum': 0,\n                    'purity_sum': 0,\n                    'boundary_sum': 0,\n                    'lifecycle_sum': 0\n                }\n\n            type_analysis[ptype]['count'] += 1\n            type_analysis[ptype]['total_confidence'] += particle.get('confidence', 0)\n            type_analysis[ptype]['total_rpbl'] += particle.get('rpbl_score', 0)\n            type_analysis[ptype]['responsibility_sum'] += particle.get('responsibility', 0)\n            type_analysis[ptype]['purity_sum'] += particle.get('purity', 0)\n            type_analysis[ptype]['boundary_sum'] += particle.get('boundary', 0)\n            type_analysis[ptype]['lifecycle_sum'] += particle.get('lifecycle', 0)\n\n        # Calculate averages for each type\n        for ptype, stats in type_analysis.items():\n            count = stats['count']\n            stats['average_confidence'] = stats['total_confidence'] / count if count > 0 else 0\n            stats['average_rpbl'] = stats['total_rpbl'] / count if count > 0 else 0\n            stats['average_responsibility'] = stats['responsibility_sum'] / count if count > 0 else 0\n            stats['average_purity'] = stats['purity_sum'] / count if count > 0 else 0\n            stats['average_boundary'] = stats['boundary_sum'] / count if count > 0 else 0\n            stats['average_lifecycle'] = stats['lifecycle_sum'] / count if count > 0 else 0\n\n        # Touchpoint analysis\n        touchpoint_counts = Counter(t['type'] for t in touchpoints)\n\n        return {\n            'file_analysis': {\n                'total_files_processed': files_analyzed,\n                'files_with_particles': files_with_particles,\n                'files_without_particles': files_analyzed - files_with_particles,\n                'file_coverage_percentage': (files_with_particles / files_analyzed * 100) if files_analyzed > 0 else 0,\n                'average_file_size': total_chars / files_analyzed if files_analyzed > 0 else 0,\n                'total_lines_analyzed': total_lines,\n                'total_characters_analyzed': total_chars\n            },\n            'particle_analysis': {\n                'total_particles_detected': len(particles),\n                'unique_particle_types': len(type_analysis),\n                'particle_type_distribution': type_analysis,\n                'particles_per_file': len(particles) / files_analyzed if files_analyzed > 0 else 0,\n                'max_particles_in_file': max([len([p for p in particles if p.get('file_path') == f])\n                                            for f in set(p.get('file_path') for p in particles)], default=0)\n            },\n            'touchpoint_analysis': {\n                'total_touchpoints_detected': len(touchpoints),\n                'unique_touchpoint_types': len(touchpoint_counts),\n                'touchpoint_distribution': dict(touchpoint_counts.most_common())\n            },\n            'language_analysis': {\n                'languages_detected': list(languages),\n                'number_of_languages': len(languages)\n            },\n            'processing_time_seconds': processing_time\n        }",
      "complexity": 0,
      "lines_of_code": 69,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator._calculate_performance",
      "name": "StatsGenerator._calculate_performance",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 179,
      "end_line": 196,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "files",
          "type": "int"
        },
        {
          "name": "lines",
          "type": "int"
        },
        {
          "name": "chars",
          "type": "int"
        },
        {
          "name": "time_seconds",
          "type": "float"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Calculate performance metrics",
      "signature": "def _calculate_performance(self, files: int, lines: int, chars: int, time_seconds: float) -> Dict:",
      "body_source": "    def _calculate_performance(self, files: int, lines: int, chars: int, time_seconds: float) -> Dict:\n        \"\"\"Calculate performance metrics\"\"\"\n        if time_seconds > 0:\n            return {\n                'files_per_second': files / time_seconds,\n                'lines_per_second': lines / time_seconds,\n                'characters_per_second': chars / time_seconds,\n                'avg_time_per_file': time_seconds / files if files > 0 else 0,\n                'success_rate': 100.0  # All files processed successfully\n            }\n        else:\n            return {\n                'files_per_second': 0,\n                'lines_per_second': 0,\n                'characters_per_second': 0,\n                'avg_time_per_file': 0,\n                'success_rate': 0\n            }",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator.save_results",
      "name": "StatsGenerator.save_results",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 198,
      "end_line": 221,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_persist",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "output_dir",
          "type": "Path"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Save results to multiple output formats",
      "signature": "def save_results(self, output_dir: Path):",
      "body_source": "    def save_results(self, output_dir: Path):\n        \"\"\"Save results to multiple output formats\"\"\"\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save JSON\n        json_file = output_dir / 'results.json'\n        with open(json_file, 'w') as f:\n            json.dump(self.results, f, indent=2)\n\n        # Save human-readable stats\n        stats_file = output_dir / 'stats.txt'\n        with open(stats_file, 'w') as f:\n            self._write_human_readable_stats(f)\n\n        # Save particles CSV\n        csv_file = output_dir / 'particles.csv'\n        with open(csv_file, 'w', newline='') as f:\n            self._write_particles_csv(f)\n\n        return {\n            'json_file': str(json_file),\n            'stats_file': str(stats_file),\n            'csv_file': str(csv_file)\n        }",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator._write_human_readable_stats",
      "name": "StatsGenerator._write_human_readable_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 223,
      "end_line": 255,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_persist",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Write human-readable statistics",
      "signature": "def _write_human_readable_stats(self, file):",
      "body_source": "    def _write_human_readable_stats(self, file):\n        \"\"\"Write human-readable statistics\"\"\"\n        file.write(\"=\"*100 + \"\\n\")\n        file.write(\"\ud83d\ude80 SPECTROMETER V12 - COMPREHENSIVE ANALYSIS RESULTS\\n\")\n        file.write(\"=\"*100 + \"\\n\\n\")\n\n        # Summary\n        file.write(\"\ud83d\udcca SUMMARY STATISTICS\\n\")\n        file.write(\"-\" * 50 + \"\\n\")\n        summary = self.results['summary']\n        file.write(f\"Total Particles Found: {summary['total_particles_found']}\\n\")\n        file.write(f\"Files Analyzed: {summary['files_analyzed']}\\n\")\n        file.write(f\"Files with Particles: {summary['files_with_particles']}\\n\")\n        file.write(f\"Coverage: {summary['coverage_percentage']:.1f}%\\n\")\n        file.write(f\"Unique Particle Types: {summary['unique_particle_types']}\\n\")\n        file.write(f\"Average Confidence: {summary['average_confidence']:.1f}%\\n\")\n        file.write(f\"Average RPBL Score: {summary['average_rpbl_score']:.1f}\\n\\n\")\n\n        # Top particle types\n        file.write(\"\ud83c\udfc6 TOP PARTICLE TYPES\\n\")\n        file.write(\"-\" * 50 + \"\\n\")\n        for i, (ptype, count) in enumerate(summary['top_particle_types'][:5], 1):\n            percentage = (count / summary['total_particles_found'] * 100) if summary['total_particles_found'] > 0 else 0\n            file.write(f\"{i}. {ptype}: {count} ({percentage:.1f}%)\\n\")\n\n        # Performance\n        file.write(f\"\\n\u26a1 PERFORMANCE METRICS\\n\")\n        file.write(\"-\" * 50 + \"\\n\")\n        perf = self.results['performance']\n        file.write(f\"Files/Second: {perf['files_per_second']:.0f}\\n\")\n        file.write(f\"Lines/Second: {perf['lines_per_second']:.0f}\\n\")\n        file.write(f\"Characters/Second: {perf['characters_per_second']:.0f}\\n\")\n        file.write(f\"Success Rate: {perf['success_rate']:.1f}%\\n\")",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py:StatsGenerator._write_particles_csv",
      "name": "StatsGenerator._write_particles_csv",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/stats_generator.py",
      "start_line": 257,
      "end_line": 279,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_persist",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Write particles to CSV",
      "signature": "def _write_particles_csv(self, file):",
      "body_source": "    def _write_particles_csv(self, file):\n        \"\"\"Write particles to CSV\"\"\"\n        fieldnames = ['type', 'name', 'file_path', 'line', 'confidence', 'responsibility', 'purity', 'boundary', 'lifecycle', 'rpbl_score', 'evidence']\n        writer = csv.DictWriter(file, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for particle in self.results['particles']:\n            # Clean evidence for CSV\n            evidence = particle.get('evidence', '')[:50].replace('\\n', ' ')\n\n            writer.writerow({\n                'type': particle.get('type', ''),\n                'name': particle.get('name', ''),\n                'file_path': particle.get('file_path', ''),\n                'line': particle.get('line', ''),\n                'confidence': particle.get('confidence', 0),\n                'responsibility': particle.get('responsibility', 0),\n                'purity': particle.get('purity', 0),\n                'boundary': particle.get('boundary', 0),\n                'lifecycle': particle.get('lifecycle', 0),\n                'rpbl_score': particle.get('rpbl_score', 0),\n                'evidence': evidence\n            })",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:Layer",
      "name": "Layer",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 17,
      "end_line": 17,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Layer(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeNode",
      "name": "PurposeNode",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 28,
      "end_line": 28,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PurposeNode:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeField",
      "name": "PurposeField",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 51,
      "end_line": 51,
      "role": "DTO",
      "role_confidence": 70.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PurposeField:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeField.summary",
      "name": "PurposeField.summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 58,
      "end_line": 69,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Summarize the purpose field",
      "signature": "def summary(self) -> dict:",
      "body_source": "    def summary(self) -> dict:\n        \"\"\"Summarize the purpose field\"\"\"\n        layer_counts = Counter(n.layer for n in self.nodes.values())\n        purpose_counts = Counter(n.atomic_purpose for n in self.nodes.values())\n        \n        return {\n            \"total_nodes\": len(self.nodes),\n            \"layers\": {l.value: c for l, c in layer_counts.items()},\n            \"purposes\": dict(purpose_counts.most_common(10)),\n            \"violations\": len(self.violations),\n            \"layer_purposes\": {l.value: p for l, p in self.layer_purposes.items()}\n        }",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector",
      "name": "PurposeFieldDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 72,
      "end_line": 72,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class PurposeFieldDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector.__init__",
      "name": "PurposeFieldDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 156,
      "end_line": 157,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.nodes: Dict[str, PurposeNode] = {}",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector.detect_field",
      "name": "PurposeFieldDetector.detect_field",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 159,
      "end_line": 192,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list",
          "default": "None"
        }
      ],
      "return_type": "PurposeField",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect the complete Purpose Field from analysis output.\n\nArgs:\n    analysis_nodes: List of nodes from unified_analysis\n    edges: List of edges (optional, for flow analysis)\n\nReturns:\n    PurposeField with all levels computed",
      "signature": "def detect_field(self, analysis_nodes: list, edges: list = None) -> PurposeField:",
      "body_source": "    def detect_field(self, analysis_nodes: list, edges: list = None) -> PurposeField:\n        \"\"\"\n        Detect the complete Purpose Field from analysis output.\n        \n        Args:\n            analysis_nodes: List of nodes from unified_analysis\n            edges: List of edges (optional, for flow analysis)\n        \n        Returns:\n            PurposeField with all levels computed\n        \"\"\"\n        edges = edges or []\n        \n        # Stage 1: Create PurposeNodes with atomic purpose (from roles)\n        self._create_nodes(analysis_nodes)\n        \n        # Stage 2: Build hierarchy (parent-child relationships)\n        self._build_hierarchy(analysis_nodes)\n        \n        # Stage 3: Compute composite purpose (emergence)\n        self._compute_composite_purposes()\n        \n        # Stage 4: Assign layers\n        self._assign_layers()\n        \n        # Stage 5: Detect purpose flow and violations\n        purpose_flow, violations = self._analyze_flow(edges)\n        \n        return PurposeField(\n            nodes=self.nodes,\n            layer_purposes=self.LAYER_PURPOSES,\n            purpose_flow=purpose_flow,\n            violations=violations\n        )",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector._create_nodes",
      "name": "PurposeFieldDetector._create_nodes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 194,
      "end_line": 221,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Create PurposeNodes from analysis output",
      "signature": "def _create_nodes(self, analysis_nodes: list):",
      "body_source": "    def _create_nodes(self, analysis_nodes: list):\n        \"\"\"Create PurposeNodes from analysis output\"\"\"\n        for i, node in enumerate(analysis_nodes):\n            # Handle both dict and object\n            if hasattr(node, 'id'):\n                node_id = node.id\n                name = node.name\n                kind = node.kind\n                role = node.role\n                conf = node.role_confidence\n            else:\n                node_id = node.get('id', '')\n                name = node.get('name', 'unknown')\n                kind = node.get('kind', 'function')\n                role = node.get('role', 'Unknown')\n                conf = node.get('role_confidence', 0.0)\n            \n            # Use name as ID if ID is empty (common issue)\n            if not node_id:\n                node_id = name or f\"node_{i}\"\n            \n            self.nodes[node_id] = PurposeNode(\n                id=node_id,\n                name=name,\n                kind=kind,\n                atomic_purpose=role,\n                atomic_confidence=conf\n            )",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector._build_hierarchy",
      "name": "PurposeFieldDetector._build_hierarchy",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 223,
      "end_line": 234,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build parent-child relationships",
      "signature": "def _build_hierarchy(self, analysis_nodes: list):",
      "body_source": "    def _build_hierarchy(self, analysis_nodes: list):\n        \"\"\"Build parent-child relationships\"\"\"\n        # Infer hierarchy from names (Class.method pattern)\n        for node_id, node in self.nodes.items():\n            if '.' in node.name:\n                parent_name = node.name.rsplit('.', 1)[0]\n                # Find parent\n                for pid, pnode in self.nodes.items():\n                    if pnode.name == parent_name:\n                        node.parent_id = pid\n                        pnode.children_ids.append(node_id)\n                        break",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector._compute_composite_purposes",
      "name": "PurposeFieldDetector._compute_composite_purposes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 236,
      "end_line": 258,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Compute emergent composite purpose for parent nodes",
      "signature": "def _compute_composite_purposes(self):",
      "body_source": "    def _compute_composite_purposes(self):\n        \"\"\"Compute emergent composite purpose for parent nodes\"\"\"\n        # Process nodes with children\n        for node_id, node in self.nodes.items():\n            if node.children_ids:\n                # Gather child purposes\n                child_purposes = []\n                for child_id in node.children_ids:\n                    if child_id in self.nodes:\n                        child_purposes.append(self.nodes[child_id].atomic_purpose)\n                \n                node.child_purposes = child_purposes\n                \n                # Apply emergence rules\n                purpose_set = frozenset(set(child_purposes))\n                \n                if purpose_set in self.EMERGENCE_RULES:\n                    node.composite_purpose = self.EMERGENCE_RULES[purpose_set]\n                else:\n                    # Default: use most common child purpose\n                    if child_purposes:\n                        most_common = Counter(child_purposes).most_common(1)[0][0]\n                        node.composite_purpose = f\"{most_common}Container\"",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector._assign_layers",
      "name": "PurposeFieldDetector._assign_layers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 260,
      "end_line": 278,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Assign architectural layers based on purpose",
      "signature": "def _assign_layers(self):",
      "body_source": "    def _assign_layers(self):\n        \"\"\"Assign architectural layers based on purpose\"\"\"\n        for node in self.nodes.values():\n            # Use composite purpose if available, else atomic\n            purpose = node.composite_purpose or node.atomic_purpose\n            \n            if purpose in self.PURPOSE_TO_LAYER:\n                node.layer = self.PURPOSE_TO_LAYER[purpose]\n            else:\n                # Try to infer from name patterns\n                name_lower = node.name.lower()\n                if any(p in name_lower for p in ['controller', 'view', 'route', 'endpoint']):\n                    node.layer = Layer.PRESENTATION\n                elif any(p in name_lower for p in ['service', 'usecase', 'handler']):\n                    node.layer = Layer.APPLICATION\n                elif any(p in name_lower for p in ['entity', 'model', 'domain', 'policy']):\n                    node.layer = Layer.DOMAIN\n                elif any(p in name_lower for p in ['repository', 'gateway', 'adapter', 'config']):\n                    node.layer = Layer.INFRASTRUCTURE",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:PurposeFieldDetector._analyze_flow",
      "name": "PurposeFieldDetector._analyze_flow",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 280,
      "end_line": 325,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "edges",
          "type": "list"
        }
      ],
      "return_type": "Tuple[List, List]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze purpose flow and detect violations",
      "signature": "def _analyze_flow(self, edges: list) -> Tuple[List, List]:",
      "body_source": "    def _analyze_flow(self, edges: list) -> Tuple[List, List]:\n        \"\"\"Analyze purpose flow and detect violations\"\"\"\n        purpose_flow = []\n        violations = []\n        \n        # Layer order (lower index = higher layer)\n        layer_order = {\n            Layer.PRESENTATION: 0,\n            Layer.APPLICATION: 1,\n            Layer.DOMAIN: 2,\n            Layer.INFRASTRUCTURE: 3,\n            Layer.UNKNOWN: 99\n        }\n        \n        for edge in edges:\n            # Handle both dict and tuple\n            if isinstance(edge, dict):\n                source = edge.get('source', edge.get('from', ''))\n                target = edge.get('target', edge.get('to', ''))\n            elif isinstance(edge, (list, tuple)) and len(edge) >= 2:\n                source, target = edge[0], edge[1]\n            else:\n                continue\n            \n            source_node = self.nodes.get(source)\n            target_node = self.nodes.get(target)\n            \n            if source_node and target_node:\n                source_layer = source_node.layer\n                target_layer = target_node.layer\n                \n                # Record flow\n                purpose_flow.append((\n                    source,\n                    target,\n                    f\"{source_layer.value} \u2192 {target_layer.value}\"\n                ))\n                \n                # Detect violations: lower layer calling higher layer\n                if layer_order.get(target_layer, 99) < layer_order.get(source_layer, 0):\n                    violations.append(\n                        f\"Purpose flow violation: {source_node.name} ({source_layer.value}) \"\n                        f\"calls {target_node.name} ({target_layer.value})\"\n                    )\n        \n        return purpose_flow, violations",
      "complexity": 0,
      "lines_of_code": 45,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py:detect_purpose_field",
      "name": "detect_purpose_field",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_field.py",
      "start_line": 328,
      "end_line": 340,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list",
          "default": "None"
        }
      ],
      "return_type": "PurposeField",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to detect purpose field.\n\nUsage:\n    from purpose_field import detect_purpose_field\n    \n    result = analyze(path)\n    field = detect_purpose_field(result.nodes, result.edges)\n    print(field.summary())",
      "signature": "def detect_purpose_field(nodes: list, edges: list = None) -> PurposeField:",
      "body_source": "def detect_purpose_field(nodes: list, edges: list = None) -> PurposeField:\n    \"\"\"\n    Convenience function to detect purpose field.\n    \n    Usage:\n        from purpose_field import detect_purpose_field\n        \n        result = analyze(path)\n        field = detect_purpose_field(result.nodes, result.edges)\n        print(field.summary())\n    \"\"\"\n    detector = PurposeFieldDetector()\n    return detector.detect_field(nodes, edges)",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:DuplicateGroup",
      "name": "DuplicateGroup",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 26,
      "end_line": 26,
      "role": "DTO",
      "role_confidence": 78.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class DuplicateGroup:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:OverEngineeringSignal",
      "name": "OverEngineeringSignal",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 35,
      "end_line": 35,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class OverEngineeringSignal:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:load_semantic_ids",
      "name": "load_semantic_ids",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 45,
      "end_line": 60,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "path",
          "type": "str | Path"
        }
      ],
      "return_type": "list[dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load semantic IDs from JSON file.",
      "signature": "def load_semantic_ids(path: str | Path) -> list[dict]:",
      "body_source": "def load_semantic_ids(path: str | Path) -> list[dict]:\n    \"\"\"Load semantic IDs from JSON file.\"\"\"\n    path = Path(path)\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    \n    # Handle both list and dict formats\n    if isinstance(data, list):\n        return data\n    elif isinstance(data, dict):\n        # If it's a dict with 'ids' key\n        if \"ids\" in data:\n            return data[\"ids\"]\n        # If it's a dict of id -> info\n        return [{\"id\": k, **v} for k, v in data.items()]\n    return []",
      "complexity": 0,
      "lines_of_code": 15,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:extract_name_from_id",
      "name": "extract_name_from_id",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 63,
      "end_line": 69,
      "role": "Transformer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "semantic_id",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract the function/class name from a semantic ID.",
      "signature": "def extract_name_from_id(semantic_id: str) -> str:",
      "body_source": "def extract_name_from_id(semantic_id: str) -> str:\n    \"\"\"Extract the function/class name from a semantic ID.\"\"\"\n    # Format: LOG.FNC.M|server.js|functionName|calls:4|...\n    parts = semantic_id.split(\"|\")\n    if len(parts) >= 3:\n        return parts[2]\n    return semantic_id",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:normalize_name",
      "name": "normalize_name",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 72,
      "end_line": 78,
      "role": "Transformer",
      "role_confidence": 77.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Normalize a function name for comparison.",
      "signature": "def normalize_name(name: str) -> str:",
      "body_source": "def normalize_name(name: str) -> str:\n    \"\"\"Normalize a function name for comparison.\"\"\"\n    # Remove common prefixes/suffixes\n    name = re.sub(r\"^(get|set|is|has|can|do|on|handle|process)\", \"\", name, flags=re.I)\n    name = re.sub(r\"(Handler|Callback|Listener|Wrapper|Helper|Util|Utils)$\", \"\", name, flags=re.I)\n    # Convert to lowercase for comparison\n    return name.lower()",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:find_semantic_duplicates",
      "name": "find_semantic_duplicates",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 81,
      "end_line": 131,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "semantic_ids",
          "type": "list[dict]"
        },
        {
          "name": "threshold",
          "type": "float",
          "default": "0.8"
        }
      ],
      "return_type": "list[DuplicateGroup]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find functions with very similar names that might be duplicates.\n\nUses fuzzy string matching on normalized function names.",
      "signature": "def find_semantic_duplicates(semantic_ids: list[dict], threshold: float = 0.8) -> list[DuplicateGroup]:",
      "body_source": "def find_semantic_duplicates(semantic_ids: list[dict], threshold: float = 0.8) -> list[DuplicateGroup]:\n    \"\"\"\n    Find functions with very similar names that might be duplicates.\n    \n    Uses fuzzy string matching on normalized function names.\n    \"\"\"\n    groups = []\n    seen = set()\n    \n    # Extract all function names with their IDs\n    functions = []\n    for item in semantic_ids:\n        sid = item.get(\"id\", item) if isinstance(item, dict) else item\n        if \"|\" in sid:\n            name = extract_name_from_id(sid)\n            file = sid.split(\"|\")[1] if len(sid.split(\"|\")) > 1 else \"\"\n            functions.append({\"id\": sid, \"name\": name, \"file\": file, \"normalized\": normalize_name(name)})\n    \n    # Group by similarity\n    for i, func1 in enumerate(functions):\n        if func1[\"id\"] in seen:\n            continue\n            \n        similar = [func1]\n        for j, func2 in enumerate(functions[i+1:], i+1):\n            if func2[\"id\"] in seen:\n                continue\n            \n            # Check name similarity\n            ratio = SequenceMatcher(None, func1[\"normalized\"], func2[\"normalized\"]).ratio()\n            \n            # Also check if names are substrings of each other\n            if len(func1[\"normalized\"]) > 3 and len(func2[\"normalized\"]) > 3:\n                if func1[\"normalized\"] in func2[\"normalized\"] or func2[\"normalized\"] in func1[\"normalized\"]:\n                    ratio = max(ratio, 0.85)\n            \n            if ratio >= threshold:\n                similar.append(func2)\n                seen.add(func2[\"id\"])\n        \n        if len(similar) > 1:\n            seen.add(func1[\"id\"])\n            group = DuplicateGroup(\n                reason=\"Similar function names\",\n                confidence=threshold,\n                members=[{\"id\": f[\"id\"], \"name\": f[\"name\"], \"file\": f[\"file\"]} for f in similar],\n                recommendation=f\"Review these {len(similar)} functions for consolidation\"\n            )\n            groups.append(group)\n    \n    return groups",
      "complexity": 0,
      "lines_of_code": 50,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:find_structural_duplicates",
      "name": "find_structural_duplicates",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 134,
      "end_line": 174,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "min_degree",
          "type": "int",
          "default": "2"
        }
      ],
      "return_type": "list[DuplicateGroup]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find functions with identical call signatures (same in/out edges).\n\nIf two functions call the exact same set of other functions,\nthey might be doing the same thing.",
      "signature": "def find_structural_duplicates(G: \"nx.DiGraph\", min_degree: int = 2) -> list[DuplicateGroup]:",
      "body_source": "def find_structural_duplicates(G: \"nx.DiGraph\", min_degree: int = 2) -> list[DuplicateGroup]:\n    \"\"\"\n    Find functions with identical call signatures (same in/out edges).\n    \n    If two functions call the exact same set of other functions,\n    they might be doing the same thing.\n    \"\"\"\n    if nx is None:\n        return []\n    \n    groups = []\n    \n    # Build call signature for each node\n    signatures = {}\n    for node in G.nodes():\n        if G.out_degree(node) >= min_degree:\n            # Signature = sorted tuple of callees\n            callees = tuple(sorted(G.successors(node)))\n            if callees:\n                if callees not in signatures:\n                    signatures[callees] = []\n                signatures[callees].append(node)\n    \n    # Find duplicate signatures\n    for callees, nodes in signatures.items():\n        if len(nodes) > 1:\n            members = []\n            for node_id in nodes:\n                name = node_id.split(\"|\")[-2] if \"|\" in node_id else node_id\n                file = node_id.split(\"|\")[1] if \"|\" in node_id and len(node_id.split(\"|\")) > 1 else \"\"\n                members.append({\"id\": node_id, \"name\": name, \"file\": file})\n            \n            group = DuplicateGroup(\n                reason=f\"Identical call pattern ({len(callees)} shared callees)\",\n                confidence=0.9,\n                members=members,\n                recommendation=\"These functions call the exact same dependencies\u2014likely duplicates\"\n            )\n            groups.append(group)\n    \n    return groups",
      "complexity": 0,
      "lines_of_code": 40,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:find_over_engineering",
      "name": "find_over_engineering",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 177,
      "end_line": 237,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "semantic_ids",
          "type": "list[dict]"
        }
      ],
      "return_type": "list[OverEngineeringSignal]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect over-engineering patterns:\n\n1. Pass-through wrappers: Functions that call exactly 1 thing\n2. Unnecessary abstractions: Many tiny functions with few calls\n3. God class indicators: Functions with huge fan-out",
      "signature": "def find_over_engineering(G: \"nx.DiGraph\", semantic_ids: list[dict]) -> list[OverEngineeringSignal]:",
      "body_source": "def find_over_engineering(G: \"nx.DiGraph\", semantic_ids: list[dict]) -> list[OverEngineeringSignal]:\n    \"\"\"\n    Detect over-engineering patterns:\n    \n    1. Pass-through wrappers: Functions that call exactly 1 thing\n    2. Unnecessary abstractions: Many tiny functions with few calls\n    3. God class indicators: Functions with huge fan-out\n    \"\"\"\n    signals = []\n    \n    if nx is None:\n        return signals\n    \n    for node in G.nodes():\n        in_deg = G.in_degree(node)\n        out_deg = G.out_degree(node)\n        name = node.split(\"|\")[-2] if \"|\" in node else node\n        file = node.split(\"|\")[1] if \"|\" in node and len(node.split(\"|\")) > 1 else \"\"\n        \n        # Skip builtins and very short names (likely minified)\n        if len(name) <= 2 or name in (\"n\", \"e\", \"t\", \"r\"):\n            continue\n        \n        # 1. Pass-through wrapper: calls exactly 1 thing, used by many\n        if out_deg == 1 and in_deg >= 3:\n            callee = list(G.successors(node))[0]\n            callee_name = callee.split(\"|\")[-2] if \"|\" in callee else callee\n            signals.append(OverEngineeringSignal(\n                node_id=node,\n                name=name,\n                file=file,\n                signal_type=\"pass_through_wrapper\",\n                evidence=f\"Only calls `{callee_name}`, used {in_deg} times\",\n                severity=\"low\"\n            ))\n        \n        # 2. Tiny function used once: possibly unnecessary abstraction\n        if in_deg == 1 and out_deg <= 1:\n            # Check if name suggests it's a helper\n            if any(x in name.lower() for x in [\"helper\", \"util\", \"wrapper\", \"do\", \"internal\"]):\n                signals.append(OverEngineeringSignal(\n                    node_id=node,\n                    name=name,\n                    file=file,\n                    signal_type=\"single_use_helper\",\n                    evidence=f\"Used only once, calls {out_deg} functions\",\n                    severity=\"low\"\n                ))\n        \n        # 3. God function: extremely high fan-out\n        if out_deg > 50:\n            signals.append(OverEngineeringSignal(\n                node_id=node,\n                name=name,\n                file=file,\n                signal_type=\"god_function\",\n                evidence=f\"Calls {out_deg} other functions\u2014too many responsibilities\",\n                severity=\"high\"\n            ))\n    \n    return signals",
      "complexity": 0,
      "lines_of_code": 60,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:generate_redundancy_report",
      "name": "generate_redundancy_report",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 240,
      "end_line": 328,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "semantic_duplicates",
          "type": "list[DuplicateGroup]"
        },
        {
          "name": "structural_duplicates",
          "type": "list[DuplicateGroup]"
        },
        {
          "name": "over_engineering",
          "type": "list[OverEngineeringSignal]"
        },
        {
          "name": "output_path",
          "type": "str | Path",
          "default": "None"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a markdown report of findings.",
      "signature": "def generate_redundancy_report(",
      "body_source": "def generate_redundancy_report(\n    semantic_duplicates: list[DuplicateGroup],\n    structural_duplicates: list[DuplicateGroup],\n    over_engineering: list[OverEngineeringSignal],\n    output_path: str | Path = None\n) -> str:\n    \"\"\"Generate a markdown report of findings.\"\"\"\n    \n    lines = [\n        \"# \ud83d\udd0d Redundancy & Over-Engineering Report\",\n        \"\",\n        f\"| Category | Count |\",\n        f\"|----------|------:|\",\n        f\"| Semantic Duplicate Groups | {len(semantic_duplicates)} |\",\n        f\"| Structural Duplicate Groups | {len(structural_duplicates)} |\",\n        f\"| Over-Engineering Signals | {len(over_engineering)} |\",\n        \"\",\n    ]\n    \n    # Semantic duplicates\n    if semantic_duplicates:\n        lines.extend([\n            \"---\",\n            \"\",\n            \"## \ud83d\udd24 Semantic Duplicates (Similar Names)\",\n            \"\",\n            \"Functions with very similar names that might be doing the same thing.\",\n            \"\",\n        ])\n        for i, group in enumerate(semantic_duplicates[:15], 1):\n            names = [m[\"name\"] for m in group.members]\n            files = set(m[\"file\"] for m in group.members)\n            lines.append(f\"**{i}. {', '.join(names[:3])}{'...' if len(names) > 3 else ''}**\")\n            lines.append(f\"   - Files: {', '.join(list(files)[:3])}\")\n            lines.append(f\"   - Count: {len(group.members)}\")\n            lines.append(\"\")\n    \n    # Structural duplicates\n    if structural_duplicates:\n        lines.extend([\n            \"---\",\n            \"\",\n            \"## \ud83d\udd17 Structural Duplicates (Same Call Pattern)\",\n            \"\",\n            \"Functions that call the exact same set of dependencies.\",\n            \"\",\n        ])\n        for i, group in enumerate(structural_duplicates[:10], 1):\n            names = [m[\"name\"] for m in group.members]\n            lines.append(f\"**{i}. {', '.join(names[:4])}**\")\n            lines.append(f\"   - {group.reason}\")\n            lines.append(\"\")\n    \n    # Over-engineering\n    if over_engineering:\n        high = [s for s in over_engineering if s.severity == \"high\"]\n        lines.extend([\n            \"---\",\n            \"\",\n            \"## \u26a0\ufe0f Over-Engineering Signals\",\n            \"\",\n        ])\n        \n        if high:\n            lines.append(\"### \ud83d\udea8 High Severity (God Functions)\")\n            lines.append(\"\")\n            lines.append(\"| Function | File | Evidence |\")\n            lines.append(\"|----------|------|----------|\")\n            for sig in high[:15]:\n                lines.append(f\"| `{sig.name}` | {sig.file} | {sig.evidence} |\")\n            lines.append(\"\")\n        \n        wrappers = [s for s in over_engineering if s.signal_type == \"pass_through_wrapper\"]\n        if wrappers:\n            lines.append(f\"### \ud83d\udce6 Pass-Through Wrappers ({len(wrappers)} found)\")\n            lines.append(\"\")\n            lines.append(\"These functions just forward to one other function\u2014consider inlining.\")\n            lines.append(\"\")\n            for sig in wrappers[:10]:\n                lines.append(f\"- `{sig.name}` ({sig.file}): {sig.evidence}\")\n            lines.append(\"\")\n    \n    report = \"\\n\".join(lines)\n    \n    if output_path:\n        Path(output_path).write_text(report, encoding=\"utf-8\")\n        print(f\"\ud83d\udcdd Report saved to: {output_path}\")\n    \n    return report",
      "complexity": 0,
      "lines_of_code": 88,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py:analyze_redundancy",
      "name": "analyze_redundancy",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/redundancy_detector.py",
      "start_line": 331,
      "end_line": 373,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "graph_path",
          "type": "str | Path"
        },
        {
          "name": "semantic_ids_path",
          "type": "str | Path",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Run full redundancy analysis.",
      "signature": "def analyze_redundancy(graph_path: str | Path, semantic_ids_path: str | Path = None):",
      "body_source": "def analyze_redundancy(graph_path: str | Path, semantic_ids_path: str | Path = None):\n    \"\"\"Run full redundancy analysis.\"\"\"\n    import sys\n    from pathlib import Path as P\n    # Add parent directory to path for imports\n    sys.path.insert(0, str(P(__file__).parent.parent))\n    from core.graph_analyzer import load_graph\n    \n    print(\"\ud83d\udd0d Running Redundancy Analysis...\")\n    \n    G = load_graph(graph_path)\n    \n    # Try to find semantic IDs\n    semantic_ids = []\n    if semantic_ids_path:\n        semantic_ids = load_semantic_ids(semantic_ids_path)\n    else:\n        # Try to find it in the same directory\n        graph_dir = Path(graph_path).parent\n        candidates = [\"semantic_ids.json\", \"learning_report.json\"]\n        for candidate in candidates:\n            candidate_path = graph_dir / candidate\n            if candidate_path.exists():\n                semantic_ids = load_semantic_ids(candidate_path)\n                if semantic_ids:\n                    print(f\"   Found semantic IDs in: {candidate}\")\n                    break\n    \n    print(f\"   Loaded {len(semantic_ids)} semantic IDs\")\n    \n    print(\"\ud83d\udd24 Finding semantic duplicates...\")\n    semantic_dups = find_semantic_duplicates(semantic_ids)\n    print(f\"   Found {len(semantic_dups)} groups\")\n    \n    print(\"\ud83d\udd17 Finding structural duplicates...\")\n    structural_dups = find_structural_duplicates(G)\n    print(f\"   Found {len(structural_dups)} groups\")\n    \n    print(\"\u26a0\ufe0f  Detecting over-engineering...\")\n    over_eng = find_over_engineering(G, semantic_ids)\n    print(f\"   Found {len(over_eng)} signals\")\n    \n    return semantic_dups, structural_dups, over_eng",
      "complexity": 0,
      "lines_of_code": 42,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomDefinition",
      "name": "AtomDefinition",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 21,
      "end_line": 21,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class AtomDefinition:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry",
      "name": "AtomRegistry",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 38,
      "end_line": 38,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class AtomRegistry:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.__init__",
      "name": "AtomRegistry.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 45,
      "end_line": 49,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.atoms: Dict[int, AtomDefinition] = {}\n        self.ast_type_map: Dict[str, int] = {}  # node_type -> atom_id\n        self.next_id: int = 97  # Original 96 + new discoveries\n        self._init_canonical_atoms()",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry._init_canonical_atoms",
      "name": "AtomRegistry._init_canonical_atoms",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 51,
      "end_line": 244,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Initialize with the original 96 + refined atoms.",
      "signature": "def _init_canonical_atoms(self):",
      "body_source": "    def _init_canonical_atoms(self):\n        \"\"\"Initialize with the original 96 + refined atoms.\"\"\"\n        \n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # DATA FOUNDATIONS (Cyan) \u2014 IDs 1-20\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        \n        # Bits (1-4)\n        self._add(1, \"BitFlag\", [\"binary_expression\"], \"Data Foundations\", \"Bits\", \"atom\",\n                  \"Bit operation with mask\", \"bit operation + constant mask\")\n        self._add(2, \"BitMask\", [\"binary_literal\"], \"Data Foundations\", \"Bits\", \"atom\",\n                  \"Binary literal value\", \"binary literal 0b...\")\n        \n        # Primitives (5-12)\n        self._add(5, \"Boolean\", [\"true\", \"false\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                  \"Boolean true/false value\", \"type bool\")\n        self._add(6, \"Integer\", [\"integer\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                  \"Integer numeric value\", \"integer type\")\n        self._add(7, \"Float\", [\"float\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                  \"Floating point value\", \"float type\")\n        self._add(8, \"StringLiteral\", [\"string\", \"concatenated_string\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                  \"String literal value\", \"string literal\")\n        self._add(9, \"NoneLiteral\", [\"none\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                  \"None/null value\", \"None/null literal\")\n        self._add(10, \"ListLiteral\", [\"list\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                   \"List literal []\", \"list brackets\")\n        self._add(11, \"DictLiteral\", [\"dictionary\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                   \"Dictionary literal {}\", \"dict braces\")\n        self._add(12, \"TupleLiteral\", [\"tuple\"], \"Data Foundations\", \"Primitives\", \"atom\",\n                   \"Tuple literal ()\", \"tuple parens\")\n        \n        # Variables (13-17)\n        self._add(13, \"LocalVar\", [\"identifier\"], \"Data Foundations\", \"Variables\", \"atom\",\n                  \"Local variable reference\", \"local declaration\")\n        self._add(14, \"Parameter\", [\"typed_parameter\", \"default_parameter\"], \"Data Foundations\", \"Variables\", \"atom\",\n                  \"Function parameter\", \"function parameter\")\n        self._add(15, \"InstanceField\", [\"attribute\"], \"Data Foundations\", \"Variables\", \"atom\",\n                  \"Instance field access\", \"this/self field access\")\n        self._add(16, \"IndexAccess\", [\"subscript\"], \"Data Foundations\", \"Variables\", \"atom\",\n                  \"Array/dict index access\", \"bracket access\")\n        self._add(17, \"SliceAccess\", [\"slice\"], \"Data Foundations\", \"Variables\", \"atom\",\n                  \"Slice access [a:b]\", \"slice notation\")\n        \n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # LOGIC & FLOW (Magenta) \u2014 IDs 18-50\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        \n        # Expressions (18-25)\n        self._add(18, \"BinaryExpr\", [\"binary_operator\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Binary operation a + b\", \"arithmetic/bitwise ops\")\n        self._add(19, \"UnaryExpr\", [\"unary_operator\", \"not_operator\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Unary operation -x, !x\", \"unary prefix\")\n        self._add(20, \"ComparisonExpr\", [\"comparison_operator\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Comparison a == b\", \"comparison ops\")\n        self._add(21, \"LogicalExpr\", [\"boolean_operator\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Logical and/or\", \"boolean ops\")\n        self._add(22, \"CallExpr\", [\"call\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Function call f(x)\", \"function call\")\n        self._add(23, \"TernaryExpr\", [\"conditional_expression\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Ternary a ? b : c\", \"conditional expression\")\n        self._add(24, \"Closure\", [\"lambda\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Lambda/closure\", \"lambda keyword\")\n        self._add(25, \"AwaitExpr\", [\"await\"], \"Logic & Flow\", \"Expressions\", \"atom\",\n                  \"Await expression\", \"await keyword\")\n        \n        # Statements (26-35)\n        self._add(26, \"Assignment\", [\"assignment\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Variable assignment\", \"= operator\")\n        self._add(27, \"AugmentedAssignment\", [\"augmented_assignment\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Augmented assignment +=\", \"+= operator\")\n        self._add(28, \"ExpressionStmt\", [\"expression_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Standalone expression\", \"expression as statement\")\n        self._add(29, \"ReturnStmt\", [\"return_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Return statement\", \"return keyword\")\n        self._add(30, \"RaiseStmt\", [\"raise_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Raise/throw exception\", \"raise keyword\")\n        self._add(31, \"AssertStmt\", [\"assert_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Assert statement\", \"assert keyword\")\n        self._add(32, \"PassStmt\", [\"pass_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Pass/no-op statement\", \"pass keyword\")\n        self._add(33, \"BreakStmt\", [\"break_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Break loop\", \"break keyword\")\n        self._add(34, \"ContinueStmt\", [\"continue_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Continue loop\", \"continue keyword\")\n        self._add(35, \"DeleteStmt\", [\"delete_statement\"], \"Logic & Flow\", \"Statements\", \"atom\",\n                  \"Delete statement\", \"del keyword\")\n        \n        # Control Structures (36-45)\n        self._add(36, \"IfBranch\", [\"if_statement\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"If conditional\", \"if/else\")\n        self._add(37, \"ElifBranch\", [\"elif_clause\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Elif branch\", \"elif keyword\")\n        self._add(38, \"ElseBranch\", [\"else_clause\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Else branch\", \"else keyword\")\n        self._add(39, \"LoopFor\", [\"for_statement\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"For loop\", \"for loop\")\n        self._add(40, \"LoopWhile\", [\"while_statement\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"While loop\", \"while loop\")\n        self._add(41, \"TryCatch\", [\"try_statement\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Try/catch block\", \"try/except\")\n        self._add(42, \"ExceptHandler\", [\"except_clause\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Exception handler\", \"except clause\")\n        self._add(43, \"FinallyBlock\", [\"finally_clause\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Finally block\", \"finally clause\")\n        self._add(44, \"ContextManager\", [\"with_statement\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"With context manager\", \"with statement\")\n        self._add(45, \"PatternMatch\", [\"match_statement\", \"case_clause\"], \"Logic & Flow\", \"Control Structures\", \"atom\",\n                  \"Pattern matching\", \"match/case\")\n        \n        # Functions (46-55)\n        self._add(46, \"Function\", [\"function_definition\"], \"Logic & Flow\", \"Functions\", \"molecule\",\n                  \"Function definition\", \"def keyword\")\n        self._add(47, \"AsyncFunction\", [\"async_function_definition\"], \"Logic & Flow\", \"Functions\", \"molecule\",\n                  \"Async function\", \"async def\")\n        self._add(48, \"DecoratedFunction\", [\"decorated_definition\"], \"Logic & Flow\", \"Functions\", \"molecule\",\n                  \"Decorated function\", \"@decorator\")\n        self._add(49, \"Generator\", [\"generator_expression\"], \"Logic & Flow\", \"Functions\", \"molecule\",\n                  \"Generator expression\", \"yield keyword\")\n        self._add(50, \"ListComprehension\", [\"list_comprehension\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"List comprehension\", \"[x for x in]\")\n        self._add(51, \"DictComprehension\", [\"dictionary_comprehension\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"Dict comprehension\", \"{k:v for}\")\n        self._add(52, \"SetComprehension\", [\"set_comprehension\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"Set comprehension\", \"{x for x}\")\n        self._add(53, \"Decorator\", [\"decorator\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"Decorator\", \"@symbol\")\n        self._add(54, \"ParameterList\", [\"parameters\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"Parameter list\", \"(params)\")\n        self._add(55, \"ArgumentList\", [\"argument_list\"], \"Logic & Flow\", \"Functions\", \"atom\",\n                  \"Argument list\", \"(args)\")\n        \n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # ORGANIZATION (Green) \u2014 IDs 56-75\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        \n        # Aggregates (56-65)\n        self._add(56, \"Class\", [\"class_definition\"], \"Organization\", \"Aggregates\", \"molecule\",\n                  \"Class definition\", \"class keyword\")\n        self._add(57, \"ValueObject\", [], \"Organization\", \"Aggregates\", \"molecule\",\n                  \"Immutable value type\", \"class immutable + no id\")\n        self._add(58, \"Entity\", [], \"Organization\", \"Aggregates\", \"molecule\",\n                  \"Entity with identity\", \"class with id field\")\n        self._add(59, \"AggregateRoot\", [], \"Organization\", \"Aggregates\", \"organelle\",\n                  \"Aggregate root\", \"raises domain events\")\n        self._add(60, \"DTO\", [], \"Organization\", \"Aggregates\", \"molecule\",\n                  \"Data transfer object\", \"data-only class\")\n        self._add(61, \"Factory\", [], \"Organization\", \"Aggregates\", \"molecule\",\n                  \"Factory class/method\", \"static create method\")\n        \n        # Modules (66-75)\n        self._add(66, \"Import\", [\"import_statement\"], \"Organization\", \"Modules\", \"atom\",\n                  \"Import statement\", \"import keyword\")\n        self._add(67, \"ImportFrom\", [\"import_from_statement\"], \"Organization\", \"Modules\", \"atom\",\n                  \"From import\", \"from x import y\")\n        self._add(68, \"ImportAlias\", [\"aliased_import\"], \"Organization\", \"Modules\", \"atom\",\n                  \"Import alias\", \"import as\")\n        self._add(69, \"DottedName\", [\"dotted_name\"], \"Organization\", \"Modules\", \"atom\",\n                  \"Dotted module path\", \"a.b.c\")\n        self._add(70, \"Comment\", [\"comment\"], \"Organization\", \"Files\", \"atom\",\n                  \"Code comment\", \"# or //\")\n        \n        # Types (71-75)\n        self._add(71, \"TypeAnnotation\", [\"type\"], \"Organization\", \"Types\", \"atom\",\n                  \"Type annotation\", \": Type\")\n        self._add(72, \"GenericType\", [\"generic_type\"], \"Organization\", \"Types\", \"atom\",\n                  \"Generic type\", \"List[T]\")\n        self._add(73, \"UnionType\", [\"union_type\"], \"Organization\", \"Types\", \"atom\",\n                  \"Union type\", \"A | B\")\n        self._add(74, \"KeywordArg\", [\"keyword_argument\"], \"Organization\", \"Types\", \"atom\",\n                  \"Keyword argument\", \"key=value\")\n        \n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        # EXECUTION (Amber) \u2014 IDs 76-96\n        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n        \n        # (These are typically organelles - inferred from patterns)\n        self._add(76, \"MainEntry\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Main entry point\", \"if __name__\")\n        self._add(77, \"APIHandler\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"API route handler\", \"@app.get/post\")\n        self._add(78, \"CommandHandler\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Command handler (CQRS)\", \"handles *Command\")\n        self._add(79, \"QueryHandler\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Query handler (CQRS)\", \"handles *Query\")\n        self._add(80, \"EventHandler\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Event handler\", \"@Subscribe\")\n        self._add(81, \"Middleware\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Middleware function\", \"calls next()\")\n        self._add(82, \"Validator\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Validation function\", \"validate* + throws\")\n        self._add(83, \"Repository\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Repository pattern\", \"save/find methods\")\n        self._add(84, \"UseCase\", [], \"Execution\", \"Executables\", \"organelle\",\n                  \"Use case handler\", \"single execute method\")",
      "complexity": 0,
      "lines_of_code": 193,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry._add",
      "name": "AtomRegistry._add",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 246,
      "end_line": 258,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "id",
          "type": "int"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "ast_types",
          "type": "List[str]"
        },
        {
          "name": "continent",
          "type": "str"
        },
        {
          "name": "fundamental",
          "type": "str"
        },
        {
          "name": "level",
          "type": "str"
        },
        {
          "name": "description",
          "type": "str"
        },
        {
          "name": "detection_rule",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Add an atom to the registry.",
      "signature": "def _add(self, id: int, name: str, ast_types: List[str], continent: str,",
      "body_source": "    def _add(self, id: int, name: str, ast_types: List[str], continent: str, \n             fundamental: str, level: str, description: str, detection_rule: str):\n        \"\"\"Add an atom to the registry.\"\"\"\n        atom = AtomDefinition(\n            id=id, name=name, ast_types=ast_types, continent=continent,\n            fundamental=fundamental, level=level, description=description,\n            detection_rule=detection_rule, source=\"original\"\n        )\n        self.atoms[id] = atom\n        \n        # Map AST types to this atom\n        for ast_type in ast_types:\n            self.ast_type_map[ast_type] = id",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.add_discovery",
      "name": "AtomRegistry.add_discovery",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 260,
      "end_line": 282,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "ast_types",
          "type": "List[str]"
        },
        {
          "name": "continent",
          "type": "str"
        },
        {
          "name": "fundamental",
          "type": "str"
        },
        {
          "name": "level",
          "type": "str"
        },
        {
          "name": "description",
          "type": "str"
        },
        {
          "name": "detection_rule",
          "type": "str"
        },
        {
          "name": "source_repo",
          "type": "str"
        }
      ],
      "return_type": "int",
      "base_classes": [],
      "decorators": [],
      "docstring": "Add a newly discovered atom to the registry.",
      "signature": "def add_discovery(self, name: str, ast_types: List[str], continent: str,",
      "body_source": "    def add_discovery(self, name: str, ast_types: List[str], continent: str,\n                      fundamental: str, level: str, description: str,\n                      detection_rule: str, source_repo: str) -> int:\n        \"\"\"Add a newly discovered atom to the registry.\"\"\"\n        atom = AtomDefinition(\n            id=self.next_id,\n            name=name,\n            ast_types=ast_types,\n            continent=continent,\n            fundamental=fundamental,\n            level=level,\n            description=description,\n            detection_rule=detection_rule,\n            source=source_repo,\n            discovered_at=datetime.now().isoformat()\n        )\n        self.atoms[self.next_id] = atom\n        \n        for ast_type in ast_types:\n            self.ast_type_map[ast_type] = self.next_id\n        \n        self.next_id += 1\n        return atom.id",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.get_by_ast_type",
      "name": "AtomRegistry.get_by_ast_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 284,
      "end_line": 288,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "ast_type",
          "type": "str"
        }
      ],
      "return_type": "Optional[AtomDefinition]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get atom definition by AST node type.",
      "signature": "def get_by_ast_type(self, ast_type: str) -> Optional[AtomDefinition]:",
      "body_source": "    def get_by_ast_type(self, ast_type: str) -> Optional[AtomDefinition]:\n        \"\"\"Get atom definition by AST node type.\"\"\"\n        if ast_type in self.ast_type_map:\n            return self.atoms[self.ast_type_map[ast_type]]\n        return None",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.get_stats",
      "name": "AtomRegistry.get_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 290,
      "end_line": 314,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get registry statistics.",
      "signature": "def get_stats(self) -> Dict:",
      "body_source": "    def get_stats(self) -> Dict:\n        \"\"\"Get registry statistics.\"\"\"\n        by_continent = {}\n        by_fundamental = {}\n        by_level = {}\n        by_source = {\"original\": 0, \"discovered\": 0}\n        \n        for atom in self.atoms.values():\n            by_continent[atom.continent] = by_continent.get(atom.continent, 0) + 1\n            by_fundamental[atom.fundamental] = by_fundamental.get(atom.fundamental, 0) + 1\n            by_level[atom.level] = by_level.get(atom.level, 0) + 1\n            if atom.source == \"original\":\n                by_source[\"original\"] += 1\n            else:\n                by_source[\"discovered\"] += 1\n        \n        return {\n            \"total_atoms\": len(self.atoms),\n            \"ast_types_mapped\": len(self.ast_type_map),\n            \"by_continent\": by_continent,\n            \"by_fundamental\": by_fundamental,\n            \"by_level\": by_level,\n            \"by_source\": by_source,\n            \"next_id\": self.next_id,\n        }",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.export_canon",
      "name": "AtomRegistry.export_canon",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 316,
      "end_line": 339,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export the canonical registry to JSON.",
      "signature": "def export_canon(self, path: str):",
      "body_source": "    def export_canon(self, path: str):\n        \"\"\"Export the canonical registry to JSON.\"\"\"\n        data = {\n            \"version\": \"1.0\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"stats\": self.get_stats(),\n            \"atoms\": {\n                str(id): {\n                    \"id\": atom.id,\n                    \"name\": atom.name,\n                    \"ast_types\": atom.ast_types,\n                    \"continent\": atom.continent,\n                    \"fundamental\": atom.fundamental,\n                    \"level\": atom.level,\n                    \"description\": atom.description,\n                    \"detection_rule\": atom.detection_rule,\n                    \"source\": atom.source,\n                    \"discovered_at\": atom.discovered_at,\n                } for id, atom in self.atoms.items()\n            }\n        }\n        \n        with open(path, 'w') as f:\n            json.dump(data, f, indent=2)",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py:AtomRegistry.print_summary",
      "name": "AtomRegistry.print_summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_registry.py",
      "start_line": 341,
      "end_line": 370,
      "role": "Utility",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Print a summary of the registry.",
      "signature": "def print_summary(self):",
      "body_source": "    def print_summary(self):\n        \"\"\"Print a summary of the registry.\"\"\"\n        stats = self.get_stats()\n        \n        print(\"=\" * 70)\n        print(\"\ud83e\uddec ATOM REGISTRY \u2014 Canonical Taxonomy\")\n        print(\"=\" * 70)\n        print()\n        print(f\"\ud83d\udcca Total Atoms: {stats['total_atoms']}\")\n        print(f\"\ud83d\udcca AST Types Mapped: {stats['ast_types_mapped']}\")\n        print(f\"\ud83d\udcca Original (96 base): {stats['by_source']['original']}\")\n        print(f\"\ud83d\udcca Discovered: {stats['by_source']['discovered']}\")\n        print()\n        \n        print(\"By Continent:\")\n        for continent, count in sorted(stats['by_continent'].items(), key=lambda x: -x[1]):\n            bar = \"\u2588\" * min(count // 2, 30)\n            print(f\"  {continent:20} {count:3} {bar}\")\n        print()\n        \n        print(\"By Fundamental:\")\n        for fund, count in sorted(stats['by_fundamental'].items(), key=lambda x: -x[1]):\n            bar = \"\u2588\" * min(count, 30)\n            print(f\"  {fund:20} {count:3} {bar}\")\n        print()\n        \n        print(\"By Level:\")\n        for level, count in sorted(stats['by_level'].items(), key=lambda x: -x[1]):\n            bar = \"\u2588\" * min(count, 30)\n            print(f\"  {level:12} {count:3} {bar}\")",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_registry.py:get_purpose",
      "name": "get_purpose",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_registry.py",
      "start_line": 299,
      "end_line": 321,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "element_type",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the purpose of any code element type.\n\nArgs:\n    element_type: Role name, atom name, or layer name\n\nReturns:\n    Purpose string describing why this element exists",
      "signature": "def get_purpose(element_type: str) -> str:",
      "body_source": "def get_purpose(element_type: str) -> str:\n    \"\"\"\n    Get the purpose of any code element type.\n    \n    Args:\n        element_type: Role name, atom name, or layer name\n    \n    Returns:\n        Purpose string describing why this element exists\n    \"\"\"\n    # Check roles first\n    if element_type in ROLE_PURPOSE:\n        return ROLE_PURPOSE[element_type]\n    \n    # Check atoms\n    if element_type in ATOM_PURPOSE:\n        return ATOM_PURPOSE[element_type]\n    \n    # Check layers\n    if element_type.lower() in LAYER_PURPOSE:\n        return LAYER_PURPOSE[element_type.lower()]\n    \n    return f\"Purpose not defined for '{element_type}'\"",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_registry.py:list_all_purposes",
      "name": "list_all_purposes",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purpose_registry.py",
      "start_line": 324,
      "end_line": 332,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Return all purposes organized by category",
      "signature": "def list_all_purposes() -> dict:",
      "body_source": "def list_all_purposes() -> dict:\n    \"\"\"Return all purposes organized by category\"\"\"\n    return {\n        \"roles\": ROLE_PURPOSE,\n        \"atoms\": ATOM_PURPOSE,\n        \"layers\": LAYER_PURPOSE,\n        \"relationships\": RELATIONSHIP_PURPOSE,\n        \"total_defined\": len(ROLE_PURPOSE) + len(ATOM_PURPOSE) + len(LAYER_PURPOSE) + len(RELATIONSHIP_PURPOSE)\n    }",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:ResolvedDependency",
      "name": "ResolvedDependency",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 12,
      "end_line": 12,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ResolvedDependency:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_posix",
      "name": "_posix",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 21,
      "end_line": 22,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "path",
          "type": "Path"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _posix(path: Path) -> str:",
      "body_source": "def _posix(path: Path) -> str:\n    return path.as_posix()",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_try_rel",
      "name": "_try_rel",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 25,
      "end_line": 29,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "repo_root",
          "type": "Path"
        },
        {
          "name": "file_path",
          "type": "Path"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _try_rel(repo_root: Path, file_path: Path) -> str:",
      "body_source": "def _try_rel(repo_root: Path, file_path: Path) -> str:\n    try:\n        return _posix(file_path.relative_to(repo_root))\n    except ValueError:\n        return _posix(file_path)",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_python_stdlib_roots",
      "name": "_python_stdlib_roots",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 32,
      "end_line": 72,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _python_stdlib_roots() -> set[str]:",
      "body_source": "def _python_stdlib_roots() -> set[str]:\n    stdlib = getattr(sys, \"stdlib_module_names\", None)\n    if isinstance(stdlib, (set, frozenset)):\n        return set(stdlib)\n    return {\n        \"__future__\",\n        \"abc\",\n        \"argparse\",\n        \"asyncio\",\n        \"base64\",\n        \"collections\",\n        \"concurrent\",\n        \"contextlib\",\n        \"csv\",\n        \"dataclasses\",\n        \"datetime\",\n        \"functools\",\n        \"hashlib\",\n        \"http\",\n        \"importlib\",\n        \"inspect\",\n        \"io\",\n        \"itertools\",\n        \"json\",\n        \"logging\",\n        \"math\",\n        \"os\",\n        \"pathlib\",\n        \"random\",\n        \"re\",\n        \"sqlite3\",\n        \"statistics\",\n        \"string\",\n        \"subprocess\",\n        \"sys\",\n        \"tempfile\",\n        \"threading\",\n        \"time\",\n        \"typing\",\n        \"uuid\",\n    }",
      "complexity": 0,
      "lines_of_code": 40,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_python_candidate_roots",
      "name": "_python_candidate_roots",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 115,
      "end_line": 121,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "repo_root",
          "type": "Path"
        }
      ],
      "return_type": "list[Path]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _python_candidate_roots(repo_root: Path) -> list[Path]:",
      "body_source": "def _python_candidate_roots(repo_root: Path) -> list[Path]:\n    roots = [repo_root]\n    for name in (\"src\", \"app\", \"lib\"):\n        p = repo_root / name\n        if p.is_dir():\n            roots.append(p)\n    return roots",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_python_module_from_rel",
      "name": "_python_module_from_rel",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 124,
      "end_line": 130,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "rel",
          "type": "Path"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _python_module_from_rel(rel: Path) -> str:",
      "body_source": "def _python_module_from_rel(rel: Path) -> str:\n    if rel.name == \"__init__.py\":\n        rel = rel.parent\n    else:\n        rel = rel.with_suffix(\"\")\n    mod = _posix(rel).replace(\"/\", \".\")\n    return mod.strip(\".\")",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_build_python_module_index",
      "name": "_build_python_module_index",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 133,
      "end_line": 157,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "repo_root",
          "type": "Path"
        },
        {
          "name": "file_paths",
          "type": "Iterable[Path]"
        }
      ],
      "return_type": "tuple[dict[str, str], dict[str, str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build module->file and file->module maps for Python files.",
      "signature": "def _build_python_module_index(repo_root: Path, file_paths: Iterable[Path]) -> tuple[dict[str, str], dict[str, str]]:",
      "body_source": "def _build_python_module_index(repo_root: Path, file_paths: Iterable[Path]) -> tuple[dict[str, str], dict[str, str]]:\n    \"\"\"Build module->file and file->module maps for Python files.\"\"\"\n    module_to_file: dict[str, str] = {}\n    file_to_module: dict[str, str] = {}\n    roots = _python_candidate_roots(repo_root)\n\n    for p in file_paths:\n        if p.suffix != \".py\":\n            continue\n        for root in roots:\n            try:\n                rel = p.relative_to(root)\n            except ValueError:\n                continue\n            mod = _python_module_from_rel(rel)\n            if not mod:\n                continue\n            rel_to_repo = _try_rel(repo_root, p)\n            module_to_file.setdefault(mod, rel_to_repo)\n            # Prefer the shortest module for a given file (avoids \"src.\" prefix when present)\n            prev = file_to_module.get(rel_to_repo)\n            if prev is None or len(mod.split(\".\")) < len(prev.split(\".\")):\n                file_to_module[rel_to_repo] = mod\n\n    return module_to_file, file_to_module",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_python_resolve_relative",
      "name": "_python_resolve_relative",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 160,
      "end_line": 187,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "file_module",
          "type": "str | None"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _python_resolve_relative(",
      "body_source": "def _python_resolve_relative(\n    file_module: str | None,\n    *,\n    level: int,\n    target_module: str,\n    is_init: bool,\n) -> str:\n    if not level:\n        return target_module\n    if not file_module:\n        return target_module\n\n    # Determine the current package: for a module a.b.c, package is a.b\n    if is_init:\n        file_package = file_module\n    elif \".\" in file_module:\n        file_package = file_module.rsplit(\".\", 1)[0]\n    else:\n        file_package = \"\"\n\n    base_parts = file_package.split(\".\") if file_package else []\n    up = max(0, level - 1)\n    if up:\n        base_parts = base_parts[: max(0, len(base_parts) - up)]\n    base = \".\".join([p for p in base_parts if p])\n    if target_module:\n        return f\"{base}.{target_module}\".strip(\".\") if base else target_module\n    return base",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_npm_package_name",
      "name": "_npm_package_name",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 190,
      "end_line": 194,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "spec",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _npm_package_name(spec: str) -> str:",
      "body_source": "def _npm_package_name(spec: str) -> str:\n    if spec.startswith(\"@\"):\n        parts = spec.split(\"/\")\n        return \"/\".join(parts[:2]) if len(parts) >= 2 else spec\n    return spec.split(\"/\")[0]",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:_resolve_js_relative",
      "name": "_resolve_js_relative",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 197,
      "end_line": 216,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "repo_root",
          "type": "Path"
        },
        {
          "name": "src_file",
          "type": "Path"
        },
        {
          "name": "spec",
          "type": "str"
        }
      ],
      "return_type": "str | None",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _resolve_js_relative(repo_root: Path, src_file: Path, spec: str) -> str | None:",
      "body_source": "def _resolve_js_relative(repo_root: Path, src_file: Path, spec: str) -> str | None:\n    base = src_file.parent.resolve()\n    candidate = (base / spec).resolve()\n    candidates: list[Path] = []\n\n    if candidate.suffix:\n        candidates.append(candidate)\n    else:\n        for ext in (\".ts\", \".tsx\", \".js\", \".jsx\", \".mjs\", \".cjs\", \".json\"):\n            candidates.append(candidate.with_suffix(ext))\n        candidates.append(candidate / \"index.ts\")\n        candidates.append(candidate / \"index.tsx\")\n        candidates.append(candidate / \"index.js\")\n        candidates.append(candidate / \"index.jsx\")\n\n    for c in candidates:\n        if c.exists() and c.is_file():\n            rel = _try_rel(repo_root, c)\n            return rel if not rel.startswith(\"..\") else None\n    return None",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:DependencyAnalyzer",
      "name": "DependencyAnalyzer",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 219,
      "end_line": 219,
      "role": "DTO",
      "role_confidence": 95.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class DependencyAnalyzer:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py:DependencyAnalyzer.analyze_repository",
      "name": "DependencyAnalyzer.analyze_repository",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/dependency_analyzer.py",
      "start_line": 222,
      "end_line": 342,
      "role": "Repository",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_root",
          "type": "str"
        },
        {
          "name": "analysis_results",
          "type": "list[dict[str, Any]]"
        }
      ],
      "return_type": "dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def analyze_repository(self, repo_root: str, analysis_results: list[dict[str, Any]]) -> dict[str, Any]:",
      "body_source": "    def analyze_repository(self, repo_root: str, analysis_results: list[dict[str, Any]]) -> dict[str, Any]:\n        root = Path(repo_root).resolve()\n\n        file_paths = []\n        for r in analysis_results:\n            fp = r.get(\"file_path\")\n            if fp:\n                file_paths.append(Path(fp).resolve())\n\n        py_module_to_file, py_file_to_module = _build_python_module_index(root, file_paths)\n        py_stdlib = _python_stdlib_roots()\n\n        external_packages: Counter[str] = Counter()\n        stdlib_deps: Counter[str] = Counter()\n        internal_edges: Counter[tuple[str, str]] = Counter()\n        unknown_deps: int = 0\n\n        for r in analysis_results:\n            fp = r.get(\"file_path\")\n            if not fp:\n                continue\n\n            src_file = Path(fp).resolve()\n            src_rel = _try_rel(root, src_file)\n            language = r.get(\"language\", \"unknown\")\n            raw_imports = r.get(\"raw_imports\") or []\n\n            deps = {\"internal\": [], \"external\": [], \"stdlib\": [], \"unknown\": []}\n\n            for imp in raw_imports:\n                kind = str(imp.get(\"kind\") or \"import\")\n                target = str(imp.get(\"target\") or \"\").strip()\n                line = int(imp.get(\"line\") or 0)\n\n                if not target and language != \"python\":\n                    continue\n\n                if language == \"python\":\n                    level = int(imp.get(\"level\") or 0)\n                    file_module = py_file_to_module.get(src_rel)\n                    resolved_target = _python_resolve_relative(\n                        file_module,\n                        level=level,\n                        target_module=target,\n                        is_init=src_file.name == \"__init__.py\",\n                    )\n                    top = resolved_target.split(\".\", 1)[0] if resolved_target else \"\"\n\n                    if top in py_stdlib:\n                        deps[\"stdlib\"].append(\n                            ResolvedDependency(kind, resolved_target, line, \"stdlib\", package=top).__dict__\n                        )\n                        stdlib_deps[top] += 1\n                        continue\n\n                    internal_file = py_module_to_file.get(resolved_target)\n                    if internal_file:\n                        deps[\"internal\"].append(\n                            ResolvedDependency(kind, resolved_target, line, \"internal\", resolved_file=internal_file).__dict__\n                        )\n                        internal_edges[(src_rel, internal_file)] += 1\n                        continue\n\n                    if resolved_target and any(\n                        m == resolved_target or m.startswith(resolved_target + \".\") for m in py_module_to_file\n                    ):\n                        deps[\"internal\"].append(\n                            ResolvedDependency(kind, resolved_target, line, \"internal\", resolved_file=None).__dict__\n                        )\n                        continue\n\n                    pkg = top if top else None\n                    deps[\"external\"].append(\n                        ResolvedDependency(kind, resolved_target, line, \"external\", package=pkg).__dict__\n                    )\n                    if pkg:\n                        external_packages[pkg] += 1\n                    continue\n\n                if language in {\"javascript\", \"typescript\"}:\n                    if target.startswith((\".\", \"/\")):\n                        resolved_file = _resolve_js_relative(root, src_file, target)\n                        deps[\"internal\"].append(\n                            ResolvedDependency(kind, target, line, \"internal\", resolved_file=resolved_file).__dict__\n                        )\n                        if resolved_file:\n                            internal_edges[(src_rel, resolved_file)] += 1\n                        continue\n\n                    pkg = _npm_package_name(target)\n                    if pkg in NODE_BUILTINS:\n                        deps[\"stdlib\"].append(\n                            ResolvedDependency(kind, target, line, \"stdlib\", package=pkg).__dict__\n                        )\n                        stdlib_deps[pkg] += 1\n                        continue\n\n                    deps[\"external\"].append(\n                        ResolvedDependency(kind, target, line, \"external\", package=pkg).__dict__\n                    )\n                    external_packages[pkg] += 1\n                    continue\n\n                # Best-effort classification for other languages\n                deps[\"unknown\"].append(ResolvedDependency(kind, target, line, \"unknown\").__dict__)\n                unknown_deps += 1\n\n            r[\"dependencies\"] = deps\n\n        top_external = [{\"package\": p, \"count\": c} for p, c in external_packages.most_common(50)]\n        top_stdlib = [{\"package\": p, \"count\": c} for p, c in stdlib_deps.most_common(50)]\n        top_internal_edges = [\n            {\"from\": a, \"to\": b, \"count\": c} for (a, b), c in internal_edges.most_common(200)\n        ]\n\n        return {\n            \"external_packages\": top_external,\n            \"stdlib_packages\": top_stdlib,\n            \"internal_edges\": top_internal_edges,\n            \"unknown_dependency_count\": unknown_deps,\n        }",
      "complexity": 0,
      "lines_of_code": 120,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py:UniversalPatternDetector",
      "name": "UniversalPatternDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py",
      "start_line": 20,
      "end_line": 20,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class UniversalPatternDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py:UniversalPatternDetector.__init__",
      "name": "UniversalPatternDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py",
      "start_line": 23,
      "end_line": 29,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.tree_sitter_engine = TreeSitterUniversalEngine()\n        self.particle_classifier = ParticleClassifier()\n        self.stats_generator = StatsGenerator()\n        self.dependency_analyzer = DependencyAnalyzer()\n        self.report_generator = ReportGenerator()\n        self.god_class_detector = GodClassDetectorLite()",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py:UniversalPatternDetector.analyze_repository",
      "name": "UniversalPatternDetector.analyze_repository",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py",
      "start_line": 31,
      "end_line": 99,
      "role": "Repository",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze entire repository for universal patterns",
      "signature": "def analyze_repository(self, repo_path: str, *, output_dir: str | Path | None = None) -> Dict[str, Any]:",
      "body_source": "    def analyze_repository(self, repo_path: str, *, output_dir: str | Path | None = None) -> Dict[str, Any]:\n        \"\"\"Analyze entire repository for universal patterns\"\"\"\n        print(f\"\ud83d\udd2c Analyzing repository: {repo_path}\")\n\n        # Step 1: Parse all files with Tree-sitter\n        print(\"\ud83d\udcc2 Parsing files with universal engine...\")\n        analysis_results = self.tree_sitter_engine.analyze_directory(repo_path)\n        depth_summary = getattr(self.tree_sitter_engine, \"depth_summary\", {})\n        if depth_summary.get(\"files_measured\", 0) > 0:\n            print(\n                f\"\ud83e\udded Depth metrics: max AST depth {depth_summary.get('max_ast_depth', 0)} \"\n                f\"across {depth_summary.get('files_measured', 0)} python files\"\n            )\n\n        # Step 1.5: Extract dependencies (internal/external/stdlib)\n        print(\"\ud83d\udd17 Analyzing dependencies...\")\n        dependency_summary = self.dependency_analyzer.analyze_repository(repo_path, analysis_results)\n\n        # Step 2: Classify particles with RPBL scores\n        print(\"\ud83d\udd2c Classifying particles with RPBL scores...\")\n        for result in analysis_results:\n            classified_particles = []\n            for particle in result.get('particles', []):\n                classified = self.particle_classifier.classify_particle(particle)\n                classified_particles.append(classified)\n            result['particles'] = classified_particles\n\n        # Step 3: Generate comprehensive statistics\n        print(\"\ud83d\udcca Generating comprehensive statistics...\")\n        comprehensive_results = self.stats_generator.generate_comprehensive_stats(analysis_results)\n        if depth_summary:\n            comprehensive_results['depth_metrics'] = depth_summary\n        comprehensive_results['dependencies'] = dependency_summary\n        \n        # Step 3.5: God Class Detection\n        print(\"\u2622\ufe0f  Scanning for Antimatter (God Classes)...\")\n        god_results = self.god_class_detector.analyze_repository(repo_path)\n        # Convert dataclass instances to dicts for JSON serialization\n        from dataclasses import asdict\n        comprehensive_results['god_classes'] = [asdict(gc) for gc in god_results['god_classes']]\n        comprehensive_results['god_class_summary'] = god_results['summary']\n        \n        # Merge God Class recommendations\n        if 'recommendations' not in comprehensive_results: comprehensive_results['recommendations'] = {}\n        comprehensive_results['recommendations']['antimatter'] = god_results['recommendations']\n\n        # Step 4: Save all results\n        resolved_output_dir = Path(output_dir) if output_dir is not None else (Path(__file__).parent.parent / \"output\")\n        output_dir = resolved_output_dir.resolve()\n        output_files = self.stats_generator.save_results(output_dir)\n        report_files = self.report_generator.generate(\n            repo_root=repo_path,\n            analysis_results=analysis_results,\n            comprehensive_results=comprehensive_results,\n            output_dir=output_dir,\n        )\n        output_files.update(report_files)\n\n        print(f\"\u2705 Analysis complete! Results saved to: {output_dir}\")\n\n        return {\n            'comprehensive_results': comprehensive_results,\n            'output_files': output_files,\n            'summary': {\n                'files_processed': len(analysis_results),\n                'particles_detected': len(comprehensive_results['particles']),\n                'output_formats': ['JSON', 'TXT', 'CSV']\n            }\n        }",
      "complexity": 0,
      "lines_of_code": 68,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py:UniversalPatternDetector.get_quick_stats",
      "name": "UniversalPatternDetector.get_quick_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/universal_detector.py",
      "start_line": 101,
      "end_line": 115,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get quick statistics about detector capabilities",
      "signature": "def get_quick_stats(self) -> Dict[str, Any]:",
      "body_source": "    def get_quick_stats(self) -> Dict[str, Any]:\n        \"\"\"Get quick statistics about detector capabilities\"\"\"\n        particle_types = self.particle_classifier.get_all_particle_types()\n        supported_languages = self.tree_sitter_engine.supported_languages\n\n        return {\n            'detector_version': 'V12.1',\n            'supported_languages': list(supported_languages.values()),\n            'number_of_languages': len(supported_languages),\n            'particle_types_supported': len(particle_types),\n            'universal_touchpoints': 22,\n            'rpbl_dimensions': 4,\n            'output_formats': ['JSON', 'TXT', 'CSV'],\n            'performance_target': '1205+ files/second'\n        }",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassification",
      "name": "AtomClassification",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 16,
      "end_line": 16,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class AtomClassification:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier",
      "name": "AtomClassifier",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 25,
      "end_line": 25,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class AtomClassifier:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier.__init__",
      "name": "AtomClassifier.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 28,
      "end_line": 41,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "atoms_path",
          "type": "str",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load atom definitions from JSON.",
      "signature": "def __init__(self, atoms_path: str = None):",
      "body_source": "    def __init__(self, atoms_path: str = None):\n        \"\"\"Load atom definitions from JSON.\"\"\"\n        if atoms_path is None:\n            atoms_path = Path(__file__).parent.parent / \"patterns\" / \"atoms.json\"\n        \n        with open(atoms_path, \"r\", encoding=\"utf-8\") as f:\n            self.taxonomy = json.load(f)\n        \n        \n        # Build lookup structures\n        self._build_lookups()\n        \n        # Load pattern repository (Learned Patterns)\n        self.repo = get_pattern_repository()",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier._build_lookups",
      "name": "AtomClassifier._build_lookups",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 43,
      "end_line": 80,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build fast lookup structures from taxonomy.",
      "signature": "def _build_lookups(self):",
      "body_source": "    def _build_lookups(self):\n        \"\"\"Build fast lookup structures from taxonomy.\"\"\"\n        # atom_id -> (phase, family, atom_info)\n        self.atoms_by_id = {}\n        \n        # subtype name -> atom_id\n        self.atoms_by_subtype = {}\n        \n        for phase_name, phase in self.taxonomy.get(\"phases\", {}).items():\n            for family_name, family in phase.get(\"families\", {}).items():\n                for atom in family.get(\"atoms\", []):\n                    atom_id = atom[\"id\"]\n                    subtype = atom[\"name\"]\n                    \n                    if atom_id not in self.atoms_by_id:\n                        self.atoms_by_id[atom_id] = (phase_name, family_name, atom)\n                    \n                    self.atoms_by_subtype[subtype.lower()] = atom_id\n        \n        # Add common aliases\n        self.atoms_by_subtype['configuration'] = self.atoms_by_subtype.get('configvalue', 'ORG.CFG.O')\n        self.atoms_by_subtype['adapter'] = self.atoms_by_subtype.get('adapter', 'ORG.SVC.M')\n        self.atoms_by_subtype['observer'] = self.atoms_by_subtype.get('eventhandler', 'EXE.HDL.O')\n        self.atoms_by_subtype['specification']\n        self.atoms_by_subtype['query']\n        self.atoms_by_subtype['entity'] = 'ORG.AGG.M'  # LEARNED: Entity data classes = self.atoms_by_subtype.get('query', 'LOG.FNC.M')  # LEARNED = self.atoms_by_subtype.get('policy', 'LOG.FNC.M')\n        \n        # Compile semantic patterns\n        self.semantic_patterns = []\n        for pattern in self.taxonomy.get(\"semantic_patterns\", {}).get(\"patterns\", []):\n            try:\n                regex = re.compile(pattern[\"pattern\"])\n                self.semantic_patterns.append((regex, pattern[\"atom\"], pattern[\"subtype\"]))\n            except re.error:\n                pass\n        \n        # Tree-sitter mappings\n        self.ts_mappings = self.taxonomy.get(\"tree_sitter_mappings\", {})",
      "complexity": 0,
      "lines_of_code": 37,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier.classify_by_name",
      "name": "AtomClassifier.classify_by_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 82,
      "end_line": 218,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "ast_node_type",
          "type": "str",
          "default": "None"
        },
        {
          "name": "language",
          "type": "str",
          "default": "None"
        },
        {
          "name": "file_path",
          "type": "str",
          "default": "None"
        }
      ],
      "return_type": "AtomClassification",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a code entity by its name and optionally AST info.\n\nArgs:\n    name: Function/class/variable name\n    ast_node_type: Tree-sitter AST node type (e.g., \"function_declaration\")\n    language: Language (e.g., \"javascript\", \"python\")\n    file_path: Optional file path for context-aware classification",
      "signature": "def classify_by_name(self, name: str, ast_node_type: str = None, language: str = None, file_path: str = None) -> AtomClassification:",
      "body_source": "    def classify_by_name(self, name: str, ast_node_type: str = None, language: str = None, file_path: str = None) -> AtomClassification:\n        \"\"\"\n        Classify a code entity by its name and optionally AST info.\n        \n        Args:\n            name: Function/class/variable name\n            ast_node_type: Tree-sitter AST node type (e.g., \"function_declaration\")\n            language: Language (e.g., \"javascript\", \"python\")\n            file_path: Optional file path for context-aware classification\n        \"\"\"\n        confidence = 0.5  # Default confidence\n        \n        # 0. Context Awareness (Path Patterns) - HIGHEST PRIORITY\n        # Learned from feedback loop: File path is strongest signal for tests/services\n        if language and language == \"python\" and self.repo and file_path:\n             path_patterns = self.repo.get_path_patterns()\n             for pattern, (role, confidence) in path_patterns.items():\n                 # Check if path contains the pattern (e.g. \"tests/\")\n                 if pattern in file_path:\n                     # Map role to Atom ID (with alias for Test -> testdouble)\n                     key = role.lower()\n                     if key == 'test': \n                         key = 'testdouble'  # Alias: generic Test -> TestDouble atom\n                     atom_id = self.atoms_by_subtype.get(key)\n                     if atom_id:\n                         return AtomClassification(\n                             phase=self.atoms_by_id[atom_id][0],\n                             family=self.atoms_by_id[atom_id][1],\n                             atom_id=atom_id, \n                             subtype=role,\n                             confidence=confidence / 100.0\n                         )\n\n        # 1. Try Tree-sitter mapping first (highest confidence for structure)\n        atom_id = None\n        if language and ast_node_type:\n            lang_mappings = self.ts_mappings.get(language, {})\n            if ast_node_type in lang_mappings:\n                atom_id = lang_mappings[ast_node_type]\n                confidence = 0.8\n        \n        # 2. Try semantic patterns on name (can override or refine)\n        subtype = None\n        for regex, pattern_atom, pattern_subtype in self.semantic_patterns:\n            if regex.match(name):\n                atom_id = pattern_atom\n                subtype = pattern_subtype\n                confidence = max(confidence, 0.75)\n                break\n        \n        # 2.5. Try PREFIX patterns from pattern repository (CRITICAL FIX)\n        if self.repo:\n            prefix_patterns = self.repo.get_prefix_patterns()\n            for pattern, (role, conf) in prefix_patterns.items():\n                if name.startswith(pattern) or name.lower().startswith(pattern.lower()):\n                    role_key = role.lower()\n                    if role_key == 'test':\n                        role_key = 'testdouble'\n                    atom_id = self.atoms_by_subtype.get(role_key)\n                    if atom_id:\n                        return AtomClassification(\n                            phase=self.atoms_by_id[atom_id][0],\n                            family=self.atoms_by_id[atom_id][1],\n                            atom_id=atom_id,\n                            subtype=role,\n                            confidence=conf / 100.0\n                        )\n        \n        # 2.6. Try SUFFIX patterns from pattern repository\n        if self.repo:\n            suffix_patterns = self.repo.get_suffix_patterns()\n            for pattern, (role, conf) in suffix_patterns.items():\n                if name.endswith(pattern):\n                    role_key = role.lower()\n                    atom_id = self.atoms_by_subtype.get(role_key)\n                    if atom_id:\n                        return AtomClassification(\n                            phase=self.atoms_by_id[atom_id][0],\n                            family=self.atoms_by_id[atom_id][1],\n                            atom_id=atom_id,\n                            subtype=role,\n                            confidence=conf / 100.0\n                        )\n\n        \n        # 3. Fall back to generic classification\n        if atom_id is None:\n            # Infer from name patterns\n            name_lower = name.lower()\n            \n            if any(x in name_lower for x in [\"handler\", \"controller\", \"endpoint\", \"route\"]):\n                atom_id = \"EXE.HDL.O\"\n                subtype = \"APIHandler\"\n            elif any(x in name_lower for x in [\"service\", \"manager\"]):\n                atom_id = \"ORG.SVC.M\"\n                subtype = \"ApplicationService\"\n            elif any(x in name_lower for x in [\"repository\", \"repo\", \"store\"]):\n                atom_id = \"ORG.SVC.M\"\n                subtype = \"Repository\"\n            elif any(x in name_lower for x in [\"entity\", \"model\", \"aggregate\"]):\n                atom_id = \"ORG.AGG.M\"\n                subtype = \"Entity\"\n            elif any(x in name_lower for x in [\"dto\", \"request\", \"response\"]):\n                atom_id = \"ORG.AGG.M\"\n                subtype = \"DTO\"\n            elif any(x in name_lower for x in [\"factory\", \"create\", \"build\"]):\n                atom_id = \"LOG.FNC.M\"\n                subtype = \"Factory\"\n            elif any(x in name_lower for x in [\"validate\", \"check\", \"verify\"]):\n                atom_id = \"LOG.FNC.M\"\n                subtype = \"Validator\"\n            elif any(x in name_lower for x in [\"map\", \"transform\", \"convert\"]):\n                atom_id = \"LOG.FNC.M\"\n                subtype = \"Mapper\"\n            elif any(x in name_lower for x in [\"test\", \"spec\", \"should\"]):\n                atom_id = \"EXE.PRB.O\"\n                subtype = \"TestDouble\"\n            else:\n                # Default to generic function\n                atom_id = \"LOG.FNC.M\"\n                subtype = \"ImpureFunction\"\n                confidence = 0.4\n        \n        # Look up phase and family\n        if atom_id in self.atoms_by_id:\n            phase, family, _ = self.atoms_by_id[atom_id]\n        else:\n            phase = atom_id.split(\".\")[0] if \".\" in atom_id else \"LOGIC\"\n            family = \"Functions\"\n        \n        return AtomClassification(\n            phase=phase,\n            family=family,\n            atom_id=atom_id,\n            subtype=subtype or atom_id.split(\".\")[-1],\n            confidence=confidence\n        )",
      "complexity": 0,
      "lines_of_code": 136,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier.classify_semantic_id",
      "name": "AtomClassifier.classify_semantic_id",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 220,
      "end_line": 254,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "semantic_id",
          "type": "str"
        }
      ],
      "return_type": "AtomClassification",
      "base_classes": [],
      "decorators": [],
      "docstring": "Reclassify an existing semantic ID with more granularity.\n\nArgs:\n    semantic_id: Existing ID like \"LOG.FNC.M|file.js|functionName|...\"\n\nReturns:\n    AtomClassification with refined subtype",
      "signature": "def classify_semantic_id(self, semantic_id: str) -> AtomClassification:",
      "body_source": "    def classify_semantic_id(self, semantic_id: str) -> AtomClassification:\n        \"\"\"\n        Reclassify an existing semantic ID with more granularity.\n        \n        Args:\n            semantic_id: Existing ID like \"LOG.FNC.M|file.js|functionName|...\"\n        \n        Returns:\n            AtomClassification with refined subtype\n        \"\"\"\n        parts = semantic_id.split(\"|\")\n        if len(parts) < 3:\n            return self.classify_by_name(semantic_id)\n        \n        base_atom = parts[0]\n        file_path = parts[1]\n        name = parts[2]\n        \n        # Infer language from file extension\n        language = None\n        if file_path.endswith(\".js\"):\n            language = \"javascript\"\n        elif file_path.endswith(\".ts\") or file_path.endswith(\".tsx\"):\n            language = \"typescript\"\n        elif file_path.endswith(\".py\"):\n            language = \"python\"\n        \n        # Classify with name\n        result = self.classify_by_name(name, language=language)\n        \n        # Preserve original atom_id if higher confidence\n        if base_atom in self.atoms_by_id:\n            result.atom_id = base_atom\n        \n        return result",
      "complexity": 0,
      "lines_of_code": 34,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier.get_atom_info",
      "name": "AtomClassifier.get_atom_info",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 256,
      "end_line": 265,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "atom_id",
          "type": "str"
        }
      ],
      "return_type": "Optional[dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get full info for an atom type.",
      "signature": "def get_atom_info(self, atom_id: str) -> Optional[dict]:",
      "body_source": "    def get_atom_info(self, atom_id: str) -> Optional[dict]:\n        \"\"\"Get full info for an atom type.\"\"\"\n        if atom_id in self.atoms_by_id:\n            phase, family, atom = self.atoms_by_id[atom_id]\n            return {\n                \"phase\": phase,\n                \"family\": family,\n                **atom\n            }\n        return None",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:AtomClassifier.list_all_atoms",
      "name": "AtomClassifier.list_all_atoms",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 267,
      "end_line": 278,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "list",
      "base_classes": [],
      "decorators": [],
      "docstring": "List all 167 atoms with their info.",
      "signature": "def list_all_atoms(self) -> list:",
      "body_source": "    def list_all_atoms(self) -> list:\n        \"\"\"List all 167 atoms with their info.\"\"\"\n        atoms = []\n        for phase_name, phase in self.taxonomy.get(\"phases\", {}).items():\n            for family_name, family in phase.get(\"families\", {}).items():\n                for atom in family.get(\"atoms\", []):\n                    atoms.append({\n                        \"phase\": phase_name,\n                        \"family\": family_name,\n                        **atom\n                    })\n        return atoms",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py:reclassify_semantic_ids",
      "name": "reclassify_semantic_ids",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/atom_classifier.py",
      "start_line": 281,
      "end_line": 319,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "input_path",
          "type": "str"
        },
        {
          "name": "output_path",
          "type": "str",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Reclassify all semantic IDs in a file with more granular subtypes.",
      "signature": "def reclassify_semantic_ids(input_path: str, output_path: str = None):",
      "body_source": "def reclassify_semantic_ids(input_path: str, output_path: str = None):\n    \"\"\"\n    Reclassify all semantic IDs in a file with more granular subtypes.\n    \"\"\"\n    classifier = AtomClassifier()\n    \n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    \n    ids = data.get(\"ids\", [])\n    classified = []\n    subtype_counts = {}\n    \n    for sid in ids:\n        result = classifier.classify_semantic_id(sid)\n        \n        # Track subtypes\n        subtype_counts[result.subtype] = subtype_counts.get(result.subtype, 0) + 1\n        \n        classified.append({\n            \"original_id\": sid,\n            \"atom_id\": result.atom_id,\n            \"subtype\": result.subtype,\n            \"phase\": result.phase,\n            \"confidence\": result.confidence\n        })\n    \n    output = {\n        \"total\": len(classified),\n        \"by_subtype\": subtype_counts,\n        \"classified\": classified\n    }\n    \n    if output_path:\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(output, f, indent=2)\n        print(f\"\ud83d\udcdd Saved to {output_path}\")\n    \n    return output",
      "complexity": 0,
      "lines_of_code": 38,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:TimeType",
      "name": "TimeType",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 25,
      "end_line": 25,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class TimeType(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformanceNode",
      "name": "PerformanceNode",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 101,
      "end_line": 101,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PerformanceNode:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformanceProfile",
      "name": "PerformanceProfile",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 123,
      "end_line": 123,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PerformanceProfile:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformanceProfile.summary",
      "name": "PerformanceProfile.summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 143,
      "end_line": 155,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Summarize performance profile",
      "signature": "def summary(self) -> dict:",
      "body_source": "    def summary(self) -> dict:\n        \"\"\"Summarize performance profile\"\"\"\n        return {\n            \"total_nodes\": len(self.nodes),\n            \"total_estimated_cost\": round(self.total_estimated_cost, 2),\n            \"average_node_cost\": round(self.average_node_cost, 2),\n            \"critical_path_length\": len(self.critical_path),\n            \"critical_path_cost\": round(self.critical_path_cost, 2),\n            \"hotspot_count\": len(self.hotspots),\n            \"time_by_type\": {k: round(v, 1) for k, v in self.time_by_type.items()},\n            \"time_by_layer\": {k: round(v, 1) for k, v in self.time_by_layer.items()},\n            \"node_count_by_type\": self.node_count_by_type\n        }",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor",
      "name": "PerformancePredictor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 158,
      "end_line": 158,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class PerformancePredictor:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor.__init__",
      "name": "PerformancePredictor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 161,
      "end_line": 162,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.nodes: Dict[str, PerformanceNode] = {}",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor.predict",
      "name": "PerformancePredictor.predict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 164,
      "end_line": 211,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        },
        {
          "name": "exec_flow",
          "default": "None"
        }
      ],
      "return_type": "PerformanceProfile",
      "base_classes": [],
      "decorators": [],
      "docstring": "Predict performance profile from analysis output.\n\nArgs:\n    analysis_nodes: Nodes from unified_analysis\n    exec_flow: Optional ExecutionFlow for chain analysis",
      "signature": "def predict(self, analysis_nodes: list, exec_flow=None) -> PerformanceProfile:",
      "body_source": "    def predict(self, analysis_nodes: list, exec_flow=None) -> PerformanceProfile:\n        \"\"\"\n        Predict performance profile from analysis output.\n        \n        Args:\n            analysis_nodes: Nodes from unified_analysis\n            exec_flow: Optional ExecutionFlow for chain analysis\n        \"\"\"\n        # Stage 1: Build performance nodes\n        self._build_nodes(analysis_nodes, exec_flow)\n        \n        # Stage 2: Classify time types\n        self._classify_time_types()\n        \n        # Stage 3: Estimate costs\n        self._estimate_costs()\n        \n        # Stage 4: Calculate hotspot scores\n        self._calculate_hotspots()\n        \n        # Stage 5: Find critical path\n        critical_path, critical_cost = self._find_critical_path(exec_flow)\n        \n        # Stage 6: Calculate distributions\n        time_by_layer, time_by_type, count_by_type = self._calculate_distributions()\n        \n        # Calculate totals\n        total_cost = sum(n.estimated_cost for n in self.nodes.values())\n        avg_cost = total_cost / len(self.nodes) if self.nodes else 0\n        \n        # Get top hotspots\n        hotspots = sorted(\n            self.nodes.keys(),\n            key=lambda k: self.nodes[k].hotspot_score,\n            reverse=True\n        )[:10]\n        \n        return PerformanceProfile(\n            nodes=self.nodes,\n            critical_path=critical_path,\n            critical_path_cost=critical_cost,\n            hotspots=hotspots,\n            time_by_layer=time_by_layer,\n            time_by_type=time_by_type,\n            node_count_by_type=count_by_type,\n            total_estimated_cost=total_cost,\n            average_node_cost=avg_cost\n        )",
      "complexity": 0,
      "lines_of_code": 47,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._build_nodes",
      "name": "PerformancePredictor._build_nodes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 213,
      "end_line": 248,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        },
        {
          "name": "exec_flow",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build PerformanceNodes from analysis output",
      "signature": "def _build_nodes(self, analysis_nodes: list, exec_flow=None):",
      "body_source": "    def _build_nodes(self, analysis_nodes: list, exec_flow=None):\n        \"\"\"Build PerformanceNodes from analysis output\"\"\"\n        exec_nodes = exec_flow.nodes if exec_flow else {}\n        \n        for i, node in enumerate(analysis_nodes):\n            # Handle both dict and object\n            if hasattr(node, 'name'):\n                node_id = node.id if node.id else node.name\n                name = node.name\n                role = getattr(node, 'role', 'Unknown')\n                loc = getattr(node, 'lines_of_code', 1) or 1\n                complexity = getattr(node, 'complexity', 1) or 1\n            else:\n                node_id = node.get('id') or node.get('name') or f\"node_{i}\"\n                name = node.get('name', '')\n                role = node.get('role', 'Unknown')\n                loc = node.get('lines_of_code', 1) or 1\n                complexity = node.get('complexity', 1) or 1\n            \n            # Get layer and in_degree from exec_flow if available\n            layer = \"unknown\"\n            in_degree = 0\n            if node_id in exec_nodes:\n                en = exec_nodes[node_id]\n                layer = en.layer\n                in_degree = en.in_degree\n            \n            self.nodes[node_id] = PerformanceNode(\n                id=node_id,\n                name=name,\n                role=role,\n                layer=layer,\n                lines_of_code=loc,\n                complexity=complexity,\n                in_degree=in_degree\n            )",
      "complexity": 0,
      "lines_of_code": 35,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._classify_time_types",
      "name": "PerformancePredictor._classify_time_types",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 250,
      "end_line": 285,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify time type for each node",
      "signature": "def _classify_time_types(self):",
      "body_source": "    def _classify_time_types(self):\n        \"\"\"Classify time type for each node\"\"\"\n        for node in self.nodes.values():\n            # First try role-based classification\n            if node.role in ROLE_TO_TIME_TYPE:\n                node.time_type = ROLE_TO_TIME_TYPE[node.role]\n            else:\n                node.time_type = TimeType.COMPUTE  # Default\n            \n            # Override based on name patterns\n            name_lower = node.name.lower()\n            \n            # Check for network I/O patterns\n            for pattern in IO_NETWORK_PATTERNS:\n                if re.search(pattern, name_lower):\n                    node.time_type = TimeType.IO_NETWORK\n                    break\n            \n            # Check for database patterns (network I/O)\n            for pattern in DATABASE_PATTERNS:\n                if re.search(pattern, name_lower):\n                    node.time_type = TimeType.IO_NETWORK\n                    break\n            \n            # Check for local I/O patterns\n            for pattern in IO_LOCAL_PATTERNS:\n                if re.search(pattern, name_lower):\n                    if node.time_type != TimeType.IO_NETWORK:  # Don't override network\n                        node.time_type = TimeType.IO_LOCAL\n                    break\n            \n            # Check for blocking patterns\n            for pattern in BLOCKING_PATTERNS:\n                if re.search(pattern, name_lower):\n                    node.time_type = TimeType.BLOCKING\n                    break",
      "complexity": 0,
      "lines_of_code": 35,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._estimate_costs",
      "name": "PerformancePredictor._estimate_costs",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 287,
      "end_line": 298,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Estimate cost for each node",
      "signature": "def _estimate_costs(self):",
      "body_source": "    def _estimate_costs(self):\n        \"\"\"Estimate cost for each node\"\"\"\n        for node in self.nodes.values():\n            base_cost = node.lines_of_code\n            multiplier = COST_MULTIPLIERS[node.time_type]\n            \n            # For compute nodes, factor in complexity\n            if node.time_type == TimeType.COMPUTE:\n                multiplier = max(1, node.complexity)\n            \n            node.base_cost = base_cost\n            node.estimated_cost = base_cost * multiplier",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._calculate_hotspots",
      "name": "PerformancePredictor._calculate_hotspots",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 300,
      "end_line": 304,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Calculate hotspot score for each node",
      "signature": "def _calculate_hotspots(self):",
      "body_source": "    def _calculate_hotspots(self):\n        \"\"\"Calculate hotspot score for each node\"\"\"\n        for node in self.nodes.values():\n            # Hotspot = high traffic \u00d7 high cost\n            node.hotspot_score = node.in_degree * node.estimated_cost",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._find_critical_path",
      "name": "PerformancePredictor._find_critical_path",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 306,
      "end_line": 324,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "exec_flow"
        }
      ],
      "return_type": "Tuple[List[str], float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find the critical path (longest by cost)",
      "signature": "def _find_critical_path(self, exec_flow) -> Tuple[List[str], float]:",
      "body_source": "    def _find_critical_path(self, exec_flow) -> Tuple[List[str], float]:\n        \"\"\"Find the critical path (longest by cost)\"\"\"\n        if not exec_flow or not exec_flow.chains:\n            return [], 0.0\n        \n        best_path = []\n        best_cost = 0.0\n        \n        for chain in exec_flow.chains:\n            chain_cost = sum(\n                self.nodes[nid].estimated_cost \n                for nid in chain.path \n                if nid in self.nodes\n            )\n            if chain_cost > best_cost:\n                best_cost = chain_cost\n                best_path = chain.path\n        \n        return best_path, best_cost",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:PerformancePredictor._calculate_distributions",
      "name": "PerformancePredictor._calculate_distributions",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 326,
      "end_line": 337,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Tuple[Dict, Dict, Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Calculate time distribution by layer and type",
      "signature": "def _calculate_distributions(self) -> Tuple[Dict, Dict, Dict]:",
      "body_source": "    def _calculate_distributions(self) -> Tuple[Dict, Dict, Dict]:\n        \"\"\"Calculate time distribution by layer and type\"\"\"\n        time_by_layer = defaultdict(float)\n        time_by_type = defaultdict(float)\n        count_by_type = defaultdict(int)\n        \n        for node in self.nodes.values():\n            time_by_layer[node.layer] += node.estimated_cost\n            time_by_type[node.time_type.value] += node.estimated_cost\n            count_by_type[node.time_type.value] += 1\n        \n        return dict(time_by_layer), dict(time_by_type), dict(count_by_type)",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py:predict_performance",
      "name": "predict_performance",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/performance_predictor.py",
      "start_line": 340,
      "end_line": 353,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "exec_flow",
          "default": "None"
        }
      ],
      "return_type": "PerformanceProfile",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to predict performance.\n\nUsage:\n    from performance_predictor import predict_performance\n    \n    result = analyze(path)\n    flow = detect_execution_flow(result.nodes, result.edges)\n    perf = predict_performance(result.nodes, flow)\n    print(perf.summary())",
      "signature": "def predict_performance(nodes: list, exec_flow=None) -> PerformanceProfile:",
      "body_source": "def predict_performance(nodes: list, exec_flow=None) -> PerformanceProfile:\n    \"\"\"\n    Convenience function to predict performance.\n    \n    Usage:\n        from performance_predictor import predict_performance\n        \n        result = analyze(path)\n        flow = detect_execution_flow(result.nodes, result.edges)\n        perf = predict_performance(result.nodes, flow)\n        print(perf.summary())\n    \"\"\"\n    predictor = PerformancePredictor()\n    return predictor.predict(nodes, exec_flow)",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:TotalityReport",
      "name": "TotalityReport",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 25,
      "end_line": 25,
      "role": "DTO",
      "role_confidence": 95.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class TotalityReport:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:TotalityReport.to_dict",
      "name": "TotalityReport.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 34,
      "end_line": 42,
      "role": "Test",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def to_dict(self) -> Dict[str, Any]:",
      "body_source": "    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"total_node_types\": self.total_node_types,\n            \"mapped_types\": self.mapped_types,\n            \"unmapped_types\": self.unmapped_types,\n            \"fallback_types\": self.fallback_types,\n            \"coverage_rate\": round(self.coverage_rate, 4),\n            \"is_total\": self.is_total,\n        }",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:collect_ast_node_types",
      "name": "collect_ast_node_types",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 45,
      "end_line": 64,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "repo_path",
          "type": "Path"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Scan a repository and collect all unique AST node types encountered.\nUses Tree-Sitter to parse files and extract node kinds.",
      "signature": "def collect_ast_node_types(repo_path: Path) -> Set[str]:",
      "body_source": "def collect_ast_node_types(repo_path: Path) -> Set[str]:\n    \"\"\"\n    Scan a repository and collect all unique AST node types encountered.\n    Uses Tree-Sitter to parse files and extract node kinds.\n    \"\"\"\n    from tree_sitter_engine import TreeSitterUniversalEngine\n    \n    engine = TreeSitterUniversalEngine()\n    node_types: Set[str] = set()\n    \n    # Analyze the repo\n    results = engine.analyze_directory(str(repo_path))\n    \n    for file_result in results:\n        for particle in file_result.get(\"particles\", []):\n            node_kind = particle.get(\"node_kind\") or particle.get(\"kind\") or particle.get(\"symbol_kind\")\n            if node_kind:\n                node_types.add(node_kind)\n    \n    return node_types",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:test_totality",
      "name": "test_totality",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 67,
      "end_line": 120,
      "role": "Test",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "classifier",
          "type": "ParticleClassifier"
        },
        {
          "name": "node_types",
          "type": "Set[str]"
        },
        {
          "name": "fallback_threshold",
          "type": "float",
          "default": "0.4"
        }
      ],
      "return_type": "TotalityReport",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test that \u03c4 maps every node type to an atom.\n\nA node type is considered:\n- Mapped: classifier returns a type != \"Unknown\" with confidence >= fallback_threshold\n- Fallback: classifier returns a type but with confidence < fallback_threshold\n- Unmapped: classifier returns \"Unknown\"\n\nArgs:\n    classifier: The particle classifier to test\n    node_types: Set of AST node types to test\n    fallback_threshold: Confidence below which a mapping is considered \"fallback\"\n    \nReturns:\n    TotalityReport with coverage statistics",
      "signature": "def test_totality(",
      "body_source": "def test_totality(\n    classifier: ParticleClassifier,\n    node_types: Set[str],\n    fallback_threshold: float = 0.4,\n) -> TotalityReport:\n    \"\"\"\n    Test that \u03c4 maps every node type to an atom.\n    \n    A node type is considered:\n    - Mapped: classifier returns a type != \"Unknown\" with confidence >= fallback_threshold\n    - Fallback: classifier returns a type but with confidence < fallback_threshold\n    - Unmapped: classifier returns \"Unknown\"\n    \n    Args:\n        classifier: The particle classifier to test\n        node_types: Set of AST node types to test\n        fallback_threshold: Confidence below which a mapping is considered \"fallback\"\n        \n    Returns:\n        TotalityReport with coverage statistics\n    \"\"\"\n    report = TotalityReport(total_node_types=len(node_types))\n    \n    for node_type in node_types:\n        # Create a minimal particle for classification\n        test_particle = {\n            \"name\": f\"test_{node_type}\",\n            \"kind\": node_type,\n            \"symbol_kind\": node_type,\n            \"line\": 1,\n            \"file_path\": \"test.py\",\n        }\n        \n        # Classify\n        classified = classifier.classify_particle(test_particle)\n        result_type = classified.get(\"type\", \"Unknown\")\n        confidence = classified.get(\"confidence\", 0.0)\n        \n        if result_type == \"Unknown\":\n            report.unmapped_types.append(node_type)\n        elif confidence < fallback_threshold:\n            report.fallback_types.append(f\"{node_type} -> {result_type} ({confidence:.2f})\")\n            report.mapped_types += 1\n        else:\n            report.mapped_types += 1\n    \n    # Calculate coverage\n    if report.total_node_types > 0:\n        report.coverage_rate = report.mapped_types / report.total_node_types\n    \n    # Totality check: must map ALL types (fallback is ok, unmapped is not)\n    report.is_total = len(report.unmapped_types) == 0\n    \n    return report",
      "complexity": 0,
      "lines_of_code": 53,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:test_determinism",
      "name": "test_determinism",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 123,
      "end_line": 156,
      "role": "Test",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "classifier",
          "type": "ParticleClassifier"
        },
        {
          "name": "test_particles",
          "type": "List[Dict[str, Any]]"
        },
        {
          "name": "iterations",
          "type": "int",
          "default": "3"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test that \u03c4 is deterministic: same input always produces same output.\n\nThis is critical for reproducibility and caching.",
      "signature": "def test_determinism(",
      "body_source": "def test_determinism(\n    classifier: ParticleClassifier,\n    test_particles: List[Dict[str, Any]],\n    iterations: int = 3,\n) -> Dict[str, Any]:\n    \"\"\"\n    Test that \u03c4 is deterministic: same input always produces same output.\n    \n    This is critical for reproducibility and caching.\n    \"\"\"\n    results: Dict[str, List[str]] = {}  # particle_name -> [result_type, ...]\n    \n    for _ in range(iterations):\n        for particle in test_particles:\n            name = particle.get(\"name\", \"unknown\")\n            classified = classifier.classify_particle(particle)\n            result_type = classified.get(\"type\", \"Unknown\")\n            \n            if name not in results:\n                results[name] = []\n            results[name].append(result_type)\n    \n    # Check for consistency\n    inconsistent = []\n    for name, types in results.items():\n        if len(set(types)) > 1:\n            inconsistent.append({\"name\": name, \"types\": types})\n    \n    return {\n        \"particles_tested\": len(test_particles),\n        \"iterations\": iterations,\n        \"is_deterministic\": len(inconsistent) == 0,\n        \"inconsistent\": inconsistent,\n    }",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py:run_standard_tests",
      "name": "run_standard_tests",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/test_totality.py",
      "start_line": 197,
      "end_line": 322,
      "role": "Test",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_test",
      "params": [],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Run the standard totality test suite.",
      "signature": "def run_standard_tests() -> Dict[str, Any]:",
      "body_source": "def run_standard_tests() -> Dict[str, Any]:\n    \"\"\"Run the standard totality test suite.\"\"\"\n    print(\"=\" * 70)\n    print(\"\ud83e\uddea TOTALITY TEST SUITE\")\n    print(\"=\" * 70)\n    \n    engine = TreeSitterUniversalEngine()\n    \n    # Test 1: Standard DDD Particles\n    print(\"\\n\ud83d\udccb Test 1: Standard DDD Particles\")\n    print(\"-\" * 40)\n    \n    mapped = 0\n    unmapped = []\n    results_detail = []\n    \n    for particle in STANDARD_TEST_PARTICLES:\n        # Use the actual \u03c4 function (type assignment)\n        classified = engine._classify_extracted_symbol(\n            name=particle[\"name\"],\n            symbol_kind=particle.get(\"symbol_kind\", particle.get(\"kind\", \"class\")),\n            file_path=particle[\"file_path\"],\n            line_num=1,\n            evidence=\"\",\n            parent=\"\",\n            base_classes=[],\n            decorators=[],\n        )\n        result_type = classified.get(\"type\", \"Unknown\")\n        confidence = classified.get(\"confidence\", 0.0)\n        \n        if result_type == \"Unknown\":\n            unmapped.append(particle[\"name\"])\n        else:\n            mapped += 1\n        \n        results_detail.append({\n            \"name\": particle[\"name\"],\n            \"type\": result_type,\n            \"confidence\": confidence,\n        })\n    \n    total = len(STANDARD_TEST_PARTICLES)\n    coverage = mapped / total if total > 0 else 0.0\n    \n    print(f\"   Total particles: {total}\")\n    print(f\"   Mapped: {mapped}\")\n    print(f\"   Coverage: {coverage:.1%}\")\n    print(f\"   Totality: {'\u2705 PASS' if not unmapped else '\u274c FAIL'}\")\n    \n    if unmapped:\n        print(f\"   Unmapped: {unmapped}\")\n    else:\n        print(\"   All particles classified!\")\n    \n    # Show sample classifications\n    print(\"\\n   Sample classifications:\")\n    for r in results_detail[:5]:\n        print(f\"     {r['name']} -> {r['type']} ({r['confidence']:.0%})\")\n    \n    # Test 2: Determinism\n    print(\"\\n\ud83d\udccb Test 2: Determinism\")\n    print(\"-\" * 40)\n    \n    # Run classification 3 times and check for consistency\n    iterations = 3\n    results_by_name: Dict[str, List[str]] = {}\n    \n    for _ in range(iterations):\n        for particle in STANDARD_TEST_PARTICLES:\n            classified = engine._classify_extracted_symbol(\n                name=particle[\"name\"],\n                symbol_kind=particle.get(\"symbol_kind\", particle.get(\"kind\", \"class\")),\n                file_path=particle[\"file_path\"],\n                line_num=1,\n                evidence=\"\",\n                parent=\"\",\n                base_classes=[],\n                decorators=[],\n            )\n            result_type = classified.get(\"type\", \"Unknown\")\n            \n            name = particle[\"name\"]\n            if name not in results_by_name:\n                results_by_name[name] = []\n            results_by_name[name].append(result_type)\n    \n    # Check for consistency\n    inconsistent = []\n    for name, types in results_by_name.items():\n        if len(set(types)) > 1:\n            inconsistent.append({\"name\": name, \"types\": types})\n    \n    is_deterministic = len(inconsistent) == 0\n    \n    print(f\"   Particles tested: {total}\")\n    print(f\"   Iterations: {iterations}\")\n    print(f\"   Deterministic: {'\u2705 PASS' if is_deterministic else '\u274c FAIL'}\")\n    \n    if inconsistent:\n        print(f\"   Inconsistent: {inconsistent}\")\n    \n    # Summary\n    print(\"\\n\" + \"=\" * 70)\n    is_total = len(unmapped) == 0\n    all_passed = is_total and is_deterministic\n    print(f\"{'\u2705 ALL TESTS PASSED' if all_passed else '\u274c SOME TESTS FAILED'}\")\n    print(\"=\" * 70)\n    \n    return {\n        \"totality\": {\n            \"total_particles\": total,\n            \"mapped\": mapped,\n            \"unmapped\": unmapped,\n            \"coverage_rate\": round(coverage, 4),\n            \"is_total\": is_total,\n            \"classifications\": results_detail,\n        },\n        \"determinism\": {\n            \"particles_tested\": total,\n            \"iterations\": iterations,\n            \"is_deterministic\": is_deterministic,\n            \"inconsistent\": inconsistent,\n        },\n        \"all_passed\": all_passed,\n    }",
      "complexity": 0,
      "lines_of_code": 125,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:EdgeType",
      "name": "EdgeType",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 11,
      "end_line": 11,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class EdgeType(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Component",
      "name": "Component",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 19,
      "end_line": 19,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Component:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Edge",
      "name": "Edge",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 29,
      "end_line": 29,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Edge:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Graph",
      "name": "Graph",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 41,
      "end_line": 41,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Graph:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Graph.add_component",
      "name": "Graph.add_component",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 48,
      "end_line": 49,
      "role": "View",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "component",
          "type": "Component"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def add_component(self, component: Component):",
      "body_source": "    def add_component(self, component: Component):\n        self.components[component.id] = component",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Graph.add_edge",
      "name": "Graph.add_edge",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 51,
      "end_line": 52,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "edge",
          "type": "Edge"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def add_edge(self, edge: Edge):",
      "body_source": "    def add_edge(self, edge: Edge):\n        self.edges.append(edge)",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Graph.to_json",
      "name": "Graph.to_json",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 54,
      "end_line": 65,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export graph to JSON string.",
      "signature": "def to_json(self) -> str:",
      "body_source": "    def to_json(self) -> str:\n        \"\"\"Export graph to JSON string.\"\"\"\n        import json\n        from dataclasses import asdict\n        \n        # Helper to handle Enum serialization\n        def default_serializer(obj):\n            if isinstance(obj, Enum):\n                return obj.value\n            return str(obj)\n\n        return json.dumps(asdict(self), default=default_serializer)",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:Graph.default_serializer",
      "name": "Graph.default_serializer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 60,
      "end_line": 63,
      "role": "Mapper",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "obj"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def default_serializer(obj):",
      "body_source": "        def default_serializer(obj):\n            if isinstance(obj, Enum):\n                return obj.value\n            return str(obj)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py:edges_from_internal_edges_list",
      "name": "edges_from_internal_edges_list",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/ir.py",
      "start_line": 67,
      "end_line": 83,
      "role": "UIComponent",
      "role_confidence": 0.8,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "internal_edges",
          "type": "List[Dict]"
        }
      ],
      "return_type": "List[Edge]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convert the `internal_edges` summary from DependencyAnalyzer to Edge IR.\n\nThis is the aggregated list: [{\"from\": \"a.py\", \"to\": \"b.py\", \"count\": 3}, ...]",
      "signature": "def edges_from_internal_edges_list(internal_edges: List[Dict]) -> List[Edge]:",
      "body_source": "def edges_from_internal_edges_list(internal_edges: List[Dict]) -> List[Edge]:\n    \"\"\"\n    Convert the `internal_edges` summary from DependencyAnalyzer to Edge IR.\n    \n    This is the aggregated list: [{\"from\": \"a.py\", \"to\": \"b.py\", \"count\": 3}, ...]\n    \"\"\"\n    return [\n        Edge(\n            source=e[\"from\"],\n            target=e[\"to\"],\n            edge_type=EdgeType.IMPORT,\n            weight=e.get(\"count\", 1),\n            category=\"internal\",\n            confidence=1.0,\n        )\n        for e in internal_edges\n    ]",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:Violation",
      "name": "Violation",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 21,
      "end_line": 21,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Violation:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:EvaluationResult",
      "name": "EvaluationResult",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 37,
      "end_line": 37,
      "role": "DTO",
      "role_confidence": 78.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class EvaluationResult:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:EvaluationResult.to_dict",
      "name": "EvaluationResult.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 45,
      "end_line": 66,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def to_dict(self) -> Dict[str, Any]:",
      "body_source": "    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"total_particles\": self.total_particles,\n            \"laws_checked\": self.laws_checked,\n            \"total_violations\": len(self.violations),\n            \"by_law\": self.by_law,\n            \"by_severity\": self.by_severity,\n            \"violations\": [\n                {\n                    \"law_id\": v.law_id,\n                    \"law_name\": v.law_name,\n                    \"particle\": v.particle_name,\n                    \"type\": v.particle_type,\n                    \"file\": v.file_path,\n                    \"line\": v.line,\n                    \"message\": v.message,\n                    \"severity\": v.severity,\n                    \"confidence\": v.confidence,\n                }\n                for v in self.violations\n            ],\n        }",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator",
      "name": "AntimatterEvaluator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 69,
      "end_line": 69,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class AntimatterEvaluator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator.__init__",
      "name": "AntimatterEvaluator.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 76,
      "end_line": 77,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "laws_path",
          "type": "Path",
          "default": "LAWS_PATH"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, laws_path: Path = LAWS_PATH):",
      "body_source": "    def __init__(self, laws_path: Path = LAWS_PATH):\n        self.laws = self._load_laws(laws_path)",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._load_laws",
      "name": "AntimatterEvaluator._load_laws",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 79,
      "end_line": 84,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "path",
          "type": "Path"
        }
      ],
      "return_type": "List[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load laws from JSON file.",
      "signature": "def _load_laws(self, path: Path) -> List[Dict[str, Any]]:",
      "body_source": "    def _load_laws(self, path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Load laws from JSON file.\"\"\"\n        if not path.exists():\n            print(f\"Warning: Laws file not found: {path}\")\n            return []\n        return json.loads(path.read_text())",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator.evaluate",
      "name": "AntimatterEvaluator.evaluate",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 86,
      "end_line": 115,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict[str, Any]]"
        },
        {
          "name": "threshold",
          "type": "float",
          "default": "0.55"
        }
      ],
      "return_type": "EvaluationResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Evaluate all laws against all particles.\n\nArgs:\n    particles: List of particle dicts with type, edges, etc.\n    threshold: Minimum confidence to report violations\n    \nReturns:\n    EvaluationResult with all violations",
      "signature": "def evaluate(",
      "body_source": "    def evaluate(\n        self,\n        particles: List[Dict[str, Any]],\n        threshold: float = 0.55,\n    ) -> EvaluationResult:\n        \"\"\"\n        Evaluate all laws against all particles.\n        \n        Args:\n            particles: List of particle dicts with type, edges, etc.\n            threshold: Minimum confidence to report violations\n            \n        Returns:\n            EvaluationResult with all violations\n        \"\"\"\n        result = EvaluationResult(\n            total_particles=len(particles),\n            laws_checked=len(self.laws),\n        )\n        \n        for law in self.laws:\n            law_violations = self._evaluate_law(law, particles, threshold)\n            result.violations.extend(law_violations)\n            result.by_law[law[\"id\"]] = len(law_violations)\n        \n        # Count by severity\n        for v in result.violations:\n            result.by_severity[v.severity] = result.by_severity.get(v.severity, 0) + 1\n        \n        return result",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._evaluate_law",
      "name": "AntimatterEvaluator._evaluate_law",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 117,
      "end_line": 146,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "law",
          "type": "Dict[str, Any]"
        },
        {
          "name": "particles",
          "type": "List[Dict[str, Any]]"
        },
        {
          "name": "threshold",
          "type": "float"
        }
      ],
      "return_type": "List[Violation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Evaluate a single law against all particles.",
      "signature": "def _evaluate_law(",
      "body_source": "    def _evaluate_law(\n        self,\n        law: Dict[str, Any],\n        particles: List[Dict[str, Any]],\n        threshold: float,\n    ) -> List[Violation]:\n        \"\"\"Evaluate a single law against all particles.\"\"\"\n        violations = []\n        pattern = law.get(\"pattern\", {})\n        particle_types = pattern.get(\"particle_types\", law.get(\"scope\", []))\n        law_threshold = law.get(\"threshold\", threshold)\n        \n        for particle in particles:\n            p_type = particle.get(\"type\", \"Unknown\")\n            p_conf = particle.get(\"confidence\", 0.5)\n            \n            # Skip if particle type doesn't match law scope\n            if p_type not in particle_types:\n                continue\n            \n            # Skip if confidence too low\n            if p_conf < law_threshold:\n                continue\n            \n            # Check for violations\n            violation = self._check_particle(law, pattern, particle)\n            if violation:\n                violations.append(violation)\n        \n        return violations",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._check_particle",
      "name": "AntimatterEvaluator._check_particle",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 148,
      "end_line": 247,
      "role": "Validator",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "law",
          "type": "Dict[str, Any]"
        },
        {
          "name": "pattern",
          "type": "Dict[str, Any]"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Optional[Violation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if a particle violates the law.",
      "signature": "def _check_particle(",
      "body_source": "    def _check_particle(\n        self,\n        law: Dict[str, Any],\n        pattern: Dict[str, Any],\n        particle: Dict[str, Any],\n    ) -> Optional[Violation]:\n        \"\"\"Check if a particle violates the law.\"\"\"\n        \n        evidence = []\n        is_violation = False\n        \n        # Check forbidden edges\n        forbidden_edges = pattern.get(\"forbidden_edges\", [])\n        if forbidden_edges:\n            particle_edges = self._get_edge_types(particle)\n            for edge in forbidden_edges:\n                if edge in particle_edges:\n                    is_violation = True\n                    evidence.append(f\"Has forbidden edge: {edge}\")\n        \n        # Check forbidden imports\n        forbidden_imports = pattern.get(\"forbidden_imports\", [])\n        if forbidden_imports:\n            particle_imports = self._get_imports(particle)\n            for imp in forbidden_imports:\n                if any(imp in i for i in particle_imports):\n                    is_violation = True\n                    evidence.append(f\"Has forbidden import: {imp}\")\n        \n        # Check forbidden calls\n        forbidden_calls = pattern.get(\"forbidden_calls\", [])\n        if forbidden_calls:\n            particle_calls = self._get_calls(particle)\n            for call in forbidden_calls:\n                if any(call in c for c in particle_calls):\n                    is_violation = True\n                    evidence.append(f\"Has forbidden call: {call}\")\n        \n        # Check forbidden methods\n        forbidden_methods = pattern.get(\"forbidden_methods\", [])\n        if forbidden_methods:\n            particle_methods = self._get_methods(particle)\n            for method in forbidden_methods:\n                if any(method in m for m in particle_methods):\n                    is_violation = True\n                    evidence.append(f\"Has forbidden method pattern: {method}\")\n        \n        # Check forbidden fields\n        forbidden_fields = pattern.get(\"forbidden_fields\", [])\n        if forbidden_fields:\n            particle_fields = self._get_fields(particle)\n            for f in forbidden_fields:\n                if f in particle_fields:\n                    is_violation = True\n                    evidence.append(f\"Has forbidden field: {f}\")\n        \n        # Check required fields (inverse - violation if MISSING)\n        required_fields = pattern.get(\"required_fields\", [])\n        if required_fields:\n            particle_fields = self._get_fields(particle)\n            has_any = any(f in particle_fields for f in required_fields)\n            if not has_any:\n                is_violation = True\n                evidence.append(f\"Missing required field (one of: {required_fields})\")\n        \n        # Check required patterns (inverse - violation if MISSING)\n        required_patterns = pattern.get(\"required_patterns\", [])\n        if required_patterns:\n            code = particle.get(\"code_excerpt\", \"\") or particle.get(\"evidence\", \"\")\n            has_any = any(p in code for p in required_patterns)\n            if not has_any and not pattern.get(\"inversion\"):\n                is_violation = True\n                evidence.append(f\"Missing required pattern (one of: {required_patterns})\")\n        \n        # Check inversion (law checks for PRESENCE of required things)\n        if pattern.get(\"inversion\"):\n            required_edges = pattern.get(\"required_edges\", [])\n            if required_edges:\n                particle_edges = self._get_edge_types(particle)\n                has_any = any(e in particle_edges for e in required_edges)\n                if not has_any:\n                    is_violation = True\n                    evidence.append(pattern.get(\"inversion_message\", \"Missing required edges\"))\n        \n        if not is_violation:\n            return None\n        \n        return Violation(\n            law_id=law[\"id\"],\n            law_name=law[\"name\"],\n            particle_id=particle.get(\"id\", \"unknown\"),\n            particle_name=particle.get(\"name\", \"unknown\"),\n            particle_type=particle.get(\"type\", \"Unknown\"),\n            file_path=particle.get(\"file_path\", \"\"),\n            line=particle.get(\"line\", 0),\n            message=law[\"statement\"],\n            severity=law.get(\"severity\", \"warning\"),\n            confidence=particle.get(\"confidence\", 0.5),\n            evidence=evidence,\n        )",
      "complexity": 0,
      "lines_of_code": 99,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._get_edge_types",
      "name": "AntimatterEvaluator._get_edge_types",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 253,
      "end_line": 256,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get edge types from particle.",
      "signature": "def _get_edge_types(self, particle: Dict[str, Any]) -> Set[str]:",
      "body_source": "    def _get_edge_types(self, particle: Dict[str, Any]) -> Set[str]:\n        \"\"\"Get edge types from particle.\"\"\"\n        edges = particle.get(\"edges\", []) or particle.get(\"outgoing_edges\", [])\n        return {e.get(\"type\", e) if isinstance(e, dict) else str(e) for e in edges}",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._get_imports",
      "name": "AntimatterEvaluator._get_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 258,
      "end_line": 260,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get imports from particle.",
      "signature": "def _get_imports(self, particle: Dict[str, Any]) -> List[str]:",
      "body_source": "    def _get_imports(self, particle: Dict[str, Any]) -> List[str]:\n        \"\"\"Get imports from particle.\"\"\"\n        return particle.get(\"imports\", []) or []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._get_calls",
      "name": "AntimatterEvaluator._get_calls",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 262,
      "end_line": 264,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get function calls from particle.",
      "signature": "def _get_calls(self, particle: Dict[str, Any]) -> List[str]:",
      "body_source": "    def _get_calls(self, particle: Dict[str, Any]) -> List[str]:\n        \"\"\"Get function calls from particle.\"\"\"\n        return particle.get(\"calls\", []) or particle.get(\"called_functions\", []) or []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._get_methods",
      "name": "AntimatterEvaluator._get_methods",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 266,
      "end_line": 268,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get method names from particle.",
      "signature": "def _get_methods(self, particle: Dict[str, Any]) -> List[str]:",
      "body_source": "    def _get_methods(self, particle: Dict[str, Any]) -> List[str]:\n        \"\"\"Get method names from particle.\"\"\"\n        return particle.get(\"methods\", []) or particle.get(\"method_names\", []) or []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:AntimatterEvaluator._get_fields",
      "name": "AntimatterEvaluator._get_fields",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 270,
      "end_line": 273,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get field/attribute names from particle.",
      "signature": "def _get_fields(self, particle: Dict[str, Any]) -> Set[str]:",
      "body_source": "    def _get_fields(self, particle: Dict[str, Any]) -> Set[str]:\n        \"\"\"Get field/attribute names from particle.\"\"\"\n        fields = particle.get(\"fields\", []) or particle.get(\"attributes\", []) or []\n        return set(fields)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:check_purity_violation",
      "name": "check_purity_violation",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 280,
      "end_line": 284,
      "role": "Validator",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        },
        {
          "name": "law",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Optional[Violation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "L3: Check if a pure function has side effects.",
      "signature": "def check_purity_violation(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:",
      "body_source": "def check_purity_violation(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:\n    \"\"\"L3: Check if a pure function has side effects.\"\"\"\n    evaluator = AntimatterEvaluator.__new__(AntimatterEvaluator)\n    evaluator.laws = []\n    return evaluator._check_particle(law, law.get(\"pattern\", {}), particle)",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:check_command_returns",
      "name": "check_command_returns",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 287,
      "end_line": 291,
      "role": "Validator",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        },
        {
          "name": "law",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Optional[Violation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "L1: Check if CommandHandler returns domain data.",
      "signature": "def check_command_returns(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:",
      "body_source": "def check_command_returns(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:\n    \"\"\"L1: Check if CommandHandler returns domain data.\"\"\"\n    evaluator = AntimatterEvaluator.__new__(AntimatterEvaluator)\n    evaluator.laws = []\n    return evaluator._check_particle(law, law.get(\"pattern\", {}), particle)",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py:check_query_mutation",
      "name": "check_query_mutation",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/antimatter_evaluator.py",
      "start_line": 294,
      "end_line": 298,
      "role": "Validator",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "particle",
          "type": "Dict[str, Any]"
        },
        {
          "name": "law",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "Optional[Violation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "L2: Check if QueryHandler mutates state.",
      "signature": "def check_query_mutation(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:",
      "body_source": "def check_query_mutation(particle: Dict[str, Any], law: Dict[str, Any]) -> Optional[Violation]:\n    \"\"\"L2: Check if QueryHandler mutates state.\"\"\"\n    evaluator = AntimatterEvaluator.__new__(AntimatterEvaluator)\n    evaluator.laws = []\n    return evaluator._check_particle(law, law.get(\"pattern\", {}), particle)",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:CanonicalType",
      "name": "CanonicalType",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 16,
      "end_line": 16,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class CanonicalType:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry",
      "name": "TypeRegistry",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 25,
      "end_line": 25,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class TypeRegistry:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.__init__",
      "name": "TypeRegistry.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 34,
      "end_line": 46,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "types_path",
          "type": "Optional[Path]",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, types_path: Optional[Path] = None):",
      "body_source": "    def __init__(self, types_path: Optional[Path] = None):\n        if types_path is None:\n            types_path = Path(__file__).parent.parent / \"patterns\" / \"canonical_types.json\"\n        \n        with open(types_path, 'r', encoding='utf-8') as f:\n            self._data = json.load(f)\n        \n        self._types: Dict[str, CanonicalType] = {}\n        self._alias_map: Dict[str, str] = {}  # alias -> canonical id\n        self._by_layer: Dict[str, List[str]] = {}\n        self._layer_weights: Dict[str, float] = {}\n        \n        self._build_registry()",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry._build_registry",
      "name": "TypeRegistry._build_registry",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 48,
      "end_line": 76,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "None",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build lookup structures from JSON data.",
      "signature": "def _build_registry(self) -> None:",
      "body_source": "    def _build_registry(self) -> None:\n        \"\"\"Build lookup structures from JSON data.\"\"\"\n        layers = self._data.get(\"layers\", {})\n        \n        for layer_name, layer_data in layers.items():\n            self._by_layer[layer_name] = []\n            \n            for type_def in layer_data.get(\"types\", []):\n                type_id = type_def[\"id\"]\n                aliases = tuple(type_def.get(\"aliases\", []))\n                \n                ct = CanonicalType(\n                    id=type_id,\n                    layer=layer_name,\n                    description=type_def.get(\"description\", \"\"),\n                    aliases=aliases,\n                    rpbl=type_def.get(\"rpbl\", {}),\n                )\n                \n                self._types[type_id] = ct\n                self._by_layer[layer_name].append(type_id)\n                \n                # Register aliases\n                for alias in aliases:\n                    self._alias_map[alias] = type_id\n        \n        # Load scoring weights\n        scoring = self._data.get(\"scoring\", {})\n        self._layer_weights = scoring.get(\"layer_weights\", {})",
      "complexity": 0,
      "lines_of_code": 28,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.get_instance",
      "name": "TypeRegistry.get_instance",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 79,
      "end_line": 83,
      "role": "Factory",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "cls"
        }
      ],
      "return_type": "'TypeRegistry'",
      "base_classes": [],
      "decorators": [
        "classmethod"
      ],
      "docstring": "Get or create the singleton instance.",
      "signature": "def get_instance(cls) -> 'TypeRegistry':",
      "body_source": "    def get_instance(cls) -> 'TypeRegistry':\n        \"\"\"Get or create the singleton instance.\"\"\"\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.normalize",
      "name": "TypeRegistry.normalize",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 85,
      "end_line": 98,
      "role": "Transformer",
      "role_confidence": 82.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Normalize a type name to its canonical form.\n\nExamples:\n    \"Event\" -> \"DomainEvent\"\n    \"Schema\" -> \"DTO\"\n    \"Service\" -> \"ApplicationService\"",
      "signature": "def normalize(self, type_name: str) -> str:",
      "body_source": "    def normalize(self, type_name: str) -> str:\n        \"\"\"\n        Normalize a type name to its canonical form.\n        \n        Examples:\n            \"Event\" -> \"DomainEvent\"\n            \"Schema\" -> \"DTO\"\n            \"Service\" -> \"ApplicationService\"\n        \"\"\"\n        if type_name in self._types:\n            return type_name\n        if type_name in self._alias_map:\n            return self._alias_map[type_name]\n        return type_name  # Return as-is if unknown",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.is_valid",
      "name": "TypeRegistry.is_valid",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 100,
      "end_line": 102,
      "role": "Specification",
      "role_confidence": 85.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if a type name (or alias) is in the canonical set.",
      "signature": "def is_valid(self, type_name: str) -> bool:",
      "body_source": "    def is_valid(self, type_name: str) -> bool:\n        \"\"\"Check if a type name (or alias) is in the canonical set.\"\"\"\n        return type_name in self._types or type_name in self._alias_map",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.get_type",
      "name": "TypeRegistry.get_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 104,
      "end_line": 107,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "Optional[CanonicalType]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the canonical type definition, resolving aliases.",
      "signature": "def get_type(self, type_name: str) -> Optional[CanonicalType]:",
      "body_source": "    def get_type(self, type_name: str) -> Optional[CanonicalType]:\n        \"\"\"Get the canonical type definition, resolving aliases.\"\"\"\n        normalized = self.normalize(type_name)\n        return self._types.get(normalized)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.get_layer",
      "name": "TypeRegistry.get_layer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 109,
      "end_line": 112,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the layer a type belongs to.",
      "signature": "def get_layer(self, type_name: str) -> Optional[str]:",
      "body_source": "    def get_layer(self, type_name: str) -> Optional[str]:\n        \"\"\"Get the layer a type belongs to.\"\"\"\n        ct = self.get_type(type_name)\n        return ct.layer if ct else None",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.get_layer_weight",
      "name": "TypeRegistry.get_layer_weight",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 114,
      "end_line": 116,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "layer",
          "type": "str"
        }
      ],
      "return_type": "float",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the scoring weight for a layer.",
      "signature": "def get_layer_weight(self, layer: str) -> float:",
      "body_source": "    def get_layer_weight(self, layer: str) -> float:\n        \"\"\"Get the scoring weight for a layer.\"\"\"\n        return self._layer_weights.get(layer, 1.0)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.all_types",
      "name": "TypeRegistry.all_types",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 118,
      "end_line": 120,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all canonical type IDs.",
      "signature": "def all_types(self) -> Set[str]:",
      "body_source": "    def all_types(self) -> Set[str]:\n        \"\"\"Get all canonical type IDs.\"\"\"\n        return set(self._types.keys())",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.types_in_layer",
      "name": "TypeRegistry.types_in_layer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 122,
      "end_line": 124,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "layer",
          "type": "str"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all type IDs in a specific layer.",
      "signature": "def types_in_layer(self, layer: str) -> List[str]:",
      "body_source": "    def types_in_layer(self, layer: str) -> List[str]:\n        \"\"\"Get all type IDs in a specific layer.\"\"\"\n        return self._by_layer.get(layer, [])",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.all_layers",
      "name": "TypeRegistry.all_layers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 126,
      "end_line": 128,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all layer names.",
      "signature": "def all_layers(self) -> List[str]:",
      "body_source": "    def all_layers(self) -> List[str]:\n        \"\"\"Get all layer names.\"\"\"\n        return list(self._by_layer.keys())",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:TypeRegistry.get_rpbl",
      "name": "TypeRegistry.get_rpbl",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 130,
      "end_line": 133,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, int]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get RPBL scores for a type.",
      "signature": "def get_rpbl(self, type_name: str) -> Dict[str, int]:",
      "body_source": "    def get_rpbl(self, type_name: str) -> Dict[str, int]:\n        \"\"\"Get RPBL scores for a type.\"\"\"\n        ct = self.get_type(type_name)\n        return ct.rpbl if ct else {}",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:get_registry",
      "name": "get_registry",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 137,
      "end_line": 139,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "TypeRegistry",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the type registry singleton.",
      "signature": "def get_registry() -> TypeRegistry:",
      "body_source": "def get_registry() -> TypeRegistry:\n    \"\"\"Get the type registry singleton.\"\"\"\n    return TypeRegistry.get_instance()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:normalize_type",
      "name": "normalize_type",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 142,
      "end_line": 144,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Normalize a type name to canonical form.",
      "signature": "def normalize_type(type_name: str) -> str:",
      "body_source": "def normalize_type(type_name: str) -> str:\n    \"\"\"Normalize a type name to canonical form.\"\"\"\n    return get_registry().normalize(type_name)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:is_valid_type",
      "name": "is_valid_type",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 147,
      "end_line": 149,
      "role": "Specification",
      "role_confidence": 85.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if a type is in the canonical set.",
      "signature": "def is_valid_type(type_name: str) -> bool:",
      "body_source": "def is_valid_type(type_name: str) -> bool:\n    \"\"\"Check if a type is in the canonical set.\"\"\"\n    return get_registry().is_valid(type_name)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:get_all_types",
      "name": "get_all_types",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 152,
      "end_line": 154,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all canonical type IDs.",
      "signature": "def get_all_types() -> Set[str]:",
      "body_source": "def get_all_types() -> Set[str]:\n    \"\"\"Get all canonical type IDs.\"\"\"\n    return get_registry().all_types()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py:get_layer",
      "name": "get_layer",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/type_registry.py",
      "start_line": 157,
      "end_line": 159,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "type_name",
          "type": "str"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the layer a type belongs to.",
      "signature": "def get_layer(type_name: str) -> Optional[str]:",
      "body_source": "def get_layer(type_name: str) -> Optional[str]:\n    \"\"\"Get the layer a type belongs to.\"\"\"\n    return get_registry().get_layer(type_name)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:Continent",
      "name": "Continent",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 39,
      "end_line": 39,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Continent(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:Fundamental",
      "name": "Fundamental",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 47,
      "end_line": 47,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Fundamental(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:Level",
      "name": "Level",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 69,
      "end_line": 69,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Level(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID",
      "name": "SemanticID",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 81,
      "end_line": 81,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class SemanticID:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.__post_init__",
      "name": "SemanticID.__post_init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 124,
      "end_line": 127,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Compute hash if not provided.",
      "signature": "def __post_init__(self):",
      "body_source": "    def __post_init__(self):\n        \"\"\"Compute hash if not provided.\"\"\"\n        if not self.id_hash:\n            self.id_hash = self._compute_hash()",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID._compute_hash",
      "name": "SemanticID._compute_hash",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 129,
      "end_line": 133,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Compute a short hash from core properties.",
      "signature": "def _compute_hash(self) -> str:",
      "body_source": "    def _compute_hash(self) -> str:\n        \"\"\"Compute a short hash from core properties.\"\"\"\n        # This is an IDENTITY hash, not content hash\n        data = f\"{self.continent.value}{self.fundamental.value}{self.module_path}{self.name}\"\n        return hashlib.md5(data.encode()).hexdigest()[:6]",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.stable_id",
      "name": "SemanticID.stable_id",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 136,
      "end_line": 143,
      "role": "Factory",
      "role_confidence": 60.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [
        "property"
      ],
      "docstring": "Generate the stable, identity-only ID string.\n(Classification | Location | Name | Hash)",
      "signature": "def stable_id(self) -> str:",
      "body_source": "    def stable_id(self) -> str:\n        \"\"\"\n        Generate the stable, identity-only ID string.\n        (Classification | Location | Name | Hash)\n        \"\"\"\n        classification = f\"{self.continent.value}.{self.fundamental.value}.{self.level.value}\"\n        location = f\"{self.module_path}|{self.name}\"\n        return f\"{classification}|{location}|{self.id_hash}\"",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.to_string",
      "name": "SemanticID.to_string",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 145,
      "end_line": 166,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate the full annotated semantic ID string for LLMs.\n(Classification | Location | Name | Props | Smells | Hash)",
      "signature": "def to_string(self) -> str:",
      "body_source": "    def to_string(self) -> str:\n        \"\"\"\n        Generate the full annotated semantic ID string for LLMs.\n        (Classification | Location | Name | Props | Smells | Hash)\n        \"\"\"\n        base = self.stable_id.rsplit(\"|\", 1)[0] # Strip hash temporarily\n        \n        # Properties (sorted for consistency)\n        props = \"|\".join(f\"{k}:{v}\" for k, v in sorted(self.properties.items()))\n        \n        # Smells (sorted for consistency)\n        smells_str = \"\"\n        if self.smell:\n            smells_str = \"|\".join(f\"smell:{k}={v:.2f}\" for k, v in sorted(self.smell.items()))\n            \n        # Combine\n        parts = [base]\n        if props: parts.append(props)\n        if smells_str: parts.append(smells_str)\n        parts.append(self.id_hash)\n        \n        return \"|\".join(parts)",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.__str__",
      "name": "SemanticID.__str__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 168,
      "end_line": 169,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __str__(self) -> str:",
      "body_source": "    def __str__(self) -> str:\n        return self.to_string()",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.__repr__",
      "name": "SemanticID.__repr__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 171,
      "end_line": 172,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __repr__(self) -> str:",
      "body_source": "    def __repr__(self) -> str:\n        return f\"SemanticID({self.to_string()})\"",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.parse",
      "name": "SemanticID.parse",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 175,
      "end_line": 226,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "cls"
        },
        {
          "name": "id_string",
          "type": "str"
        }
      ],
      "return_type": "'SemanticID'",
      "base_classes": [],
      "decorators": [
        "classmethod"
      ],
      "docstring": "Parse a semantic ID string back into an object.",
      "signature": "def parse(cls, id_string: str) -> 'SemanticID':",
      "body_source": "    def parse(cls, id_string: str) -> 'SemanticID':\n        \"\"\"Parse a semantic ID string back into an object.\"\"\"\n        parts = id_string.split(\"|\")\n        \n        # Parse classification\n        classification = parts[0].split(\".\")\n        continent = Continent(classification[0])\n        fundamental = Fundamental(classification[1])\n        level = Level(classification[2])\n        \n        # Parse location\n        module_path = parts[1]\n        name = parts[2]\n        \n        # Parse properties and smells\n        properties = {}\n        smell = {}\n        \n        for part in parts[3:-1]:  # Skip last (hash)\n            if part.startswith(\"smell:\"):\n                # Parse smell:key=value\n                try:\n                    s_content = part[6:] # key=value\n                    # Limit split to 1 to allow strictly formed keys\n                    k, v = s_content.split(\"=\", 1)\n                    smell[k] = float(v)\n                except (ValueError, IndexError):\n                    pass\n            elif \":\" in part:\n                k, v = part.split(\":\", 1)\n                # Try to parse value types\n                if v.lower() == \"true\":\n                    properties[k] = True\n                elif v.lower() == \"false\":\n                    properties[k] = False\n                elif v.isdigit():\n                    properties[k] = int(v)\n                else:\n                    properties[k] = v\n        \n        id_hash = parts[-1]\n        \n        return cls(\n            continent=continent,\n            fundamental=fundamental,\n            level=level,\n            module_path=module_path,\n            name=name,\n            properties=properties,\n            smell=smell,\n            id_hash=id_hash,\n        )",
      "complexity": 0,
      "lines_of_code": 51,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.to_llm_context",
      "name": "SemanticID.to_llm_context",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 228,
      "end_line": 254,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a natural language description for LLM context.\n\nThis is what an LLM reads to understand the entity.",
      "signature": "def to_llm_context(self) -> str:",
      "body_source": "    def to_llm_context(self) -> str:\n        \"\"\"\n        Generate a natural language description for LLM context.\n        \n        This is what an LLM reads to understand the entity.\n        \"\"\"\n        continent_names = {\n            Continent.DATA: \"Data Foundations\",\n            Continent.LOGIC: \"Logic & Flow\",\n            Continent.ORG: \"Organization\",\n            Continent.EXEC: \"Execution\",\n        }\n        \n        level_names = {\n            Level.ATOM: \"syntax primitive\",\n            Level.MOLECULE: \"compound structure\",\n            Level.ORGANELLE: \"architecture role\",\n        }\n        \n        props_desc = \", \".join(f\"{k}={v}\" for k, v in self.properties.items())\n        \n        return (\n            f\"{self.name} is a {level_names[self.level]} in the {continent_names[self.continent]} \"\n            f\"continent, specifically a {self.fundamental.name}. \"\n            f\"Located at {self.module_path}. \"\n            f\"Properties: {props_desc or 'none'}.\"\n        )",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticID.similarity_vector",
      "name": "SemanticID.similarity_vector",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 256,
      "end_line": 267,
      "role": "Specification",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Tuple[int, int, int, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Return a tuple that can be used for similarity comparison.\n\nEntities with similar vectors are architecturally similar.",
      "signature": "def similarity_vector(self) -> Tuple[int, int, int, str]:",
      "body_source": "    def similarity_vector(self) -> Tuple[int, int, int, str]:\n        \"\"\"\n        Return a tuple that can be used for similarity comparison.\n        \n        Entities with similar vectors are architecturally similar.\n        \"\"\"\n        return (\n            list(Continent).index(self.continent),\n            list(Fundamental).index(self.fundamental),\n            list(Level).index(self.level),\n            self.module_path.split(\".\")[0] if self.module_path else \"\",\n        )",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator",
      "name": "SemanticIDGenerator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 274,
      "end_line": 274,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SemanticIDGenerator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.__init__",
      "name": "SemanticIDGenerator.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 282,
      "end_line": 305,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        # Mapping from AST types to classification\n        self.ast_to_classification = {\n            # Data Foundations\n            \"integer\": (Continent.DATA, Fundamental.PRIM, Level.ATOM),\n            \"float\": (Continent.DATA, Fundamental.PRIM, Level.ATOM),\n            \"string\": (Continent.DATA, Fundamental.PRIM, Level.ATOM),\n            \"identifier\": (Continent.DATA, Fundamental.VAR, Level.ATOM),\n            \"attribute\": (Continent.DATA, Fundamental.VAR, Level.ATOM),\n            \n            # Logic & Flow\n            \"call\": (Continent.LOGIC, Fundamental.EXPR, Level.ATOM),\n            \"binary_operator\": (Continent.LOGIC, Fundamental.EXPR, Level.ATOM),\n            \"assignment\": (Continent.LOGIC, Fundamental.STMT, Level.ATOM),\n            \"return_statement\": (Continent.LOGIC, Fundamental.STMT, Level.ATOM),\n            \"if_statement\": (Continent.LOGIC, Fundamental.CTRL, Level.ATOM),\n            \"for_statement\": (Continent.LOGIC, Fundamental.CTRL, Level.ATOM),\n            \"function_definition\": (Continent.LOGIC, Fundamental.FUNC, Level.MOLECULE),\n            \n            # Organization\n            \"class_definition\": (Continent.ORG, Fundamental.AGG, Level.MOLECULE),\n            \"import_statement\": (Continent.ORG, Fundamental.MOD, Level.ATOM),\n            \"import_from_statement\": (Continent.ORG, Fundamental.MOD, Level.ATOM),\n        }",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.generate_ids",
      "name": "SemanticIDGenerator.generate_ids",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 307,
      "end_line": 359,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "codebase"
        }
      ],
      "return_type": "List[SemanticID]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate semantic IDs for the entire codebase.",
      "signature": "def generate_ids(self, codebase) -> List[SemanticID]:",
      "body_source": "    def generate_ids(self, codebase) -> List[SemanticID]:\n        \"\"\"Generate semantic IDs for the entire codebase.\"\"\"\n        from dataclasses import asdict\n        ids = []\n\n        # \ud83d\ude80 UNIFICATION: Pre-scan with TreeSitterUniversalEngine for high-fidelity rules\n        # This bridges the gap between the Benchmark Engine and the CLI\n        particle_map = {}\n        try:\n            # Try core import first, then local\n            try:\n                from core.tree_sitter_engine import TreeSitterUniversalEngine\n            except ImportError:\n                from tree_sitter_engine import TreeSitterUniversalEngine\n                \n            engine = TreeSitterUniversalEngine()\n            print(\"  \u26a1 Enhancing analysis with TreeSitterUniversalEngine rules...\")\n            \n            for file_path, code in codebase.files.items():\n                if file_path.endswith(\".py\"):\n                    try:\n                        particles = engine._extract_python_particles_ast(code, file_path)\n                        for p in particles:\n                            # Key by (file, qualified_name)\n                            key = (file_path, p['name'])\n                            particle_map[key] = p['type']\n                    except Exception:\n                        pass\n        except ImportError:\n            pass\n        \n        # Functions\n        for func in codebase.functions.values():\n            # Convert dataclass to dict for compatibility\n            data = asdict(func)\n            \n            # Determine qualified name for matching\n            qname = func.name\n            if \":\" in func.id:\n                suffix = func.id.split(\":\")[-1]\n                if \".\" in suffix:\n                    qname = suffix\n            \n            refined_type = particle_map.get((func.file, qname))\n            ids.append(self.from_function(data, func.file, refined_type))\n            \n        # Classes\n        for cls in codebase.classes.values():\n            data = asdict(cls)\n            refined_type = particle_map.get((cls.file, cls.name))\n            ids.append(self.from_class(data, cls.file, refined_type))\n            \n        return ids",
      "complexity": 0,
      "lines_of_code": 52,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.from_function",
      "name": "SemanticIDGenerator.from_function",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 361,
      "end_line": 423,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "func_data",
          "type": "Dict"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "refined_type",
          "type": "Optional[str]",
          "default": "None"
        }
      ],
      "return_type": "SemanticID",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate semantic ID for a function.",
      "signature": "def from_function(self, func_data: Dict, file_path: str, refined_type: Optional[str] = None) -> SemanticID:",
      "body_source": "    def from_function(self, func_data: Dict, file_path: str, refined_type: Optional[str] = None) -> SemanticID:\n        \"\"\"Generate semantic ID for a function.\"\"\"\n        \n        # Determine classification based on function properties\n        is_async = func_data.get(\"is_async\", False)\n        has_io = any(call in str(func_data.get(\"calls\", [])).lower() \n                     for call in [\"save\", \"insert\", \"fetch\", \"request\", \"open\", \"write\"])\n        \n        # Is it a handler/organelle?\n        name = func_data.get(\"name\", \"\")\n        is_handler = any(x in name.lower() for x in [\"handle\", \"execute\", \"run\", \"process\"])\n        is_validator = name.lower().startswith(\"validate\")\n        \n        # Confidence Scoring\n        confidence = 50  # Base confidence for heuristic/structural guess\n        \n        # Override with refined type (High Fidelity)\n        if refined_type and refined_type != \"Unknown\":\n            confidence = 95\n            if refined_type == \"Validator\": is_validator = True\n            if refined_type == \"Command\": is_handler = True\n            \n        if refined_type == \"Configuration\":\n             level = Level.MOLECULE\n             fundamental = Fundamental.AGG \n             continent = Continent.ORG\n        elif is_handler or is_validator:\n            level = Level.ORGANELLE\n            fundamental = Fundamental.HANDLER\n            continent = Continent.EXEC\n            # Boost confidence if name explicitly matches pattern\n            if not refined_type and (is_handler or is_validator):\n                confidence = 70\n        else:\n            level = Level.MOLECULE\n            fundamental = Fundamental.FUNC\n            continent = Continent.LOGIC\n        \n        # Build properties\n        combined_properties = {\n            \"async\": is_async,\n            \"io\": has_io,\n            \"params\": len(func_data.get(\"parameters\", [])),\n            \"calls\": len(func_data.get(\"calls\", [])),\n            \"lines\": func_data.get(\"end_line\", 0) - func_data.get(\"start_line\", 0),\n            \"confidence\": confidence,\n        }\n        \n        # Remove false booleans/None but PRESERVE ZEROS\n        combined_properties = {k: v for k, v in combined_properties.items() if v is not None and v is not False}\n        \n        # Module path from file\n        module_path = file_path.replace(\"/\", \".\").replace(\".py\", \"\")\n        \n        return SemanticID(\n            continent=continent,\n            fundamental=fundamental,\n            level=level,\n            module_path=module_path,\n            name=name,\n            properties=combined_properties,\n            evidence=[refined_type] if refined_type else [],\n        )",
      "complexity": 0,
      "lines_of_code": 62,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.from_class",
      "name": "SemanticIDGenerator.from_class",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 425,
      "end_line": 513,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "class_data",
          "type": "Dict"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "refined_type",
          "type": "Optional[str]",
          "default": "None"
        }
      ],
      "return_type": "SemanticID",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate semantic ID for a class.",
      "signature": "def from_class(self, class_data: Dict, file_path: str, refined_type: Optional[str] = None) -> SemanticID:",
      "body_source": "    def from_class(self, class_data: Dict, file_path: str, refined_type: Optional[str] = None) -> SemanticID:\n        \"\"\"Generate semantic ID for a class.\"\"\"\n        \n        name = class_data.get(\"name\", \"\")\n        bases = class_data.get(\"bases\", [])\n        methods = class_data.get(\"methods\", [])\n        instance_vars = class_data.get(\"instance_vars\", [])\n        \n        # Determine if it's a DDD pattern\n        is_entity = \"id\" in [v.lower() for v in instance_vars]\n        is_repository = \"Repository\" in name or any(\"save\" in m or \"find\" in m for m in methods)\n        is_usecase = \"UseCase\" in name or \"execute\" in methods\n        is_value_object = not is_entity and len(instance_vars) > 0\n        \n        # Confidence Scoring\n        confidence = 50   # Default: Structural guess\n        \n        # Override with refined type (High Fidelity)\n        if refined_type and refined_type != \"Unknown\":\n             confidence = 95\n             is_repository = refined_type in [\"Repository\", \"RepositoryImpl\"]\n             is_usecase = refined_type == \"UseCase\"\n             is_entity = refined_type == \"Entity\"\n             is_value_object = refined_type == \"ValueObject\"\n             \n        if refined_type == \"Configuration\" or refined_type == \"BaseSettings\":\n             level = Level.MOLECULE\n             fundamental = Fundamental.AGG\n             continent = Continent.ORG\n             # properties handling moved to bottom to avoid overwrite\n\n        elif refined_type == \"DomainEvent\":\n             level = Level.MOLECULE\n             fundamental = Fundamental.AGG\n             continent = Continent.LOGIC # Events are data+logic\n        elif refined_type in [\"Validator\", \"Command\", \"UseCase\", \"Controller\", \"Service\", \"Algorithm\"]:\n             level = Level.ORGANELLE\n             fundamental = Fundamental.HANDLER\n             continent = Continent.EXEC\n        elif is_repository:\n            level = Level.ORGANELLE\n            fundamental = Fundamental.HANDLER\n            continent = Continent.EXEC\n            if not refined_type: confidence = 70\n        elif is_usecase:\n            level = Level.ORGANELLE\n            fundamental = Fundamental.HANDLER\n            continent = Continent.EXEC\n            if not refined_type: confidence = 70\n        elif is_entity:\n            level = Level.MOLECULE\n            fundamental = Fundamental.AGG\n            continent = Continent.ORG\n            if not refined_type: confidence = 60 # Field heuristic\n        else:\n            level = Level.MOLECULE\n            fundamental = Fundamental.AGG\n            continent = Continent.ORG\n        \n        # Build properties\n        properties = {\n            \"bases\": len(bases),\n            \"methods\": len(methods),\n            \"vars\": len(instance_vars),\n        }\n        \n        if is_entity:\n            properties[\"has_id\"] = True\n        if is_repository:\n            properties[\"repo\"] = True\n        if is_usecase:\n            properties[\"use_case\"] = True\n        if refined_type in [\"Configuration\", \"BaseSettings\"]:\n            properties[\"config\"] = True\n\n        properties = {k: v for k, v in properties.items() if v is not None and v is not False}\n        properties[\"confidence\"] = confidence\n        \n        module_path = file_path.replace(\"/\", \".\").replace(\".py\", \"\")\n        \n        return SemanticID(\n            continent=continent,\n            fundamental=fundamental,\n            level=level,\n            module_path=module_path,\n            name=name,\n            properties=properties,\n            evidence=[refined_type] if refined_type else [],\n        )",
      "complexity": 0,
      "lines_of_code": 88,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.from_atom",
      "name": "SemanticIDGenerator.from_atom",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 515,
      "end_line": 532,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "ast_type",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "line",
          "type": "int"
        }
      ],
      "return_type": "SemanticID",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate semantic ID for an atom.",
      "signature": "def from_atom(self, ast_type: str, file_path: str, line: int) -> SemanticID:",
      "body_source": "    def from_atom(self, ast_type: str, file_path: str, line: int) -> SemanticID:\n        \"\"\"Generate semantic ID for an atom.\"\"\"\n        \n        classification = self.ast_to_classification.get(\n            ast_type, \n            (Continent.DATA, Fundamental.PRIM, Level.ATOM)\n        )\n        \n        module_path = file_path.replace(\"/\", \".\").replace(\".py\", \"\")\n        \n        return SemanticID(\n            continent=classification[0],\n            fundamental=classification[1],\n            level=classification[2],\n            module_path=module_path,\n            name=f\"{ast_type}@L{line}\",\n            properties={\"line\": line},\n        )",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticIDGenerator.from_particle",
      "name": "SemanticIDGenerator.from_particle",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 534,
      "end_line": 604,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle",
          "type": "Dict"
        },
        {
          "name": "smells",
          "type": "Dict[str, float]",
          "default": "None"
        }
      ],
      "return_type": "SemanticID",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate semantic ID from a Universal Detector particle.",
      "signature": "def from_particle(self, particle: Dict, smells: Dict[str, float] = None) -> SemanticID:",
      "body_source": "    def from_particle(self, particle: Dict, smells: Dict[str, float] = None) -> SemanticID:\n        \"\"\"Generate semantic ID from a Universal Detector particle.\"\"\"\n        ptype = particle.get(\"type\", \"Unknown\")\n        name = particle.get(\"name\", \"\")\n        file_path = particle.get(\"file_path\", \"\")\n        symbol_kind = particle.get(\"symbol_kind\", \"\")\n        \n        # Default classification\n        continent = Continent.ORG\n        fundamental = Fundamental.AGG\n        level = Level.MOLECULE\n        \n        # Map known types\n        if ptype in [\"UseCase\", \"Controller\", \"EventHandler\", \"Observer\", \"Command\", \"Query\"]:\n            continent = Continent.EXEC\n            fundamental = Fundamental.HANDLER\n            level = Level.ORGANELLE\n        elif ptype in [\"Repository\", \"RepositoryImpl\", \"Service\", \"DomainService\", \"Factory\", \"Policy\"]:\n            continent = Continent.EXEC\n            fundamental = Fundamental.HANDLER\n            level = Level.ORGANELLE\n        elif ptype in [\"Entity\", \"ValueObject\", \"DTO\", \"Specification\"]:\n            continent = Continent.ORG\n            fundamental = Fundamental.AGG\n            level = Level.MOLECULE\n        elif symbol_kind in {\"function\", \"method\"}:\n            continent = Continent.LOGIC\n            fundamental = Fundamental.FUNC\n            level = Level.MOLECULE\n        elif symbol_kind in {\"variable\", \"const\"}:\n            continent = Continent.DATA\n            fundamental = Fundamental.VAR\n            level = Level.MOLECULE\n        elif symbol_kind in {\"type\", \"interface\", \"enum\"}:\n            continent = Continent.ORG\n            fundamental = Fundamental.TYPE\n            level = Level.MOLECULE\n            \n        module_path = file_path.replace(\"/\", \".\").replace(\".py\", \"\")\n        repo_marker = Path(__file__).resolve().parents[1].name\n        for marker in (repo_marker, \"spectrometer_v12_minimal\", \"standard-code-spectrometer\"):\n            if marker in module_path:\n                try:\n                    idx = module_path.index(marker)\n                    module_path = module_path[idx + len(marker) + 1 :]\n                except ValueError:\n                    pass\n                break\n        if module_path.startswith(\".\"): module_path = module_path[1:]\n        \n        properties = {\n            \"type\": ptype,\n            \"line\": particle.get(\"line\", 0),\n            \"confidence\": particle.get(\"confidence\", 0),\n            \"file_path\": file_path,\n            \"symbol_kind\": symbol_kind,\n        }\n\n        parent = particle.get(\"parent\")\n        if isinstance(parent, str) and parent:\n            properties[\"parent\"] = parent\n        \n        return SemanticID(\n            continent=continent,\n            fundamental=fundamental,\n            level=level,\n            module_path=module_path,\n            name=name,\n            properties=properties,\n            smell=smells or {},\n        )",
      "complexity": 0,
      "lines_of_code": 70,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix",
      "name": "SemanticMatrix",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 611,
      "end_line": 611,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SemanticMatrix:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix.__init__",
      "name": "SemanticMatrix.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 621,
      "end_line": 626,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.ids: List[SemanticID] = []\n        self.by_continent: Dict[Continent, List[SemanticID]] = {c: [] for c in Continent}\n        self.by_fundamental: Dict[Fundamental, List[SemanticID]] = {f: [] for f in Fundamental}\n        self.by_level: Dict[Level, List[SemanticID]] = {l: [] for l in Level}\n        self.by_module: Dict[str, List[SemanticID]] = {}",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix.add",
      "name": "SemanticMatrix.add",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 628,
      "end_line": 638,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "semantic_id",
          "type": "SemanticID"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Add a semantic ID to the matrix.",
      "signature": "def add(self, semantic_id: SemanticID):",
      "body_source": "    def add(self, semantic_id: SemanticID):\n        \"\"\"Add a semantic ID to the matrix.\"\"\"\n        self.ids.append(semantic_id)\n        self.by_continent[semantic_id.continent].append(semantic_id)\n        self.by_fundamental[semantic_id.fundamental].append(semantic_id)\n        self.by_level[semantic_id.level].append(semantic_id)\n        \n        module = semantic_id.module_path.split(\".\")[0] if semantic_id.module_path else \"root\"\n        if module not in self.by_module:\n            self.by_module[module] = []\n        self.by_module[module].append(semantic_id)",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix.to_llm_context",
      "name": "SemanticMatrix.to_llm_context",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 640,
      "end_line": 662,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "max_items",
          "type": "int",
          "default": "100"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a complete LLM context from the matrix.\n\nThis is what you feed to an LLM to understand the codebase.",
      "signature": "def to_llm_context(self, max_items: int = 100) -> str:",
      "body_source": "    def to_llm_context(self, max_items: int = 100) -> str:\n        \"\"\"\n        Generate a complete LLM context from the matrix.\n        \n        This is what you feed to an LLM to understand the codebase.\n        \"\"\"\n        lines = [\n            \"# Codebase Semantic Map\",\n            \"\",\n            f\"Total entities: {len(self.ids)}\",\n            \"\",\n        ]\n        \n        # Group by continent\n        for continent in Continent:\n            items = self.by_continent[continent]\n            if items:\n                lines.append(f\"## {continent.name} ({len(items)} items)\")\n                for item in items[:max_items // 4]:\n                    lines.append(f\"  - {item.to_string()}\")\n                lines.append(\"\")\n        \n        return \"\\n\".join(lines)",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix.get_stats",
      "name": "SemanticMatrix.get_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 664,
      "end_line": 671,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get matrix statistics.",
      "signature": "def get_stats(self) -> Dict:",
      "body_source": "    def get_stats(self) -> Dict:\n        \"\"\"Get matrix statistics.\"\"\"\n        return {\n            \"total\": len(self.ids),\n            \"by_continent\": {c.name: len(self.by_continent[c]) for c in Continent},\n            \"by_level\": {l.name: len(self.by_level[l]) for l in Level},\n            \"modules\": len(self.by_module),\n        }",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py:SemanticMatrix.export_for_embedding",
      "name": "SemanticMatrix.export_for_embedding",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/semantic_ids.py",
      "start_line": 673,
      "end_line": 686,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export IDs in a format suitable for vector embedding.\n\nEach entry can be embedded and used for semantic search.",
      "signature": "def export_for_embedding(self) -> List[Dict]:",
      "body_source": "    def export_for_embedding(self) -> List[Dict]:\n        \"\"\"\n        Export IDs in a format suitable for vector embedding.\n        \n        Each entry can be embedded and used for semantic search.\n        \"\"\"\n        return [\n            {\n                \"id\": sid.to_string(),\n                \"text\": sid.to_llm_context(),\n                \"vector_key\": sid.similarity_vector(),\n            }\n            for sid in self.ids\n        ]",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py:call_ollama",
      "name": "call_ollama",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py",
      "start_line": 24,
      "end_line": 68,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "system_prompt",
          "type": "str"
        },
        {
          "name": "user_prompt",
          "type": "str"
        },
        {
          "name": "model",
          "type": "str",
          "default": "'qwen2.5:7b-instruct'"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Call Ollama via CLI for classification.",
      "signature": "def call_ollama(system_prompt: str, user_prompt: str, model: str = \"qwen2.5:7b-instruct\") -> dict:",
      "body_source": "def call_ollama(system_prompt: str, user_prompt: str, model: str = \"qwen2.5:7b-instruct\") -> dict:\n    \"\"\"Call Ollama via CLI for classification.\"\"\"\n    import subprocess\n    \n    # Combine prompts\n    full_prompt = f\"\"\"{system_prompt}\n\n{user_prompt}\n\nIMPORTANT: Respond with valid JSON only, no markdown, no explanation outside JSON.\"\"\"\n    \n    try:\n        result = subprocess.run(\n            [\"ollama\", \"run\", model],\n            input=full_prompt,\n            capture_output=True,\n            text=True,\n            timeout=60\n        )\n        \n        response = result.stdout.strip()\n        \n        # Try to parse JSON from response\n        # Sometimes LLM wraps in markdown code blocks\n        if \"```json\" in response:\n            response = response.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in response:\n            response = response.split(\"```\")[1].split(\"```\")[0].strip()\n        \n        return json.loads(response)\n        \n    except subprocess.TimeoutExpired:\n        print(\"\u26a0\ufe0f LLM call timed out\")\n        return {\"error\": \"timeout\", \"role\": \"Unknown\", \"confidence\": 0}\n    except json.JSONDecodeError as e:\n        print(f\"\u26a0\ufe0f LLM returned invalid JSON\")\n        # Try to extract role from raw response\n        raw = result.stdout if 'result' in dir() else \"\"\n        for role in [\"Test\", \"Factory\", \"Service\", \"Entity\", \"Repository\", \"UseCase\", \"Configuration\"]:\n            if role in raw:\n                return {\"role\": role, \"confidence\": 0.5, \"evidence\": [], \"reasoning\": f\"Fallback extraction: found '{role}' in response\"}\n        return {\"error\": \"invalid_json\", \"role\": \"Unknown\", \"confidence\": 0}\n    except Exception as e:\n        print(f\"\u26a0\ufe0f LLM call failed: {e}\")\n        return {\"error\": str(e), \"role\": \"Unknown\", \"confidence\": 0}",
      "complexity": 0,
      "lines_of_code": 44,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py:classify_component",
      "name": "classify_component",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py",
      "start_line": 71,
      "end_line": 83,
      "role": "View",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "card",
          "type": "ComponentCard"
        },
        {
          "name": "validator",
          "type": "EvidenceValidator"
        }
      ],
      "return_type": "ClassificationResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a single component using LLM.",
      "signature": "def classify_component(card: ComponentCard, validator: EvidenceValidator) -> ClassificationResult:",
      "body_source": "def classify_component(card: ComponentCard, validator: EvidenceValidator) -> ClassificationResult:\n    \"\"\"Classify a single component using LLM.\"\"\"\n    # Format prompts\n    system_prompt = format_system_prompt()\n    user_prompt = format_card_for_llm(card)\n    \n    # Call LLM\n    raw_result = call_ollama(system_prompt, user_prompt)\n    \n    # Validate result\n    result = validator.validate(raw_result, card)\n    \n    return result",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py:test_pipeline",
      "name": "test_pipeline",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/llm_test.py",
      "start_line": 86,
      "end_line": 161,
      "role": "Test",
      "role_confidence": 90.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "graph_path",
          "type": "str"
        },
        {
          "name": "repo_path",
          "type": "str"
        },
        {
          "name": "limit",
          "type": "int",
          "default": "5"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test the full classification pipeline.",
      "signature": "def test_pipeline(graph_path: str, repo_path: str, limit: int = 5):",
      "body_source": "def test_pipeline(graph_path: str, repo_path: str, limit: int = 5):\n    \"\"\"Test the full classification pipeline.\"\"\"\n    print(\"=\" * 60)\n    print(\"\ud83e\uddea LLM CLASSIFICATION PIPELINE TEST\")\n    print(\"=\" * 60)\n    print()\n    \n    # Load graph\n    with open(graph_path) as f:\n        graph_data = json.load(f)\n    \n    print(f\"\ud83d\udcc2 Repo: {repo_path}\")\n    print(f\"\ud83d\udcca Graph: {graph_path}\")\n    \n    # Count unknowns\n    components = graph_data.get(\"components\", {})\n    unknowns = [c for c in components.values() if c.get(\"type\") == \"Unknown\"]\n    print(f\"\u2753 Unknown nodes: {len(unknowns)}\")\n    print(f\"\ud83c\udfaf Testing on: {min(limit, len(unknowns))} samples\")\n    print()\n    \n    # Extract ComponentCards\n    print(\"\ud83d\udce6 Extracting ComponentCards...\")\n    extractor = SmartExtractor(repo_path)\n    cards = extractor.extract_unknowns(graph_data, limit=limit)\n    print(f\"   Extracted: {len(cards)} cards\")\n    print()\n    \n    # Classify each\n    validator = EvidenceValidator(strict=True)\n    results = []\n    \n    print(\"\ud83e\udd16 Classifying with LLM...\")\n    for i, card in enumerate(cards, 1):\n        print(f\"   [{i}/{len(cards)}] {card.name[:40]}...\", end=\" \", flush=True)\n        result = classify_component(card, validator)\n        results.append(result)\n        print(f\"\u2192 {result.role} ({result.confidence:.0%})\")\n    \n    # Summary\n    print()\n    print(\"=\" * 60)\n    print(\"\ud83d\udcca RESULTS SUMMARY\")\n    print(\"=\" * 60)\n    print()\n    \n    classified = [r for r in results if r.role != \"Unknown\"]\n    unknown_still = [r for r in results if r.role == \"Unknown\"]\n    \n    print(f\"\u2705 Classified: {len(classified)}/{len(results)} ({len(classified)/len(results)*100:.0f}%)\")\n    print(f\"\u2753 Still Unknown: {len(unknown_still)}/{len(results)}\")\n    print()\n    \n    print(\"\ud83d\udccb Classifications:\")\n    for r in results:\n        evidence_count = len(r.evidence)\n        print(f\"   {r.node_id[:60]}...\")\n        print(f\"      \u2192 {r.role} ({r.confidence:.0%}, {evidence_count} evidence)\")\n        if r.reasoning:\n            print(f\"      \ud83d\udcac {r.reasoning[:80]}...\")\n    \n    # Return aggregated stats\n    return {\n        \"total\": len(results),\n        \"classified\": len(classified),\n        \"unknown\": len(unknown_still),\n        \"results\": [\n            {\n                \"node_id\": r.node_id,\n                \"role\": r.role,\n                \"confidence\": r.confidence,\n                \"evidence_count\": len(r.evidence),\n            }\n            for r in results\n        ]\n    }",
      "complexity": 0,
      "lines_of_code": 75,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py:HeuristicClassifier",
      "name": "HeuristicClassifier",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py",
      "start_line": 14,
      "end_line": 14,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class HeuristicClassifier:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py:HeuristicClassifier.__init__",
      "name": "HeuristicClassifier.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py",
      "start_line": 164,
      "end_line": 166,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.discovered_patterns: Dict[str, int] = Counter()\n        self.unknown_names: List[str] = []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py:HeuristicClassifier.classify_by_pattern",
      "name": "HeuristicClassifier.classify_by_pattern",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py",
      "start_line": 168,
      "end_line": 504,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Deterministically classify a function/method name by pattern.\nReturns (type, confidence).",
      "signature": "def classify_by_pattern(self, name: str) -> Tuple[str, float]:",
      "body_source": "    def classify_by_pattern(self, name: str) -> Tuple[str, float]:\n        \"\"\"\n        Deterministically classify a function/method name by pattern.\n        Returns (type, confidence).\n        \"\"\"\n        if not name:\n            return ('Unknown', 0.0)\n        \n        # Get the short name (last part after dot)\n        short = name.split('.')[-1] if '.' in name else name\n        short_lower = short.lower()\n        full_name_lower = name.lower()\n        \n        # 0. TEST CONTEXT DETECTION (highest priority after dunders)\n        # Check if this is in a test context based on:\n        # - Full name contains \"Test\" (class name)\n        # - Full name contains \"test_\" anywhere\n        # - Name ends with \"_test\" or \"_tests\"\n        # - File path would contain \"test\" (handled separately)\n        if any(pattern in full_name_lower for pattern in ['test.', '.test', 'test_', '_test', 'tests.', '.tests']):\n            self.discovered_patterns['test_context'] += 1\n            return ('Test', 90.0)\n        \n        # 1. Check dunder methods (highest confidence)\n        if short_lower.startswith('__') and short_lower.endswith('__'):\n            for dunder, role in self.DUNDER_ROLES.items():\n                if short_lower == dunder:\n                    return (role, 95.0)\n            # Unknown dunder\n            return ('Utility', 70.0)\n        \n        # 2. Check prefix patterns\n        for prefix, role in self.PREFIX_ROLES.items():\n            if short_lower.startswith(prefix):\n                self.discovered_patterns[f'prefix:{prefix}'] += 1\n                return (role, 85.0)\n        \n        # 2.5 Check Java/TypeScript/Go patterns (camelCase prefixes)\n        for prefix, role in self.JAVA_TS_PREFIX_ROLES.items():\n            # Check exact match for short names\n            if short == prefix or short_lower == prefix.lower():\n                self.discovered_patterns[f'java_ts:{prefix}'] += 1\n                return (role, 85.0)\n            # Check camelCase prefix (e.g., testUserLogin, shouldReturnUser)\n            if short.startswith(prefix) and len(short) > len(prefix):\n                next_char = short[len(prefix)]\n                if next_char.isupper() or next_char == '_':\n                    self.discovered_patterns[f'java_ts:{prefix}'] += 1\n                    return (role, 80.0)        \n        # 3. Check suffix patterns (both lowercase and original case)\n        for suffix, role in self.SUFFIX_ROLES.items():\n            # Check lowercase suffix\n            if short_lower.endswith(suffix.lower()):\n                self.discovered_patterns[f'suffix:{suffix}'] += 1\n                return (role, 85.0)\n            # Check original case suffix (e.g., UserService)\n            if short.endswith(suffix):\n                self.discovered_patterns[f'suffix:{suffix}'] += 1\n                return (role, 85.0)\n        \n        # 4. Private methods (single underscore prefix)\n        if short_lower.startswith('_') and not short_lower.startswith('__'):\n            self.discovered_patterns['_private'] += 1\n            return ('Internal', 70.0)\n        \n        # 5. Entry points / main entry\n        if short_lower in ('index', 'main', 'app', 'application', 'root', 'home', 'default'):\n            self.discovered_patterns['entry_point'] += 1\n            return ('Controller', 75.0)\n        \n        # 6. Fixture/Example patterns (common in framework docs/tests)\n        # IMPORTANT: Only match if NOT also matching a suffix pattern (e.g., UserService)\n        fixture_tokens = {'fake', 'mock', 'stub', 'dummy', 'sample', 'example', 'demo', \n                         'fixture', 'hero', 'item', 'todo', 'post', 'comment'}\n        # Remove 'user' from here - too common in real class names\n        is_fixture = any(token in short_lower for token in fixture_tokens)\n        # Check if it's actually a domain class pattern\n        has_domain_suffix = any(short.endswith(s) for s in ['Service', 'Repository', 'Controller', 'Handler', 'Factory', 'Builder', 'Mapper', 'Validator'])\n        if is_fixture and not has_domain_suffix:\n            self.discovered_patterns['fixture/example'] += 1\n            return ('Fixture', 70.0)\n        \n        # 7. Common framework patterns\n        if any(x in short_lower for x in ['endpoint', 'route', 'view', 'page']):\n            self.discovered_patterns['endpoint'] += 1\n            return ('Controller', 75.0)\n        \n        # 8. Data operations\n        if any(x in short_lower for x in ['decode', 'encode', 'serialize', 'deserialize', 'dump', 'load']):\n            self.discovered_patterns['data_ops'] += 1\n            return ('Mapper', 75.0)\n        \n        # 9. Session/state management\n        if any(x in short_lower for x in ['session', 'state', 'context', 'scope']):\n            self.discovered_patterns['state_mgmt'] += 1\n            return ('Service', 70.0)\n        \n        # 10. Common verbs that are typically Commands\n        if any(x in short_lower for x in ['login', 'logout', 'register', 'submit', 'send', 'publish', 'emit']):\n            self.discovered_patterns['action_verb'] += 1\n            return ('Command', 75.0)\n        \n        # 11. Common verbs that are typically Queries\n        if any(x in short_lower for x in ['read', 'retrieve', 'search', 'lookup', 'resolve']):\n            self.discovered_patterns['query_verb'] += 1\n            return ('Query', 75.0)\n        \n        # 12. Collector/aggregator patterns\n        if any(x in short_lower for x in ['collect', 'gather', 'aggregate', 'combine', 'merge']):\n            self.discovered_patterns['aggregator'] += 1\n            return ('Service', 70.0)\n        \n        # 13. Output/display patterns\n        if any(x in short_lower for x in ['print', 'display', 'show', 'output', 'terminal', 'console']):\n            self.discovered_patterns['output'] += 1\n            return ('Utility', 70.0)\n        \n        # 14. Flush/sync patterns\n        if any(x in short_lower for x in ['flush', 'sync', 'commit', 'persist', 'write']):\n            self.discovered_patterns['persist'] += 1\n            return ('Command', 75.0)\n        \n        # ========== NEW PATTERNS FOR 100% COVERAGE ==========\n        \n        # 15. Database/DB patterns\n        if any(x in short_lower for x in ['database', 'db', 'sql', 'query', 'cursor', 'connection', 'pool']):\n            self.discovered_patterns['database'] += 1\n            return ('Repository', 75.0)\n        \n        # 16. Schema/Migration patterns  \n        if any(x in short_lower for x in ['schema', 'migration', 'upgrade', 'downgrade', 'migrate', 'revision']):\n            self.discovered_patterns['migration'] += 1\n            return ('Command', 75.0)\n        \n        # 17. Introspection/Reflection patterns\n        if any(x in short_lower for x in ['introspect', 'reflect', 'inspect', 'meta', 'describe']):\n            self.discovered_patterns['introspection'] += 1\n            return ('Query', 75.0)\n        \n        # 18. Visitor pattern\n        if any(x in short_lower for x in ['visit', 'visitor', 'walk', 'traverse', 'accept']):\n            self.discovered_patterns['visitor'] += 1\n            return ('Service', 75.0)\n        \n        # 19. Widget/Input/UI patterns\n        if any(x in short_lower for x in ['widget', 'input', 'button', 'slider', 'select', 'checkbox', \n                                           'radio', 'dropdown', 'picker', 'chooser', 'selector']):\n            self.discovered_patterns['widget'] += 1\n            return ('Controller', 75.0)\n        \n        # 20. Dialog/Modal/Popup patterns\n        if any(x in short_lower for x in ['dialog', 'modal', 'popup', 'overlay', 'toast', 'alert', 'confirm']):\n            self.discovered_patterns['dialog'] += 1\n            return ('Controller', 75.0)\n        \n        # 21. Property/Attribute patterns\n        if any(x in short_lower for x in ['property', 'prop', 'attribute', 'attr', 'value', 'field']):\n            self.discovered_patterns['property'] += 1\n            return ('Query', 70.0)\n        \n        # 22. URL/Path/Route patterns\n        if any(x in short_lower for x in ['url', 'path', 'uri', 'link', 'href', 'redirect']):\n            self.discovered_patterns['url'] += 1\n            return ('Query', 70.0)\n        \n        # 23. Clear/Reset/Cleanup patterns\n        if any(x in short_lower for x in ['clear', 'reset', 'clean', 'purge', 'wipe', 'remove_all']):\n            self.discovered_patterns['cleanup'] += 1\n            return ('Command', 75.0)\n        \n        # 24. Error/Exception/Not Found patterns\n        if any(x in short_lower for x in ['error', 'exception', 'not_found', 'missing', 'invalid', 'fail']):\n            self.discovered_patterns['error'] += 1\n            return ('Exception', 75.0)\n        \n        # 25. Mixin patterns\n        if 'mixin' in short_lower:\n            self.discovered_patterns['mixin'] += 1\n            return ('Utility', 75.0)\n        \n        # 26. Callback/Hook/Event patterns\n        if any(x in short_lower for x in ['callback', 'hook', 'event', 'signal', 'listener', 'trigger']):\n            self.discovered_patterns['callback'] += 1\n            return ('EventHandler', 75.0)\n        \n        # 27. Patch/Mock/Stub (test doubles)\n        if any(x in short_lower for x in ['patch', 'mock', 'stub', 'spy', 'double']):\n            self.discovered_patterns['test_double'] += 1\n            return ('Test', 75.0)\n        \n        # 28. Filter/Sort/Order patterns\n        if any(x in short_lower for x in ['filter', 'sort', 'order', 'group', 'partition', 'bucket']):\n            self.discovered_patterns['filter'] += 1\n            return ('Query', 75.0)\n        \n        # 29. Config/Setting/Option patterns\n        if any(x in short_lower for x in ['config', 'setting', 'option', 'preference', 'env', 'environ']):\n            self.discovered_patterns['config'] += 1\n            return ('Configuration', 75.0)\n        \n        # 30. Pool/Cache/Buffer patterns\n        if any(x in short_lower for x in ['pool', 'cache', 'buffer', 'queue', 'stack']):\n            self.discovered_patterns['cache'] += 1\n            return ('Service', 75.0)\n        \n        # 31. Auth/Permission/Access patterns\n        if any(x in short_lower for x in ['auth', 'permission', 'access', 'grant', 'deny', 'role', 'acl']):\n            self.discovered_patterns['auth'] += 1\n            return ('Policy', 75.0)\n        \n        # 32. Log/Trace/Debug patterns\n        if any(x in short_lower for x in ['log', 'trace', 'debug', 'audit', 'metric', 'stat']):\n            self.discovered_patterns['logging'] += 1\n            return ('Utility', 75.0)\n        \n        # 33. Template/Render patterns\n        if any(x in short_lower for x in ['template', 'render', 'generate', 'emit', 'produce']):\n            self.discovered_patterns['template'] += 1\n            return ('Factory', 75.0)\n        \n        # 34. Import/Export patterns\n        if any(x in short_lower for x in ['import', 'export', 'ingest', 'extract', 'download', 'upload']):\n            self.discovered_patterns['importexport'] += 1\n            return ('Service', 75.0)\n        \n        # 35. Iterator/Generator patterns\n        if any(x in short_lower for x in ['iter', 'generator', 'yield', 'stream', 'cursor', 'scroll']):\n            self.discovered_patterns['iterator'] += 1\n            return ('Iterator', 75.0)\n        \n        # 36. Clone/Copy/Duplicate patterns\n        if any(x in short_lower for x in ['clone', 'copy', 'duplicate', 'replicate', 'mirror']):\n            self.discovered_patterns['clone'] += 1\n            return ('Factory', 75.0)\n        \n        # 37. Connect/Disconnect/Open/Close patterns\n        if any(x in short_lower for x in ['connect', 'disconnect', 'open', 'close', 'start', 'stop', 'shutdown', 'terminate']):\n            self.discovered_patterns['lifecycle'] += 1\n            return ('Lifecycle', 75.0)\n        \n        # 38. Wrapper/Decorator patterns\n        if any(x in short_lower for x in ['wrapper', 'decorator', 'wrap', 'decorate', 'proxy']):\n            self.discovered_patterns['wrapper'] += 1\n            return ('Utility', 75.0)\n        \n        # 39. Nested function patterns (common)\n        if any(x in short_lower for x in ['inner', 'outer', 'closure', 'nested', 'wrapped']):\n            self.discovered_patterns['nested'] += 1\n            return ('Internal', 70.0)\n        \n        # 40. Count/Size/Length patterns\n        if any(x in short_lower for x in ['count', 'size', 'length', 'total', 'sum', 'avg', 'min', 'max']):\n            self.discovered_patterns['aggregate'] += 1\n            return ('Query', 75.0)\n        \n        # 41. Compare/Diff/Match patterns  \n        if any(x in short_lower for x in ['compare', 'diff', 'match', 'equal', 'same', 'similar']):\n            self.discovered_patterns['compare'] += 1\n            return ('Specification', 75.0)\n        \n        # 42. Split/Join/Concat patterns\n        if any(x in short_lower for x in ['split', 'join', 'concat', 'append', 'prepend', 'extend']):\n            self.discovered_patterns['string_ops'] += 1\n            return ('Utility', 75.0)\n        \n        # 43. Wait/Sleep/Delay patterns\n        if any(x in short_lower for x in ['wait', 'sleep', 'delay', 'timeout', 'pause', 'retry']):\n            self.discovered_patterns['timing'] += 1\n            return ('Utility', 75.0)\n        \n        # 44. Lock/Mutex/Semaphore patterns\n        if any(x in short_lower for x in ['lock', 'mutex', 'semaphore', 'acquire', 'release', 'synchron']):\n            self.discovered_patterns['concurrency'] += 1\n            return ('Service', 75.0)\n        \n        # 45. Schedule/Job/Task/Worker patterns\n        if any(x in short_lower for x in ['schedule', 'job', 'task', 'worker', 'cron', 'periodic', 'interval']):\n            self.discovered_patterns['scheduler'] += 1\n            return ('Job', 75.0)\n        \n        # 46. Parse/Lex/Token patterns\n        if any(x in short_lower for x in ['parse', 'lex', 'token', 'ast', 'syntax', 'grammar']):\n            self.discovered_patterns['parser'] += 1\n            return ('Utility', 75.0)\n        \n        # 47. Format/Pretty/Style patterns\n        if any(x in short_lower for x in ['format', 'pretty', 'style', 'beautify', 'minify', 'compress']):\n            self.discovered_patterns['format'] += 1\n            return ('Utility', 75.0)\n        \n        # 48. Version/Revision/History patterns\n        if any(x in short_lower for x in ['version', 'revision', 'history', 'changelog', 'release']):\n            self.discovered_patterns['version'] += 1\n            return ('Query', 70.0)\n        \n        # 49. Short names (1-3 chars) are typically variables/lambdas - classify as Utility\n        if len(short_lower) <= 3:\n            self.discovered_patterns['short_name'] += 1\n            return ('Utility', 60.0)\n        \n        # 50. CamelCase class names (starts uppercase) - check for patterns\n        if short[0].isupper():\n            # Classes with specific patterns\n            if any(x in short_lower for x in ['exception', 'error']):\n                return ('Exception', 75.0)\n            if any(x in short_lower for x in ['factory', 'builder', 'creator']):\n                return ('Factory', 75.0)\n            if any(x in short_lower for x in ['handler', 'processor', 'worker']):\n                return ('Service', 75.0)\n            if any(x in short_lower for x in ['validator', 'checker', 'verifier']):\n                return ('Validator', 75.0)\n            if any(x in short_lower for x in ['service', 'manager', 'controller']):\n                return ('Service', 75.0)\n            if any(x in short_lower for x in ['repository', 'store', 'dao']):\n                return ('Repository', 75.0)\n            if any(x in short_lower for x in ['client', 'adapter', 'gateway']):\n                return ('Adapter', 75.0)\n            if any(x in short_lower for x in ['config', 'settings', 'options']):\n                return ('Configuration', 75.0)\n            # Generic class - likely DTO/Entity\n            self.discovered_patterns['noun_entity'] += 1\n            return ('DTO', 65.0)\n        \n        # 51. All remaining functions - classify by structure\n        # If has underscores, likely internal utility\n        if '_' in short_lower:\n            self.discovered_patterns['underscore_func'] += 1\n            return ('Utility', 60.0)\n        \n        # 52. camelCase functions (no underscore, starts lowercase)\n        if short[0].islower() and '_' not in short:\n            self.discovered_patterns['camelCase'] += 1\n            return ('Utility', 60.0)\n        \n        # 53. Record as unknown for pattern analysis (should rarely reach here now)\n        self.unknown_names.append(short_lower)\n        return ('Unknown', 30.0)",
      "complexity": 0,
      "lines_of_code": 336,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py:HeuristicClassifier.get_pattern_report",
      "name": "HeuristicClassifier.get_pattern_report",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py",
      "start_line": 506,
      "end_line": 524,
      "role": "Factory",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate report of discovered patterns.",
      "signature": "def get_pattern_report(self) -> Dict:",
      "body_source": "    def get_pattern_report(self) -> Dict:\n        \"\"\"Generate report of discovered patterns.\"\"\"\n        # Analyze remaining unknown names for new patterns\n        token_freq = Counter()\n        for name in self.unknown_names:\n            tokens = re.findall(r'[a-z]+', name)\n            for token in tokens:\n                if len(token) > 2:\n                    token_freq[token] += 1\n        \n        return {\n            'total_classified': sum(self.discovered_patterns.values()),\n            'total_unknown': len(self.unknown_names),\n            'top_patterns': self.discovered_patterns.most_common(20),\n            'suggested_new_patterns': token_freq.most_common(10),\n            'coverage': sum(self.discovered_patterns.values()) / \n                       (sum(self.discovered_patterns.values()) + len(self.unknown_names)) * 100\n                       if (sum(self.discovered_patterns.values()) + len(self.unknown_names)) > 0 else 0\n        }",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py:apply_heuristics",
      "name": "apply_heuristics",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/heuristic_classifier.py",
      "start_line": 527,
      "end_line": 550,
      "role": "Command",
      "role_confidence": 73.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "particles",
          "type": "List[Dict]"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Apply heuristic classification to particles during pipeline.\nModifies particles in place to update Unknown types.",
      "signature": "def apply_heuristics(particles: List[Dict]) -> List[Dict]:",
      "body_source": "def apply_heuristics(particles: List[Dict]) -> List[Dict]:\n    \"\"\"\n    Apply heuristic classification to particles during pipeline.\n    Modifies particles in place to update Unknown types.\n    \"\"\"\n    discovery = HeuristicClassifier()\n    updated = 0\n    \n    for particle in particles:\n        if particle.get('type') == 'Unknown':\n            name = particle.get('name', '')\n            new_type, confidence = discovery.classify_by_pattern(name)\n            \n            if new_type != 'Unknown':\n                particle['type'] = new_type\n                particle['confidence'] = confidence\n                particle['discovery_method'] = 'heuristic_pattern'\n                updated += 1\n    \n    # Generate report\n    report = discovery.get_pattern_report()\n    report['particles_updated'] = updated\n    \n    return particles, report",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py:LanguageLoader",
      "name": "LanguageLoader",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py",
      "start_line": 12,
      "end_line": 12,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class LanguageLoader:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py:LanguageLoader.load_all",
      "name": "LanguageLoader.load_all",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py",
      "start_line": 34,
      "end_line": 82,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_parser",
      "params": [],
      "return_type": "Tuple[Dict[str, Parser], Dict[str, Language], Dict[str, list]]",
      "base_classes": [],
      "decorators": [
        "staticmethod"
      ],
      "docstring": "Load all available parsers.\nReturns: (parsers, languages, extensions_map)",
      "signature": "def load_all() -> Tuple[Dict[str, Parser], Dict[str, Language], Dict[str, list]]:",
      "body_source": "    def load_all() -> Tuple[Dict[str, Parser], Dict[str, Language], Dict[str, list]]:\n        \"\"\"\n        Load all available parsers.\n        Returns: (parsers, languages, extensions_map)\n        \"\"\"\n        parsers = {}\n        languages = {}\n        extensions = {} # language_key -> [exts]\n        \n        for pkg_name, (lang_name, exts) in LanguageLoader.KNOWN_BINDINGS.items():\n            try:\n                module = importlib.import_module(pkg_name)\n                \n                # Get the language object\n                # Most bindings expose a method like language()\n                # TypeScript is special: language_typescript() and language_tsx()\n                \n                if pkg_name == \"tree_sitter_typescript\":\n                    # Load TypeScript\n                    try:\n                        ts_lang = Language(module.language_typescript())\n                        parsers[\"typescript\"] = Parser(ts_lang)\n                        languages[\"typescript\"] = ts_lang\n                        extensions[\"typescript\"] = [\".ts\"]\n                    except AttributeError:\n                        pass\n                        \n                    # Load TSX\n                    try:\n                        tsx_lang = Language(module.language_tsx())\n                        parsers[\"tsx\"] = Parser(tsx_lang)\n                        languages[\"tsx\"] = tsx_lang\n                        extensions[\"tsx\"] = [\".tsx\"]\n                    except AttributeError:\n                        pass\n                else:\n                    # Standard bindings\n                    if hasattr(module, \"language\"):\n                        lang_obj = Language(module.language())\n                        parsers[lang_name] = Parser(lang_obj)\n                        languages[lang_name] = lang_obj\n                        extensions[lang_name] = exts\n                        \n            except ImportError:\n                continue\n            except Exception as e:\n                print(f\"Warning: Failed to load {pkg_name}: {e}\")\n                \n        return parsers, languages, extensions",
      "complexity": 0,
      "lines_of_code": 48,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py:LanguageLoader.get_supported_languages",
      "name": "LanguageLoader.get_supported_languages",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/language_loader.py",
      "start_line": 85,
      "end_line": 88,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "list",
      "base_classes": [],
      "decorators": [
        "staticmethod"
      ],
      "docstring": "Return list of currently supported (installed) languages.",
      "signature": "def get_supported_languages() -> list:",
      "body_source": "    def get_supported_languages() -> list:\n        \"\"\"Return list of currently supported (installed) languages.\"\"\"\n        _, _, exts = LanguageLoader.load_all()\n        return list(exts.keys())",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:_posix",
      "name": "_posix",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 12,
      "end_line": 13,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "path",
          "type": "Path"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _posix(path: Path) -> str:",
      "body_source": "def _posix(path: Path) -> str:\n    return path.as_posix()",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:_safe_rel",
      "name": "_safe_rel",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 16,
      "end_line": 26,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "repo_root",
          "type": "Path"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _safe_rel(repo_root: Path, file_path: str) -> str:",
      "body_source": "def _safe_rel(repo_root: Path, file_path: str) -> str:\n    try:\n        p = Path(file_path).resolve()\n    except Exception:\n        return file_path.replace(\"\\\\\", \"/\")\n    if p.is_absolute():\n        try:\n            return _posix(p.relative_to(repo_root))\n        except ValueError:\n            return _posix(p)\n    return file_path.replace(\"\\\\\", \"/\")",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:_stable_id",
      "name": "_stable_id",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 29,
      "end_line": 34,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "*parts",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _stable_id(*parts: str) -> str:",
      "body_source": "def _stable_id(*parts: str) -> str:\n    h = hashlib.sha1()\n    for part in parts:\n        h.update(part.encode(\"utf-8\", errors=\"ignore\"))\n        h.update(b\"\\0\")\n    return h.hexdigest()[:12]",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:_group_key",
      "name": "_group_key",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 37,
      "end_line": 48,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "rel_file",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _group_key(rel_file: str) -> str:",
      "body_source": "def _group_key(rel_file: str) -> str:\n    p = rel_file.replace(\"\\\\\", \"/\").lower()\n    if \"/domain/\" in f\"/{p}/\" or p.startswith(\"domain/\"):\n        return \"domain\"\n    if \"/usecase/\" in f\"/{p}/\" or \"/use_case/\" in f\"/{p}/\" or \"/application/\" in f\"/{p}/\":\n        return \"usecase\"\n    if \"/infrastructure/\" in f\"/{p}/\":\n        return \"infrastructure\"\n    if \"/presentation/\" in f\"/{p}/\" or \"/controllers/\" in f\"/{p}/\" or \"/api/\" in f\"/{p}/\":\n        return \"presentation\"\n    first = p.split(\"/\", 1)[0].strip()\n    return first or \"root\"",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:_mermaid_id",
      "name": "_mermaid_id",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 51,
      "end_line": 55,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "group",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _mermaid_id(group: str) -> str:",
      "body_source": "def _mermaid_id(group: str) -> str:\n    safe = \"\".join(ch if ch.isalnum() else \"_\" for ch in group)\n    if not safe or safe[0].isdigit():\n        safe = f\"g_{safe}\"\n    return safe",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ComponentRow",
      "name": "ComponentRow",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 59,
      "end_line": 59,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ComponentRow:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ReportGenerator",
      "name": "ReportGenerator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 74,
      "end_line": 74,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class ReportGenerator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ReportGenerator.generate",
      "name": "ReportGenerator.generate",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 77,
      "end_line": 140,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict[str, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def generate(",
      "body_source": "    def generate(\n        self,\n        *,\n        repo_root: str,\n        analysis_results: list[dict[str, Any]],\n        comprehensive_results: dict[str, Any],\n        output_dir: Path,\n    ) -> dict[str, str]:\n        root = Path(repo_root).resolve()\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        deps_by_file: dict[str, dict[str, Any]] = {}\n        for r in analysis_results:\n            fp = r.get(\"file_path\")\n            if not fp:\n                continue\n            rel = _safe_rel(root, fp)\n            deps = r.get(\"dependencies\") or {}\n            deps_by_file[rel] = deps\n\n        components: list[ComponentRow] = []\n        for p in comprehensive_results.get(\"particles\", []):\n            rel_file = _safe_rel(root, str(p.get(\"file_path\") or \"\"))\n            deps = deps_by_file.get(rel_file, {})\n            internal = len(deps.get(\"internal\") or [])\n            external = len(deps.get(\"external\") or [])\n            stdlib = len(deps.get(\"stdlib\") or [])\n            ptype = str(p.get(\"type\") or \"Unknown\")\n            name = str(p.get(\"name\") or \"\")\n            symbol_kind = str(p.get(\"symbol_kind\") or \"\")\n            line = int(p.get(\"line\") or 0)\n            confidence = float(p.get(\"confidence\") or 0.0)\n            purpose = str(p.get(\"description\") or \"\")\n            evidence = str(p.get(\"evidence\") or \"\")\n            component_id = _stable_id(ptype, name, rel_file, str(line))\n\n            components.append(\n                ComponentRow(\n                    component_id=component_id,\n                    type=ptype,\n                    symbol_kind=symbol_kind,\n                    name=name,\n                    rel_file=rel_file,\n                    line=line,\n                    confidence=confidence,\n                    evidence=evidence[:200],\n                    purpose=purpose,\n                    internal_deps=internal,\n                    external_deps=external,\n                    stdlib_deps=stdlib,\n                )\n            )\n\n        components_csv = output_dir / \"components.csv\"\n        self._write_components_csv(components_csv, components)\n\n        mermaid = self._build_mermaid(root.name, components, comprehensive_results.get(\"dependencies\") or {})\n        report_md = output_dir / \"report.md\"\n        report_md.write_text(\n            self._build_markdown(root, components, comprehensive_results, components_csv.name, mermaid),\n            encoding=\"utf-8\",\n        )\n\n        return {\"report_md\": str(report_md), \"components_csv\": str(components_csv)}",
      "complexity": 0,
      "lines_of_code": 63,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ReportGenerator._write_components_csv",
      "name": "ReportGenerator._write_components_csv",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 142,
      "end_line": 163,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "path",
          "type": "Path"
        },
        {
          "name": "rows",
          "type": "list[ComponentRow]"
        }
      ],
      "return_type": "None",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _write_components_csv(self, path: Path, rows: list[ComponentRow]) -> None:",
      "body_source": "    def _write_components_csv(self, path: Path, rows: list[ComponentRow]) -> None:\n        with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.DictWriter(\n                f,\n                fieldnames=[\n                    \"component_id\",\n                    \"type\",\n                    \"symbol_kind\",\n                    \"name\",\n                    \"rel_file\",\n                    \"line\",\n                    \"confidence\",\n                    \"evidence\",\n                    \"purpose\",\n                    \"internal_deps\",\n                    \"external_deps\",\n                    \"stdlib_deps\",\n                ],\n            )\n            writer.writeheader()\n            for r in rows:\n                writer.writerow(r.__dict__)",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ReportGenerator._build_mermaid",
      "name": "ReportGenerator._build_mermaid",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 165,
      "end_line": 196,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_name",
          "type": "str"
        },
        {
          "name": "components",
          "type": "list[ComponentRow]"
        },
        {
          "name": "dep_summary",
          "type": "dict[str, Any]"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _build_mermaid(self, repo_name: str, components: list[ComponentRow], dep_summary: dict[str, Any]) -> str:",
      "body_source": "    def _build_mermaid(self, repo_name: str, components: list[ComponentRow], dep_summary: dict[str, Any]) -> str:\n        counts_by_group: dict[str, int] = {}\n        for c in components:\n            g = _group_key(c.rel_file)\n            counts_by_group[g] = counts_by_group.get(g, 0) + 1\n\n        edges_by_group: dict[tuple[str, str], int] = {}\n        for edge in dep_summary.get(\"internal_edges\") or []:\n            src = str(edge.get(\"from\") or \"\")\n            dst = str(edge.get(\"to\") or \"\")\n            count = int(edge.get(\"count\") or 0)\n            if not src or not dst or count <= 0:\n                continue\n            gs = _group_key(src)\n            gd = _group_key(dst)\n            if gs == gd:\n                continue\n            edges_by_group[(gs, gd)] = edges_by_group.get((gs, gd), 0) + count\n\n        lines: list[str] = []\n        lines.append(\"graph LR\")\n        lines.append(f\"  %% {repo_name}\")\n\n        for group in sorted(counts_by_group.keys()):\n            gid = _mermaid_id(group)\n            label = f\"{group} ({counts_by_group[group]})\"\n            lines.append(f'  {gid}[\"{label}\"]')\n\n        for (gs, gd), count in sorted(edges_by_group.items(), key=lambda kv: kv[1], reverse=True)[:80]:\n            lines.append(f\"  {_mermaid_id(gs)} -->|{count}| {_mermaid_id(gd)}\")\n\n        return \"\\n\".join(lines) + \"\\n\"",
      "complexity": 0,
      "lines_of_code": 31,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py:ReportGenerator._build_markdown",
      "name": "ReportGenerator._build_markdown",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/report_generator.py",
      "start_line": 198,
      "end_line": 253,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_root",
          "type": "Path"
        },
        {
          "name": "components",
          "type": "list[ComponentRow]"
        },
        {
          "name": "comprehensive_results",
          "type": "dict[str, Any]"
        },
        {
          "name": "components_csv_name",
          "type": "str"
        },
        {
          "name": "mermaid",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _build_markdown(",
      "body_source": "    def _build_markdown(\n        self,\n        repo_root: Path,\n        components: list[ComponentRow],\n        comprehensive_results: dict[str, Any],\n        components_csv_name: str,\n        mermaid: str,\n    ) -> str:\n        summary = comprehensive_results.get(\"summary\") or {}\n        deps = comprehensive_results.get(\"dependencies\") or {}\n\n        typed = [c for c in components if c.type != \"Unknown\"]\n        unknown = [c for c in components if c.type == \"Unknown\"]\n\n        external = deps.get(\"external_packages\") or []\n        top_external = \", \".join([f\"{d['package']}({d['count']})\" for d in external[:15] if d.get(\"package\")])\n\n        lines: list[str] = []\n        lines.append(f\"# Spectrometer Report \u2014 {repo_root.name}\")\n        lines.append(\"\")\n        lines.append(f\"- Generated: {datetime.now().isoformat(timespec='seconds')}\")\n        lines.append(f\"- Repo root: `{repo_root}`\")\n        lines.append(\"\")\n        lines.append(\"## Summary\")\n        lines.append(f\"- Files analyzed: {summary.get('files_analyzed', 0)}\")\n        lines.append(f\"- Components extracted: {summary.get('total_particles_found', 0)}\")\n        if \"recognized_percentage\" in summary:\n            lines.append(f\"- Recognized (non-Unknown): {summary.get('recognized_percentage', 0):.1f}%\")\n        lines.append(f\"- Typed components: {len(typed)}\")\n        lines.append(f\"- Unclassified components: {len(unknown)}\")\n        lines.append(\"\")\n        lines.append(\"## Dependencies\")\n        lines.append(f\"- External packages (top): {top_external or 'n/a'}\")\n        lines.append(\"\")\n        lines.append(\"## Outputs\")\n        lines.append(f\"- Full component list: `{components_csv_name}`\")\n        lines.append(\"- Raw particles: `particles.csv` (no IDs/deps)\")\n        lines.append(\"- Full JSON: `results.json`\")\n        lines.append(\"\")\n        lines.append(\"## Architecture (Mermaid)\")\n        lines.append(\"```mermaid\")\n        lines.append(mermaid.rstrip())\n        lines.append(\"```\")\n        lines.append(\"\")\n        lines.append(\"## Unclassified Samples\")\n        lines.append(\"| component_id | kind | name | file | line | evidence |\")\n        lines.append(\"|---|---|---|---|---|---|\")\n        for c in sorted(unknown, key=lambda x: (x.rel_file, x.line, x.name))[:50]:\n            evidence = (c.evidence or \"\").replace(\"|\", \"\\\\|\")\n            lines.append(\n                f\"| `{c.component_id}` | {c.symbol_kind or '\u2014'} | `{c.name}` | `{c.rel_file}` | {c.line} | `{evidence}` |\"\n            )\n        if len(unknown) > 50:\n            lines.append(f\"\\n(Showing 50/{len(unknown)} unclassified components; full list in `{components_csv_name}`.)\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)",
      "complexity": 0,
      "lines_of_code": 55,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:InferenceRule",
      "name": "InferenceRule",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 24,
      "end_line": 24,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class InferenceRule:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:InferenceRule.__post_init__",
      "name": "InferenceRule.__post_init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 38,
      "end_line": 40,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __post_init__(self):",
      "body_source": "    def __post_init__(self):\n        self.caller_types = self.caller_types or set()\n        self.callee_types = self.callee_types or set()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:infer_from_structure",
      "name": "infer_from_structure",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 133,
      "end_line": 213,
      "role": "Analyzer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "node",
          "type": "Dict"
        }
      ],
      "return_type": "Tuple[str, float, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Infer type from node's own structural properties.\nWorks even when no neighbors have known types.\nReturns (type, confidence, rule_name) or None.",
      "signature": "def infer_from_structure(node: Dict) -> Tuple[str, float, str]:",
      "body_source": "def infer_from_structure(node: Dict) -> Tuple[str, float, str]:\n    \"\"\"\n    Infer type from node's own structural properties.\n    Works even when no neighbors have known types.\n    Returns (type, confidence, rule_name) or None.\n    \"\"\"\n    name = node.get('name', '').split('.')[-1].lower()\n    return_type = node.get('return_type', '').lower()\n    params = node.get('params', [])\n    docstring = node.get('docstring', '').lower()\n    kind = node.get('kind', node.get('symbol_kind', ''))\n    decorators = node.get('decorators', [])\n    base_classes = node.get('base_classes', [])\n    \n    # Skip if already high confidence\n    current_conf = node.get('role_confidence', 0)\n    if current_conf >= 85:\n        return None\n    \n    # -------------------------------------------------------------------------\n    # STRUCTURAL RULE 1: Return type indicates Factory/Builder\n    # -------------------------------------------------------------------------\n    if return_type:\n        # Returns a new object instance \u2192 Factory\n        if any(x in return_type for x in ['new', 'create', 'make', 'build']):\n            return ('Factory', 82.0, 'return_type_factory')\n        # Returns a bool \u2192 likely Specification/Validator\n        if return_type in ('bool', 'boolean'):\n            return ('Specification', 78.0, 'return_type_bool')\n        # Returns an error \u2192 Exception-related\n        if 'error' in return_type or 'err' in return_type:\n            return ('Exception', 78.0, 'return_type_error')\n    \n    # -------------------------------------------------------------------------\n    # STRUCTURAL RULE 2: Parameter patterns\n    # -------------------------------------------------------------------------\n    if params:\n        param_names = [p.get('name', '').lower() for p in params]\n        param_types = [p.get('type', '').lower() for p in params]\n        \n        # Takes context as first param \u2192 often a Command/Query handler\n        if param_names and param_names[0] in ('ctx', 'context', 'c'):\n            return ('Command', 75.0, 'context_first_param')\n        \n        # Takes request/response \u2192 Controller/Handler\n        if any(p in param_names for p in ['request', 'req', 'response', 'resp', 'w', 'r']):\n            return ('Controller', 78.0, 'http_params')\n        \n        # Takes reader/writer \u2192 IO utility\n        if any('reader' in p or 'writer' in p for p in param_types):\n            return ('Utility', 75.0, 'io_params')\n    \n    # -------------------------------------------------------------------------\n    # STRUCTURAL RULE 3: Docstring patterns\n    # -------------------------------------------------------------------------\n    if docstring:\n        if any(x in docstring for x in ['test', 'verify', 'assert']):\n            return ('Test', 82.0, 'docstring_test')\n        if any(x in docstring for x in ['create', 'build', 'construct', 'generate']):\n            return ('Factory', 78.0, 'docstring_factory')\n        if any(x in docstring for x in ['validate', 'check', 'ensure']):\n            return ('Validator', 78.0, 'docstring_validator')\n        if any(x in docstring for x in ['parse', 'decode', 'unmarshal']):\n            return ('Query', 78.0, 'docstring_parser')\n        if any(x in docstring for x in ['write', 'save', 'store', 'persist']):\n            return ('Command', 78.0, 'docstring_persist')\n    \n    # -------------------------------------------------------------------------\n    # STRUCTURAL RULE 4: High complexity \u2192 likely Service\n    # -------------------------------------------------------------------------\n    complexity = node.get('complexity', 0)\n    if complexity > 20:\n        return ('Service', 72.0, 'high_complexity')\n    \n    # -------------------------------------------------------------------------\n    # STRUCTURAL RULE 5: Zero out-degree leaf \u2192 likely DTO/ValueObject\n    # -------------------------------------------------------------------------\n    if kind == 'class' and node.get('out_degree', 0) == 0:\n        return ('DTO', 72.0, 'leaf_class')\n    \n    return None",
      "complexity": 0,
      "lines_of_code": 80,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference",
      "name": "GraphTypeInference",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 216,
      "end_line": 216,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class GraphTypeInference:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference.__init__",
      "name": "GraphTypeInference.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 222,
      "end_line": 224,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.rules = INFERENCE_RULES\n        self.inference_log: List[Dict] = []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference.build_graph_index",
      "name": "GraphTypeInference.build_graph_index",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 226,
      "end_line": 255,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "List[Dict]"
        },
        {
          "name": "edges",
          "type": "List[Dict]"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build index structures for efficient graph traversal.",
      "signature": "def build_graph_index(self, nodes: List[Dict], edges: List[Dict]) -> Dict:",
      "body_source": "    def build_graph_index(self, nodes: List[Dict], edges: List[Dict]) -> Dict:\n        \"\"\"Build index structures for efficient graph traversal.\"\"\"\n        \n        # Node lookup by ID\n        node_by_id = {n.get('id', n.get('name', '')): n for n in nodes}\n        \n        # Caller/callee relationships\n        callers = defaultdict(set)  # node_id -> set of caller node_ids\n        callees = defaultdict(set)  # node_id -> set of callee node_ids\n        \n        for edge in edges:\n            source = edge.get('source', '')\n            target = edge.get('target', '')\n            edge_type = edge.get('edge_type', '')\n            \n            if edge_type in ('calls', 'uses', 'imports'):\n                callees[source].add(target)\n                callers[target].add(source)\n        \n        # In/out degree\n        in_degree = {n_id: len(callers[n_id]) for n_id in node_by_id}\n        out_degree = {n_id: len(callees[n_id]) for n_id in node_by_id}\n        \n        return {\n            'node_by_id': node_by_id,\n            'callers': dict(callers),\n            'callees': dict(callees),\n            'in_degree': in_degree,\n            'out_degree': out_degree,\n        }",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference.get_neighbor_types",
      "name": "GraphTypeInference.get_neighbor_types",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 257,
      "end_line": 267,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node_id",
          "type": "str"
        },
        {
          "name": "neighbors",
          "type": "Set[str]"
        },
        {
          "name": "node_by_id",
          "type": "Dict"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the types/roles of neighboring nodes.",
      "signature": "def get_neighbor_types(self, node_id: str, neighbors: Set[str],",
      "body_source": "    def get_neighbor_types(self, node_id: str, neighbors: Set[str], \n                           node_by_id: Dict) -> Set[str]:\n        \"\"\"Get the types/roles of neighboring nodes.\"\"\"\n        types = set()\n        for neighbor_id in neighbors:\n            neighbor = node_by_id.get(neighbor_id)\n            if neighbor:\n                role = neighbor.get('role', neighbor.get('type', 'Unknown'))\n                if role and role != 'Unknown':\n                    types.add(role)\n        return types",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference.infer_type",
      "name": "GraphTypeInference.infer_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 269,
      "end_line": 317,
      "role": "Analyzer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Dict"
        },
        {
          "name": "graph_index",
          "type": "Dict"
        }
      ],
      "return_type": "Tuple[str, float, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Infer type for a single node based on graph context.\nReturns (inferred_type, confidence, rule_name).",
      "signature": "def infer_type(self, node: Dict, graph_index: Dict) -> Tuple[str, float, str]:",
      "body_source": "    def infer_type(self, node: Dict, graph_index: Dict) -> Tuple[str, float, str]:\n        \"\"\"\n        Infer type for a single node based on graph context.\n        Returns (inferred_type, confidence, rule_name).\n        \"\"\"\n        node_id = node.get('id', node.get('name', ''))\n        \n        # Get graph context\n        callers = graph_index['callers'].get(node_id, set())\n        callees = graph_index['callees'].get(node_id, set())\n        in_deg = graph_index['in_degree'].get(node_id, 0)\n        out_deg = graph_index['out_degree'].get(node_id, 0)\n        \n        caller_types = self.get_neighbor_types(\n            node_id, callers, graph_index['node_by_id'])\n        callee_types = self.get_neighbor_types(\n            node_id, callees, graph_index['node_by_id'])\n        \n        # Try each rule\n        best_match = None\n        best_confidence = 0.0\n        \n        for rule in self.rules:\n            matches = True\n            \n            # Check caller type conditions\n            if rule.caller_types:\n                if not caller_types.intersection(rule.caller_types):\n                    matches = False\n            \n            # Check callee type conditions\n            if rule.callee_types:\n                if not callee_types.intersection(rule.callee_types):\n                    matches = False\n            \n            # Check degree conditions\n            if in_deg < rule.min_in_degree or in_deg > rule.max_in_degree:\n                matches = False\n            if out_deg < rule.min_out_degree or out_deg > rule.max_out_degree:\n                matches = False\n            \n            if matches and rule.confidence > best_confidence:\n                best_match = rule\n                best_confidence = rule.confidence\n        \n        if best_match:\n            return (best_match.inferred_type, best_match.confidence, best_match.name)\n        \n        return ('Unknown', 0.0, 'no_rule_matched')",
      "complexity": 0,
      "lines_of_code": 48,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:GraphTypeInference.infer_all",
      "name": "GraphTypeInference.infer_all",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 319,
      "end_line": 357,
      "role": "Analyzer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "List[Dict]"
        },
        {
          "name": "edges",
          "type": "List[Dict]"
        }
      ],
      "return_type": "Tuple[List[Dict], Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Infer types for all unknown nodes.\nReturns updated nodes and inference report.",
      "signature": "def infer_all(self, nodes: List[Dict], edges: List[Dict]) -> Tuple[List[Dict], Dict]:",
      "body_source": "    def infer_all(self, nodes: List[Dict], edges: List[Dict]) -> Tuple[List[Dict], Dict]:\n        \"\"\"\n        Infer types for all unknown nodes.\n        Returns updated nodes and inference report.\n        \"\"\"\n        # Build graph index\n        graph_index = self.build_graph_index(nodes, edges)\n        \n        inferred_count = 0\n        inference_details = []\n        \n        for node in nodes:\n            current_role = node.get('role', node.get('type', 'Unknown'))\n            \n            # Only infer for unknown nodes\n            if current_role == 'Unknown':\n                inferred_type, confidence, rule = self.infer_type(node, graph_index)\n                \n                if inferred_type != 'Unknown':\n                    node['role'] = inferred_type\n                    node['type'] = inferred_type\n                    node['role_confidence'] = confidence\n                    node['discovery_method'] = f'graph_inference:{rule}'\n                    inferred_count += 1\n                    \n                    inference_details.append({\n                        'node': node.get('name', ''),\n                        'inferred': inferred_type,\n                        'confidence': confidence,\n                        'rule': rule,\n                    })\n        \n        report = {\n            'total_inferred': inferred_count,\n            'rules_applied': len(set(d['rule'] for d in inference_details)),\n            'details': inference_details[:20],  # Top 20 examples\n        }\n        \n        return nodes, report",
      "complexity": 0,
      "lines_of_code": 38,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py:apply_graph_inference",
      "name": "apply_graph_inference",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_type_inference.py",
      "start_line": 360,
      "end_line": 420,
      "role": "Analyzer",
      "role_confidence": 88.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "nodes",
          "type": "List[Dict]"
        },
        {
          "name": "edges",
          "type": "List[Dict]"
        }
      ],
      "return_type": "Tuple[List[Dict], Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Apply graph-based type inference to reduce unknowns.\nIncludes parent-role inheritance for nested functions.\nDeterministic - no LLM required.",
      "signature": "def apply_graph_inference(nodes: List[Dict], edges: List[Dict]) -> Tuple[List[Dict], Dict]:",
      "body_source": "def apply_graph_inference(nodes: List[Dict], edges: List[Dict]) -> Tuple[List[Dict], Dict]:\n    \"\"\"\n    Apply graph-based type inference to reduce unknowns.\n    Includes parent-role inheritance for nested functions.\n    Deterministic - no LLM required.\n    \"\"\"\n    engine = GraphTypeInference()\n    nodes, report = engine.infer_all(nodes, edges)\n    \n    # =========================================================================\n    # STRUCTURAL INFERENCE - runs on ALL nodes, can boost low-confidence\n    # =========================================================================\n    structural_boosted = 0\n    for node in nodes:\n        result = infer_from_structure(node)\n        if result:\n            inferred_type, confidence, rule = result\n            current_conf = node.get('role_confidence', 0)\n            \n            # Apply if: Unknown role OR structural confidence is higher\n            if node.get('role') == 'Unknown' or confidence > current_conf:\n                node['role'] = inferred_type\n                node['type'] = inferred_type\n                node['role_confidence'] = confidence\n                node['discovery_method'] = f'structural:{rule}'\n                structural_boosted += 1\n    \n    report['structural_boosted'] = structural_boosted\n    report['total_inferred'] = report.get('total_inferred', 0) + structural_boosted\n    \n    # =========================================================================\n    # PARENT-ROLE INHERITANCE\n    # Nested functions (like test_options_work.index) inherit parent's role\n    # =========================================================================\n    parent_inherited = 0\n    node_by_name = {n.get('name', ''): n for n in nodes}\n    \n    for node in nodes:\n        if node.get('role') == 'Unknown' or node.get('type') == 'Unknown':\n            name = node.get('name', '')\n            parent_name = node.get('parent', '')\n            \n            # If no explicit parent, try to extract from dotted name\n            if not parent_name and '.' in name:\n                parent_name = name.rsplit('.', 1)[0]\n            \n            if parent_name:\n                parent = node_by_name.get(parent_name)\n                if parent:\n                    parent_role = parent.get('role', parent.get('type', 'Unknown'))\n                    if parent_role and parent_role != 'Unknown':\n                        node['role'] = parent_role\n                        node['type'] = parent_role\n                        node['role_confidence'] = parent.get('role_confidence', 70.0) * 0.9  # Slightly lower confidence\n                        node['discovery_method'] = f'parent_inheritance:{parent_name}'\n                        parent_inherited += 1\n    \n    report['parent_inherited'] = parent_inherited\n    report['total_inferred'] = report.get('total_inferred', 0) + parent_inherited\n    \n    return nodes, report",
      "complexity": 0,
      "lines_of_code": 60,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:CodeTemplate",
      "name": "CodeTemplate",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 13,
      "end_line": 13,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class CodeTemplate:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:FixGenerator",
      "name": "FixGenerator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 462,
      "end_line": 462,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class FixGenerator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:FixGenerator.__init__",
      "name": "FixGenerator.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 465,
      "end_line": 466,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "language",
          "type": "str",
          "default": "'python'"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, language: str = \"python\"):",
      "body_source": "    def __init__(self, language: str = \"python\"):\n        self.language = language",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:FixGenerator.generate_fix",
      "name": "FixGenerator.generate_fix",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 468,
      "end_line": 505,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "schema_name",
          "type": "str"
        },
        {
          "name": "context",
          "type": "dict",
          "default": "None"
        }
      ],
      "return_type": "Optional[CodeTemplate]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a code template for an optimization schema.\n\nArgs:\n    schema_name: Name of the schema (e.g., \"REPOSITORY_PATTERN\")\n    context: Dict with placeholders like {\"entity\": \"User\", \"Entity\": \"User\"}\n\nReturns:\n    CodeTemplate with filled placeholders",
      "signature": "def generate_fix(self, schema_name: str, context: dict = None) -> Optional[CodeTemplate]:",
      "body_source": "    def generate_fix(self, schema_name: str, context: dict = None) -> Optional[CodeTemplate]:\n        \"\"\"\n        Generate a code template for an optimization schema.\n        \n        Args:\n            schema_name: Name of the schema (e.g., \"REPOSITORY_PATTERN\")\n            context: Dict with placeholders like {\"entity\": \"User\", \"Entity\": \"User\"}\n        \n        Returns:\n            CodeTemplate with filled placeholders\n        \"\"\"\n        context = context or {}\n        \n        if schema_name not in TEMPLATES:\n            return None\n        \n        schema_templates = TEMPLATES[schema_name]\n        \n        if self.language not in schema_templates:\n            # Fallback to first available language\n            self.language = next(iter(schema_templates.keys()))\n        \n        template = schema_templates[self.language]\n        \n        # Fill placeholders\n        filled_code = template.code\n        filled_filename = template.filename\n        \n        for key, value in context.items():\n            filled_code = filled_code.replace(\"{\" + key + \"}\", value)\n            filled_filename = filled_filename.replace(\"{\" + key + \"}\", value.lower())\n        \n        return CodeTemplate(\n            language=template.language,\n            filename=filled_filename,\n            code=filled_code,\n            description=template.description\n        )",
      "complexity": 0,
      "lines_of_code": 37,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:FixGenerator.generate_all_fixes",
      "name": "FixGenerator.generate_all_fixes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 507,
      "end_line": 522,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "insights",
          "type": "list"
        }
      ],
      "return_type": "List[CodeTemplate]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate all fixes for a list of insights.",
      "signature": "def generate_all_fixes(self, insights: list) -> List[CodeTemplate]:",
      "body_source": "    def generate_all_fixes(self, insights: list) -> List[CodeTemplate]:\n        \"\"\"Generate all fixes for a list of insights.\"\"\"\n        fixes = []\n        \n        for insight in insights:\n            if insight.schema:\n                # Extract entity name from affected components if possible\n                context = {\"entity\": \"Entity\", \"Entity\": \"Entity\", \n                          \"component\": \"Component\", \"Component\": \"Component\",\n                          \"class\": \"Class\", \"Class\": \"Class\"}\n                \n                fix = self.generate_fix(insight.schema, context)\n                if fix:\n                    fixes.append(fix)\n        \n        return fixes",
      "complexity": 0,
      "lines_of_code": 15,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py:generate_fixes",
      "name": "generate_fixes",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/fix_generator.py",
      "start_line": 525,
      "end_line": 539,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "insights",
          "type": "list"
        },
        {
          "name": "language",
          "type": "str",
          "default": "'python'"
        }
      ],
      "return_type": "List[CodeTemplate]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to generate fixes from insights.\n\nUsage:\n    from fix_generator import generate_fixes\n    \n    insights, _ = generate_insights(nodes)\n    fixes = generate_fixes(insights)\n    for fix in fixes:\n        print(f\"# {fix.filename}\")\n        print(fix.code)",
      "signature": "def generate_fixes(insights: list, language: str = \"python\") -> List[CodeTemplate]:",
      "body_source": "def generate_fixes(insights: list, language: str = \"python\") -> List[CodeTemplate]:\n    \"\"\"\n    Convenience function to generate fixes from insights.\n    \n    Usage:\n        from fix_generator import generate_fixes\n        \n        insights, _ = generate_insights(nodes)\n        fixes = generate_fixes(insights)\n        for fix in fixes:\n            print(f\"# {fix.filename}\")\n            print(fix.code)\n    \"\"\"\n    generator = FixGenerator(language)\n    return generator.generate_all_fixes(insights)",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:ProbeResult",
      "name": "ProbeResult",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 17,
      "end_line": 17,
      "role": "DTO",
      "role_confidence": 78.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ProbeResult:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite",
      "name": "NewmanSuite",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 24,
      "end_line": 24,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class NewmanSuite:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite.run_all",
      "name": "NewmanSuite.run_all",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 27,
      "end_line": 33,
      "role": "Command",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[ProbeResult]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def run_all(self) -> List[ProbeResult]:",
      "body_source": "    def run_all(self) -> List[ProbeResult]:\n        results = []\n        results.append(self.probe_universal_detector())\n        results.append(self.probe_god_class_regex())\n        results.append(self.probe_graph_integrity())\n        results.append(self.probe_ollama_connectivity())\n        return results",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite.probe_universal_detector",
      "name": "NewmanSuite.probe_universal_detector",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 35,
      "end_line": 61,
      "role": "Test",
      "role_confidence": 92.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "ProbeResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test if regex patterns catch standard python classes.",
      "signature": "def probe_universal_detector(self) -> ProbeResult:",
      "body_source": "    def probe_universal_detector(self) -> ProbeResult:\n        \"\"\"Test if regex patterns catch standard python classes.\"\"\"\n        start = time.time()\n        try:\n            from core.universal_detector import UniversalPatternDetector\n            \n            # Create a mock file content\n            content = \"class UserRepository(BaseRepository):\\n    def find_user(self): pass\"\n            \n            # We can't easily invoke the full detector without a file, \n            # so we'll test the regex patterns if they are exposed, or run against a temp file.\n            # actually UniversalPatternDetector uses TreeSitter mostly but has regex fallback.\n            # Let's try to instantiate it at least.\n            \n            detector = UniversalPatternDetector()\n            # If we successfully created it, that's a good step.\n            \n            # Let's verify it has knowledge of patterns\n            if hasattr(detector, 'patterns'):\n                details = f\"Loaded {len(detector.patterns)} pattern families\"\n            else:\n                details = \"Detector initialized (Tree-Sitter mode)\"\n                \n            return ProbeResult(\"Universal Detector\", \"OK\", (time.time() - start) * 1000, details)\n            \n        except Exception as e:\n            return ProbeResult(\"Universal Detector\", \"FAIL\", (time.time() - start) * 1000, \"Init failed\", str(e))",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite.probe_god_class_regex",
      "name": "NewmanSuite.probe_god_class_regex",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 63,
      "end_line": 90,
      "role": "Test",
      "role_confidence": 60.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "ProbeResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test if God Class detector correctly counts methods using MULTILINE regex.",
      "signature": "def probe_god_class_regex(self) -> ProbeResult:",
      "body_source": "    def probe_god_class_regex(self) -> ProbeResult:\n        \"\"\"Test if God Class detector correctly counts methods using MULTILINE regex.\"\"\"\n        start = time.time()\n        try:\n            from core.god_class_detector_lite import GodClassDetectorLite\n            \n            detector = GodClassDetectorLite()\n            \n            # Test content with 2 methods\n            content = \"\"\"\nclass TestClass:\n    def method_one(self):\n        pass\n        \n    def method_two(self):\n        pass\n\"\"\"\n            # Manually invoke _analyze_class logic or just the regex\n            patterns = detector.language_patterns['python']\n            method_count = len(re.findall(patterns['method_pattern'], content, re.MULTILINE))\n            \n            if method_count == 2:\n                return ProbeResult(\"God Class Regex\", \"OK\", (time.time() - start) * 1000, f\"Correctly counted {method_count} methods\")\n            else:\n                return ProbeResult(\"God Class Regex\", \"FAIL\", (time.time() - start) * 1000, f\"Expected 2 methods, found {method_count}\", \"Regex missing MULTILINE flag?\")\n                \n        except Exception as e:\n            return ProbeResult(\"God Class Regex\", \"FAIL\", (time.time() - start) * 1000, \"Probe failed\", str(e))",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite.probe_graph_integrity",
      "name": "NewmanSuite.probe_graph_integrity",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 92,
      "end_line": 111,
      "role": "Test",
      "role_confidence": 60.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "ProbeResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test if IR graph can be created and exported to JSON.",
      "signature": "def probe_graph_integrity(self) -> ProbeResult:",
      "body_source": "    def probe_graph_integrity(self) -> ProbeResult:\n        \"\"\"Test if IR graph can be created and exported to JSON.\"\"\"\n        start = time.time()\n        try:\n            from core.ir import Graph, Component, Edge, EdgeType\n            \n            graph = Graph(\"test_repo\", \"/tmp\")\n            comp = Component(\"id1\", \"TestComponent\", \"class\", \"test.py\", role=\"Service\")\n            graph.add_component(comp)\n            \n            json_out = graph.to_json()\n            data = json.loads(json_out)\n            \n            if len(data['components']) == 1 and data['components']['id1']['role'] == \"Service\":\n                return ProbeResult(\"Graph Integrity\", \"OK\", (time.time() - start) * 1000, \"IR Graph JSON valid\")\n            else:\n                return ProbeResult(\"Graph Integrity\", \"FAIL\", (time.time() - start) * 1000, \"JSON structure mismatch\")\n                \n        except Exception as e:\n            return ProbeResult(\"Graph Integrity\", \"FAIL\", (time.time() - start) * 1000, \"Export failed\", str(e))",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py:NewmanSuite.probe_ollama_connectivity",
      "name": "NewmanSuite.probe_ollama_connectivity",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_suite.py",
      "start_line": 113,
      "end_line": 131,
      "role": "Test",
      "role_confidence": 75.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "ProbeResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Test connection to local Ollama instance.",
      "signature": "def probe_ollama_connectivity(self) -> ProbeResult:",
      "body_source": "    def probe_ollama_connectivity(self) -> ProbeResult:\n        \"\"\"Test connection to local Ollama instance.\"\"\"\n        start = time.time()\n        try:\n            from core.ollama_client import OllamaClient, OllamaConfig\n            \n            config = OllamaConfig()\n            client = OllamaClient(config)\n            \n            if client.is_available():\n                # Try a very cheap categorization\n                # We won't actually call classify to save time/compute during health check \n                # unless explicitly requested, but connectivity is ample.\n                return ProbeResult(\"LLM Connectivity\", \"OK\", (time.time() - start) * 1000, f\"Ollama online at {config.base_url}\")\n            else:\n                return ProbeResult(\"LLM Connectivity\", \"WARN\", (time.time() - start) * 1000, \"Ollama not responding (is it running?)\")\n                \n        except Exception as e:\n            return ProbeResult(\"LLM Connectivity\", \"FAIL\", (time.time() - start) * 1000, \"Client init failed\", str(e))",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:FlowType",
      "name": "FlowType",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 22,
      "end_line": 22,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class FlowType(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:FlowNode",
      "name": "FlowNode",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 31,
      "end_line": 31,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class FlowNode:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:CausalityChain",
      "name": "CausalityChain",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 56,
      "end_line": 56,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class CausalityChain:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:IntegrationError",
      "name": "IntegrationError",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 66,
      "end_line": 66,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class IntegrationError:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlow",
      "name": "ExecutionFlow",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 75,
      "end_line": 75,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ExecutionFlow:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlow.summary",
      "name": "ExecutionFlow.summary",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 88,
      "end_line": 108,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Summarize execution flow",
      "signature": "def summary(self) -> dict:",
      "body_source": "    def summary(self) -> dict:\n        \"\"\"Summarize execution flow\"\"\"\n        layer_distribution = defaultdict(int)\n        orphan_by_layer = defaultdict(list)\n        \n        for node in self.nodes.values():\n            layer_distribution[node.layer] += 1\n            if node.is_orphan:\n                orphan_by_layer[node.layer].append(node.name)\n        \n        return {\n            \"total_nodes\": self.total_nodes,\n            \"entry_points\": len(self.entry_points),\n            \"reachable_nodes\": self.reachable_nodes,\n            \"orphan_count\": len(self.orphans),\n            \"dead_code_percent\": self.dead_code_percent,\n            \"chains_count\": len(self.chains),\n            \"integration_errors\": len(self.integration_errors),\n            \"layer_distribution\": dict(layer_distribution),\n            \"orphans_by_layer\": {k: len(v) for k, v in orphan_by_layer.items()}\n        }",
      "complexity": 0,
      "lines_of_code": 20,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlow.correlate_with_purpose",
      "name": "ExecutionFlow.correlate_with_purpose",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 110,
      "end_line": 128,
      "role": "Utility",
      "role_confidence": 60.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "purpose_field"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Correlate execution flow with purpose field",
      "signature": "def correlate_with_purpose(self, purpose_field) -> dict:",
      "body_source": "    def correlate_with_purpose(self, purpose_field) -> dict:\n        \"\"\"Correlate execution flow with purpose field\"\"\"\n        # Count flow violations by layer pair\n        layer_violations = defaultdict(int)\n        \n        for chain in self.chains:\n            if chain.has_violation:\n                for i in range(len(chain.layers_crossed) - 1):\n                    pair = f\"{chain.layers_crossed[i]} \u2192 {chain.layers_crossed[i+1]}\"\n                    layer_violations[pair] += 1\n        \n        return {\n            \"flow_layer_violations\": dict(layer_violations),\n            \"orphans_with_purpose\": {\n                node_id: self.nodes[node_id].role \n                for node_id in self.orphans \n                if node_id in self.nodes\n            }\n        }",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector",
      "name": "ExecutionFlowDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 131,
      "end_line": 131,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class ExecutionFlowDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector.__init__",
      "name": "ExecutionFlowDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 153,
      "end_line": 156,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.nodes: Dict[str, FlowNode] = {}\n        self.edges: Dict[str, Set[str]] = defaultdict(set)  # source -> targets\n        self.reverse_edges: Dict[str, Set[str]] = defaultdict(set)  # target -> sources",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector.detect_flow",
      "name": "ExecutionFlowDetector.detect_flow",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 158,
      "end_line": 202,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list"
        },
        {
          "name": "purpose_nodes",
          "type": "dict",
          "default": "None"
        }
      ],
      "return_type": "ExecutionFlow",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect execution flow from analysis output.\n\nArgs:\n    analysis_nodes: Nodes from unified_analysis\n    edges: Edges from unified_analysis\n    purpose_nodes: Optional Purpose Field nodes for correlation",
      "signature": "def detect_flow(self, analysis_nodes: list, edges: list,",
      "body_source": "    def detect_flow(self, analysis_nodes: list, edges: list, \n                    purpose_nodes: dict = None) -> ExecutionFlow:\n        \"\"\"\n        Detect execution flow from analysis output.\n        \n        Args:\n            analysis_nodes: Nodes from unified_analysis\n            edges: Edges from unified_analysis\n            purpose_nodes: Optional Purpose Field nodes for correlation\n        \"\"\"\n        # Stage 1: Build flow nodes\n        self._build_nodes(analysis_nodes, purpose_nodes)\n        \n        # Stage 2: Build edge graph\n        self._build_edges(edges)\n        \n        # Stage 3: Detect entry points\n        entry_points = self._detect_entry_points()\n        \n        # Stage 4: Find reachable nodes (from entry points)\n        reachable = self._find_reachable(entry_points)\n        \n        # Stage 5: Detect orphans\n        orphans = self._detect_orphans(reachable)\n        \n        # Stage 6: Build causality chains\n        chains = self._build_chains(entry_points)\n        \n        # Stage 7: Detect integration errors\n        errors = self._detect_integration_errors()\n        \n        # Calculate statistics\n        total = len(self.nodes)\n        dead_code_pct = len(orphans) / total * 100 if total else 0\n        \n        return ExecutionFlow(\n            nodes=self.nodes,\n            chains=chains,\n            orphans=orphans,\n            entry_points=entry_points,\n            integration_errors=errors,\n            total_nodes=total,\n            reachable_nodes=len(reachable),\n            dead_code_percent=round(dead_code_pct, 2)\n        )",
      "complexity": 0,
      "lines_of_code": 44,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._build_nodes",
      "name": "ExecutionFlowDetector._build_nodes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 204,
      "end_line": 243,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "analysis_nodes",
          "type": "list"
        },
        {
          "name": "purpose_nodes",
          "type": "dict",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build FlowNodes from analysis output",
      "signature": "def _build_nodes(self, analysis_nodes: list, purpose_nodes: dict = None):",
      "body_source": "    def _build_nodes(self, analysis_nodes: list, purpose_nodes: dict = None):\n        \"\"\"Build FlowNodes from analysis output\"\"\"\n        purpose_nodes = purpose_nodes or {}\n        \n        for i, node in enumerate(analysis_nodes):\n            # Handle both dict and object\n            if hasattr(node, 'name'):\n                node_id = node.id if node.id else node.name\n                name = node.name\n                kind = node.kind\n                role = getattr(node, 'role', 'Unknown')\n                decorators = getattr(node, 'decorators', [])\n            else:\n                node_id = node.get('id') or node.get('name') or f\"node_{i}\"\n                name = node.get('name', '')\n                kind = node.get('kind', 'function')\n                role = node.get('role', 'Unknown')\n                decorators = node.get('decorators', [])\n            \n            # Get layer from purpose field if available\n            layer = \"unknown\"\n            if node_id in purpose_nodes:\n                pn = purpose_nodes[node_id]\n                layer = pn.layer.value if hasattr(pn.layer, 'value') else str(pn.layer)\n            \n            # Determine if public (no underscore prefix)\n            is_public = not name.startswith('_') or name.startswith('__')\n            \n            # Check if entry point\n            is_entry = self._is_entry_point(name, decorators)\n            \n            self.nodes[node_id] = FlowNode(\n                id=node_id,\n                name=name,\n                kind=kind,\n                role=role,\n                layer=layer,\n                is_entry_point=is_entry,\n                is_public=is_public\n            )",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._build_edges",
      "name": "ExecutionFlowDetector._build_edges",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 245,
      "end_line": 266,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "edges",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build edge graph",
      "signature": "def _build_edges(self, edges: list):",
      "body_source": "    def _build_edges(self, edges: list):\n        \"\"\"Build edge graph\"\"\"\n        for edge in edges:\n            if isinstance(edge, dict):\n                source = edge.get('source', edge.get('from', ''))\n                target = edge.get('target', edge.get('to', ''))\n            elif isinstance(edge, (list, tuple)) and len(edge) >= 2:\n                source, target = edge[0], edge[1]\n            else:\n                continue\n            \n            if source and target:\n                self.edges[source].add(target)\n                self.reverse_edges[target].add(source)\n                \n                # Update degrees\n                if source in self.nodes:\n                    self.nodes[source].out_degree += 1\n                    self.nodes[source].callees.append(target)\n                if target in self.nodes:\n                    self.nodes[target].in_degree += 1\n                    self.nodes[target].callers.append(source)",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._is_entry_point",
      "name": "ExecutionFlowDetector._is_entry_point",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 268,
      "end_line": 296,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "decorators",
          "type": "list"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Determine if a node is an entry point",
      "signature": "def _is_entry_point(self, name: str, decorators: list) -> bool:",
      "body_source": "    def _is_entry_point(self, name: str, decorators: list) -> bool:\n        \"\"\"Determine if a node is an entry point\"\"\"\n        name_lower = name.lower()\n        \n        # Check decorators\n        for dec in decorators:\n            dec_str = str(dec).lower()\n            for pattern in self.ENTRY_PATTERNS[\"decorators\"]:\n                if pattern.lower() in dec_str:\n                    return True\n        \n        # Check name patterns\n        for entry_name in self.ENTRY_PATTERNS[\"names\"]:\n            if name_lower == entry_name:\n                return True\n        \n        for prefix in self.ENTRY_PATTERNS[\"prefixes\"]:\n            if name_lower.startswith(prefix):\n                return True\n        \n        for suffix in self.ENTRY_PATTERNS[\"suffixes\"]:\n            if name_lower.endswith(suffix):\n                return True\n        \n        # Special: if __name__ == \"__main__\" patterns\n        if name_lower in [\"__main__\", \"if_main\"]:\n            return True\n        \n        return False",
      "complexity": 0,
      "lines_of_code": 28,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._detect_entry_points",
      "name": "ExecutionFlowDetector._detect_entry_points",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 298,
      "end_line": 310,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find all entry points",
      "signature": "def _detect_entry_points(self) -> List[str]:",
      "body_source": "    def _detect_entry_points(self) -> List[str]:\n        \"\"\"Find all entry points\"\"\"\n        entries = []\n        \n        for node_id, node in self.nodes.items():\n            # Explicit entry points\n            if node.is_entry_point:\n                entries.append(node_id)\n            # Implicit: public functions with no callers\n            elif node.in_degree == 0 and node.is_public and node.kind in ['function', 'class']:\n                entries.append(node_id)\n        \n        return entries",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._find_reachable",
      "name": "ExecutionFlowDetector._find_reachable",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 312,
      "end_line": 327,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "entry_points",
          "type": "List[str]"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find all nodes reachable from entry points (BFS)",
      "signature": "def _find_reachable(self, entry_points: List[str]) -> Set[str]:",
      "body_source": "    def _find_reachable(self, entry_points: List[str]) -> Set[str]:\n        \"\"\"Find all nodes reachable from entry points (BFS)\"\"\"\n        reachable = set()\n        queue = list(entry_points)\n        \n        while queue:\n            current = queue.pop(0)\n            if current in reachable:\n                continue\n            reachable.add(current)\n            \n            for target in self.edges.get(current, []):\n                if target not in reachable:\n                    queue.append(target)\n        \n        return reachable",
      "complexity": 0,
      "lines_of_code": 15,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._detect_orphans",
      "name": "ExecutionFlowDetector._detect_orphans",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 329,
      "end_line": 339,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "reachable",
          "type": "Set[str]"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find orphan nodes (not reachable and not entry points)",
      "signature": "def _detect_orphans(self, reachable: Set[str]) -> List[str]:",
      "body_source": "    def _detect_orphans(self, reachable: Set[str]) -> List[str]:\n        \"\"\"Find orphan nodes (not reachable and not entry points)\"\"\"\n        orphans = []\n        \n        for node_id, node in self.nodes.items():\n            # Only flag public non-entry-point nodes\n            if node_id not in reachable and node.is_public and not node.is_entry_point:\n                node.is_orphan = True\n                orphans.append(node_id)\n        \n        return orphans",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._build_chains",
      "name": "ExecutionFlowDetector._build_chains",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 341,
      "end_line": 362,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "entry_points",
          "type": "List[str]"
        },
        {
          "name": "max_depth",
          "type": "int",
          "default": "20"
        }
      ],
      "return_type": "List[CausalityChain]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build causality chains from entry points",
      "signature": "def _build_chains(self, entry_points: List[str], max_depth: int = 20) -> List[CausalityChain]:",
      "body_source": "    def _build_chains(self, entry_points: List[str], max_depth: int = 20) -> List[CausalityChain]:\n        \"\"\"Build causality chains from entry points\"\"\"\n        chains = []\n        \n        for entry in entry_points[:50]:  # Limit for performance\n            visited = set()\n            path = []\n            layers = []\n            \n            self._trace_chain(entry, path, layers, visited, max_depth)\n            \n            if path:\n                has_violation = self._check_layer_violation(layers)\n                chains.append(CausalityChain(\n                    entry_point=entry,\n                    path=path[:max_depth],\n                    length=len(path),\n                    layers_crossed=layers[:max_depth],\n                    has_violation=has_violation\n                ))\n        \n        return chains",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._trace_chain",
      "name": "ExecutionFlowDetector._trace_chain",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 364,
      "end_line": 377,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node_id",
          "type": "str"
        },
        {
          "name": "path",
          "type": "list"
        },
        {
          "name": "layers",
          "type": "list"
        },
        {
          "name": "visited",
          "type": "set"
        },
        {
          "name": "depth",
          "type": "int"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Recursively trace a causality chain",
      "signature": "def _trace_chain(self, node_id: str, path: list, layers: list,",
      "body_source": "    def _trace_chain(self, node_id: str, path: list, layers: list, \n                     visited: set, depth: int):\n        \"\"\"Recursively trace a causality chain\"\"\"\n        if depth <= 0 or node_id in visited:\n            return\n        \n        visited.add(node_id)\n        path.append(node_id)\n        \n        if node_id in self.nodes:\n            layers.append(self.nodes[node_id].layer)\n        \n        for target in list(self.edges.get(node_id, []))[:5]:  # Limit branching\n            self._trace_chain(target, path, layers, visited, depth - 1)",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._check_layer_violation",
      "name": "ExecutionFlowDetector._check_layer_violation",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 379,
      "end_line": 389,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "layers",
          "type": "List[str]"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if layer sequence has violations",
      "signature": "def _check_layer_violation(self, layers: List[str]) -> bool:",
      "body_source": "    def _check_layer_violation(self, layers: List[str]) -> bool:\n        \"\"\"Check if layer sequence has violations\"\"\"\n        for i in range(len(layers) - 1):\n            current_order = self.LAYER_ORDER.get(layers[i], 99)\n            next_order = self.LAYER_ORDER.get(layers[i + 1], 99)\n            \n            # Violation: lower layer (infra) calling higher layer (presentation)\n            if current_order > next_order and current_order != 99 and next_order != 99:\n                return True\n        \n        return False",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:ExecutionFlowDetector._detect_integration_errors",
      "name": "ExecutionFlowDetector._detect_integration_errors",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 391,
      "end_line": 416,
      "role": "Exception",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_error",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[IntegrationError]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect integration errors",
      "signature": "def _detect_integration_errors(self) -> List[IntegrationError]:",
      "body_source": "    def _detect_integration_errors(self) -> List[IntegrationError]:\n        \"\"\"Detect integration errors\"\"\"\n        errors = []\n        \n        # Find calls to non-existent nodes\n        for source, targets in self.edges.items():\n            for target in targets:\n                if target not in self.nodes:\n                    errors.append(IntegrationError(\n                        type=\"missing_reference\",\n                        source=source,\n                        target=target,\n                        message=f\"{source} references missing {target}\"\n                    ))\n        \n        # Find circular dependencies (simplified)\n        for node_id in list(self.nodes.keys())[:100]:  # Limit for performance\n            if node_id in self.edges.get(node_id, set()):\n                errors.append(IntegrationError(\n                    type=\"self_reference\",\n                    source=node_id,\n                    target=node_id,\n                    message=f\"{node_id} references itself\"\n                ))\n        \n        return errors",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py:detect_execution_flow",
      "name": "detect_execution_flow",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/execution_flow.py",
      "start_line": 419,
      "end_line": 435,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list"
        },
        {
          "name": "purpose_field",
          "default": "None"
        }
      ],
      "return_type": "ExecutionFlow",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to detect execution flow.\n\nUsage:\n    from execution_flow import detect_execution_flow\n    \n    result = analyze(path)\n    flow = detect_execution_flow(result.nodes, result.edges)\n    print(flow.summary())",
      "signature": "def detect_execution_flow(nodes: list, edges: list, purpose_field=None) -> ExecutionFlow:",
      "body_source": "def detect_execution_flow(nodes: list, edges: list, purpose_field=None) -> ExecutionFlow:\n    \"\"\"\n    Convenience function to detect execution flow.\n    \n    Usage:\n        from execution_flow import detect_execution_flow\n        \n        result = analyze(path)\n        flow = detect_execution_flow(result.nodes, result.edges)\n        print(flow.summary())\n    \"\"\"\n    purpose_nodes = {}\n    if purpose_field and hasattr(purpose_field, 'nodes'):\n        purpose_nodes = purpose_field.nodes\n    \n    detector = ExecutionFlowDetector()\n    return detector.detect_flow(nodes, edges, purpose_nodes)",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:ComponentCard",
      "name": "ComponentCard",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 18,
      "end_line": 18,
      "role": "DTO",
      "role_confidence": 0.8,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ComponentCard:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor",
      "name": "SmartExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 49,
      "end_line": 49,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SmartExtractor:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor.__init__",
      "name": "SmartExtractor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 69,
      "end_line": 72,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, repo_path: str):",
      "body_source": "    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        self._file_cache: Dict[str, List[str]] = {}\n        self._import_cache: Dict[str, List[str]] = {}",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor.extract_card",
      "name": "SmartExtractor.extract_card",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 74,
      "end_line": 113,
      "role": "Transformer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "Dict"
        },
        {
          "name": "graph_edges",
          "type": "Optional[List[Dict]]",
          "default": "None"
        }
      ],
      "return_type": "ComponentCard",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract a ComponentCard for a single node.\n\nArgs:\n    node: Node dict from graph.json with name, file_path, line, type, etc.\n    graph_edges: Optional list of edges for call graph context",
      "signature": "def extract_card(self,",
      "body_source": "    def extract_card(self, \n                     node: Dict,\n                     graph_edges: Optional[List[Dict]] = None) -> ComponentCard:\n        \"\"\"\n        Extract a ComponentCard for a single node.\n        \n        Args:\n            node: Node dict from graph.json with name, file_path, line, type, etc.\n            graph_edges: Optional list of edges for call graph context\n        \"\"\"\n        file_path = node.get(\"file_path\", \"\")\n        name = node.get(\"name\", \"\")\n        line_num = node.get(\"line\", 1)\n        kind = node.get(\"symbol_kind\", \"function\")\n        \n        # Create basic card\n        card = ComponentCard(\n            node_id=f\"{file_path}:{name}:{line_num}\",\n            file_path=file_path,\n            name=name,\n            kind=kind,\n            start_line=line_num,\n            end_line=line_num + 50,  # Will be refined by AST\n            heuristic_type=node.get(\"type\", \"Unknown\"),\n            heuristic_confidence=node.get(\"confidence\", 0.0),\n        )\n        \n        # Enrich with source code\n        abs_path = self.repo_path / file_path\n        if abs_path.exists():\n            self._enrich_from_source(card, abs_path)\n        \n        # Infer folder layer\n        card.folder_layer = self._infer_layer(file_path)\n        \n        # Add graph context if available\n        if graph_edges:\n            self._add_graph_context(card, graph_edges)\n        \n        return card",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._enrich_from_source",
      "name": "SmartExtractor._enrich_from_source",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 115,
      "end_line": 141,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "card",
          "type": "ComponentCard"
        },
        {
          "name": "file_path",
          "type": "Path"
        }
      ],
      "return_type": "None",
      "base_classes": [],
      "decorators": [],
      "docstring": "Read source file and extract code context.",
      "signature": "def _enrich_from_source(self, card: ComponentCard, file_path: Path) -> None:",
      "body_source": "    def _enrich_from_source(self, card: ComponentCard, file_path: Path) -> None:\n        \"\"\"Read source file and extract code context.\"\"\"\n        try:\n            # Cache file contents\n            if str(file_path) not in self._file_cache:\n                content = file_path.read_text(encoding='utf-8', errors='ignore')\n                self._file_cache[str(file_path)] = content.splitlines()\n                \n                # Parse imports once per file\n                self._import_cache[str(file_path)] = self._extract_imports(content)\n            \n            lines = self._file_cache[str(file_path)]\n            \n            # Extract code excerpt (100 lines max around the node)\n            start = max(0, card.start_line - 1)\n            end = min(len(lines), start + 100)\n            card.code_excerpt = \"\\n\".join(lines[start:end])\n            card.end_line = end\n            \n            # Add imports from file\n            card.imports = self._import_cache[str(file_path)]\n            \n            # Parse AST for more details\n            self._enrich_from_ast(card, \"\\n\".join(lines))\n            \n        except Exception as e:\n            card.code_excerpt = f\"# Error reading source: {e}\"",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._enrich_from_ast",
      "name": "SmartExtractor._enrich_from_ast",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 143,
      "end_line": 195,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "card",
          "type": "ComponentCard"
        },
        {
          "name": "source",
          "type": "str"
        }
      ],
      "return_type": "None",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract AST information: decorators, base classes, docstring.",
      "signature": "def _enrich_from_ast(self, card: ComponentCard, source: str) -> None:",
      "body_source": "    def _enrich_from_ast(self, card: ComponentCard, source: str) -> None:\n        \"\"\"Extract AST information: decorators, base classes, docstring.\"\"\"\n        try:\n            tree = ast.parse(source)\n            \n            # Find the node in AST\n            for node in ast.walk(tree):\n                if isinstance(node, ast.ClassDef):\n                    if node.name == card.name.split(\".\")[-1]:\n                        # Extract base classes\n                        card.base_classes = [\n                            self._get_name(base) for base in node.bases\n                        ]\n                        \n                        # Extract decorators\n                        card.decorators = [\n                            self._get_decorator_name(d) for d in node.decorator_list\n                        ]\n                        \n                        # Extract docstring\n                        card.docstring = ast.get_docstring(node) or \"\"\n                        \n                        # Get signature (class definition line)\n                        card.signature = f\"class {node.name}({', '.join(card.base_classes)})\"\n                        \n                        # Precise line range\n                        card.start_line = node.lineno\n                        card.end_line = node.end_lineno or card.end_line\n                        break\n                        \n                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                    func_name = card.name.split(\".\")[-1]\n                    if node.name == func_name:\n                        # Extract decorators\n                        card.decorators = [\n                            self._get_decorator_name(d) for d in node.decorator_list\n                        ]\n                        \n                        # Extract docstring\n                        card.docstring = ast.get_docstring(node) or \"\"\n                        \n                        # Get signature\n                        args = [a.arg for a in node.args.args]\n                        prefix = \"async def\" if isinstance(node, ast.AsyncFunctionDef) else \"def\"\n                        card.signature = f\"{prefix} {node.name}({', '.join(args)})\"\n                        \n                        # Precise line range\n                        card.start_line = node.lineno\n                        card.end_line = node.end_lineno or card.end_line\n                        break\n                        \n        except SyntaxError:\n            pass  # Not valid Python, skip AST enrichment",
      "complexity": 0,
      "lines_of_code": 52,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._get_name",
      "name": "SmartExtractor._get_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 197,
      "end_line": 205,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get name from AST node (handles Name, Attribute, Subscript).",
      "signature": "def _get_name(self, node) -> str:",
      "body_source": "    def _get_name(self, node) -> str:\n        \"\"\"Get name from AST node (handles Name, Attribute, Subscript).\"\"\"\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            return f\"{self._get_name(node.value)}.{node.attr}\"\n        elif isinstance(node, ast.Subscript):\n            return self._get_name(node.value)\n        return str(node)",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._get_decorator_name",
      "name": "SmartExtractor._get_decorator_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 207,
      "end_line": 215,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get decorator name from AST node.",
      "signature": "def _get_decorator_name(self, node) -> str:",
      "body_source": "    def _get_decorator_name(self, node) -> str:\n        \"\"\"Get decorator name from AST node.\"\"\"\n        if isinstance(node, ast.Name):\n            return f\"@{node.id}\"\n        elif isinstance(node, ast.Call):\n            return f\"@{self._get_name(node.func)}(...)\"\n        elif isinstance(node, ast.Attribute):\n            return f\"@{self._get_name(node)}\"\n        return \"@unknown\"",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._extract_imports",
      "name": "SmartExtractor._extract_imports",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 217,
      "end_line": 236,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "source",
          "type": "str"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract import statements from source.",
      "signature": "def _extract_imports(self, source: str) -> List[str]:",
      "body_source": "    def _extract_imports(self, source: str) -> List[str]:\n        \"\"\"Extract import statements from source.\"\"\"\n        imports = []\n        try:\n            tree = ast.parse(source)\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        imports.append(f\"import {alias.name}\")\n                elif isinstance(node, ast.ImportFrom):\n                    module = node.module or \"\"\n                    for alias in node.names:\n                        imports.append(f\"from {module} import {alias.name}\")\n        except SyntaxError:\n            # Fallback: regex-based extraction\n            for line in source.splitlines()[:50]:\n                if line.strip().startswith((\"import \", \"from \")):\n                    imports.append(line.strip())\n        \n        return imports[:20]  # Limit to 20 imports",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._infer_layer",
      "name": "SmartExtractor._infer_layer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 238,
      "end_line": 246,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Infer architectural layer from file path.",
      "signature": "def _infer_layer(self, file_path: str) -> str:",
      "body_source": "    def _infer_layer(self, file_path: str) -> str:\n        \"\"\"Infer architectural layer from file path.\"\"\"\n        normalized = file_path.lower().replace(\"\\\\\", \"/\")\n        \n        for layer, patterns in self.LAYER_PATTERNS.items():\n            if any(p in normalized for p in patterns):\n                return layer\n        \n        return \"unknown\"",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor._add_graph_context",
      "name": "SmartExtractor._add_graph_context",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 248,
      "end_line": 264,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "card",
          "type": "ComponentCard"
        },
        {
          "name": "edges",
          "type": "List[Dict]"
        }
      ],
      "return_type": "None",
      "base_classes": [],
      "decorators": [],
      "docstring": "Add call graph context (who calls this, who does this call).",
      "signature": "def _add_graph_context(self, card: ComponentCard, edges: List[Dict]) -> None:",
      "body_source": "    def _add_graph_context(self, card: ComponentCard, edges: List[Dict]) -> None:\n        \"\"\"Add call graph context (who calls this, who does this call).\"\"\"\n        node_id = card.node_id\n        name = card.name\n        \n        for edge in edges:\n            source = edge.get(\"source\", \"\")\n            target = edge.get(\"target\", \"\")\n            \n            if name in source or node_id in source:\n                card.outgoing_calls.append(target)\n            elif name in target or node_id in target:\n                card.incoming_calls.append(source)\n        \n        # Limit for prompt size\n        card.outgoing_calls = card.outgoing_calls[:10]\n        card.incoming_calls = card.incoming_calls[:10]",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:SmartExtractor.extract_unknowns",
      "name": "SmartExtractor.extract_unknowns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 266,
      "end_line": 291,
      "role": "Transformer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph_data",
          "type": "Dict"
        },
        {
          "name": "limit",
          "type": "Optional[int]",
          "default": "None"
        }
      ],
      "return_type": "List[ComponentCard]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract ComponentCards for all Unknown nodes in a graph.\n\nArgs:\n    graph_data: Loaded graph.json\n    limit: Optional limit on number of cards to extract",
      "signature": "def extract_unknowns(self,",
      "body_source": "    def extract_unknowns(self, \n                         graph_data: Dict,\n                         limit: Optional[int] = None) -> List[ComponentCard]:\n        \"\"\"\n        Extract ComponentCards for all Unknown nodes in a graph.\n        \n        Args:\n            graph_data: Loaded graph.json\n            limit: Optional limit on number of cards to extract\n        \"\"\"\n        components = graph_data.get(\"components\", {})\n        edges = graph_data.get(\"edges\", [])\n        \n        unknowns = []\n        for name, node in components.items():\n            if node.get(\"type\") == \"Unknown\":\n                unknowns.append(node)\n                if limit and len(unknowns) >= limit:\n                    break\n        \n        cards = []\n        for node in unknowns:\n            card = self.extract_card(node, edges)\n            cards.append(card)\n        \n        return cards",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py:format_card_for_llm",
      "name": "format_card_for_llm",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/smart_extractor.py",
      "start_line": 294,
      "end_line": 333,
      "role": "Utility",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "card",
          "type": "ComponentCard"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Format a ComponentCard as a prompt for LLM classification.",
      "signature": "def format_card_for_llm(card: ComponentCard) -> str:",
      "body_source": "def format_card_for_llm(card: ComponentCard) -> str:\n    \"\"\"Format a ComponentCard as a prompt for LLM classification.\"\"\"\n    return f\"\"\"Classify this code component:\n\nNODE ID: {card.node_id}\nFILE: {card.file_path}\nNAME: {card.name}\nKIND: {card.kind}\nLINES: {card.start_line}-{card.end_line}\n\nSIGNATURE:\n```\n{card.signature}\n```\n\nDOCSTRING:\n```\n{card.docstring or \"(no docstring)\"}\n```\n\nDECORATORS: {', '.join(card.decorators) or \"(none)\"}\nBASE CLASSES: {', '.join(card.base_classes) or \"(none)\"}\nIMPORTS (sample): {', '.join(card.imports[:5]) or \"(none)\"}\n\nFOLDER LAYER (heuristic): {card.folder_layer}\n\nCALLS (outgoing): {', '.join(card.outgoing_calls[:5]) or \"(none)\"}\nCALLED BY: {', '.join(card.incoming_calls[:5]) or \"(none)\"}\n\nCODE EXCERPT:\n```python\n{card.code_excerpt[:2000]}\n```\n\nHEURISTIC PRE-CLASSIFICATION:\n- Type guess: {card.heuristic_type}\n- Confidence: {card.heuristic_confidence:.2f}\n\nPlease classify this component and provide evidence-anchored justification.\n\"\"\"",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentAnalysis",
      "name": "IntentAnalysis",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 17,
      "end_line": 17,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class IntentAnalysis:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentAnalysis.__post_init__",
      "name": "IntentAnalysis.__post_init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 37,
      "end_line": 39,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __post_init__(self):",
      "body_source": "    def __post_init__(self):\n        if self.detected_smells is None:\n            self.detected_smells = []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector",
      "name": "IntentDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 89,
      "end_line": 89,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class IntentDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector.__init__",
      "name": "IntentDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 94,
      "end_line": 96,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "llm_classifier",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, llm_classifier=None):",
      "body_source": "    def __init__(self, llm_classifier=None):\n        self.llm_classifier = llm_classifier\n        self.available = llm_classifier is not None",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector.analyze",
      "name": "IntentDetector.analyze",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 98,
      "end_line": 136,
      "role": "Analyzer",
      "role_confidence": 88.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "semantic_ids",
          "type": "List"
        },
        {
          "name": "file_contents",
          "type": "Dict[str, str]",
          "default": "None"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze semantic IDs for intent.\n\nArgs:\n    semantic_ids: List of SemanticID objects\n    file_contents: Optional dict of {file_path: content} for deeper analysis\n    \nReturns:\n    {\n        \"available\": bool,\n        \"analyses\": List[IntentAnalysis],\n        \"patterns_detected\": Dict[str, int],\n        \"smells_detected\": Dict[str, int],\n    }",
      "signature": "def analyze(self, semantic_ids: List, file_contents: Dict[str, str] = None) -> Dict:",
      "body_source": "    def analyze(self, semantic_ids: List, file_contents: Dict[str, str] = None) -> Dict:\n        \"\"\"\n        Analyze semantic IDs for intent.\n        \n        Args:\n            semantic_ids: List of SemanticID objects\n            file_contents: Optional dict of {file_path: content} for deeper analysis\n            \n        Returns:\n            {\n                \"available\": bool,\n                \"analyses\": List[IntentAnalysis],\n                \"patterns_detected\": Dict[str, int],\n                \"smells_detected\": Dict[str, int],\n            }\n        \"\"\"\n        analyses = []\n        patterns_count = {}\n        smells_count = {}\n        \n        for sid in semantic_ids:\n            analysis = self._analyze_single(sid, file_contents)\n            analyses.append(analysis)\n            \n            # Count patterns\n            if analysis.detected_pattern:\n                patterns_count[analysis.detected_pattern] = \\\n                    patterns_count.get(analysis.detected_pattern, 0) + 1\n            \n            # Count smells\n            for smell in analysis.detected_smells:\n                smells_count[smell] = smells_count.get(smell, 0) + 1\n        \n        return {\n            \"available\": True,\n            \"analyses\": analyses,\n            \"patterns_detected\": patterns_count,\n            \"smells_detected\": smells_count,\n        }",
      "complexity": 0,
      "lines_of_code": 38,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector._analyze_single",
      "name": "IntentDetector._analyze_single",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 138,
      "end_line": 170,
      "role": "Stream",
      "role_confidence": 88.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "sid"
        },
        {
          "name": "file_contents",
          "type": "Dict",
          "default": "None"
        }
      ],
      "return_type": "IntentAnalysis",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a single semantic ID for intent.",
      "signature": "def _analyze_single(self, sid, file_contents: Dict = None) -> IntentAnalysis:",
      "body_source": "    def _analyze_single(self, sid, file_contents: Dict = None) -> IntentAnalysis:\n        \"\"\"Analyze a single semantic ID for intent.\"\"\"\n        name = sid.name\n        props = sid.properties\n        file_path = sid.module_path.replace(\".\", \"/\") + \".py\"\n        \n        analysis = IntentAnalysis(name=name, file_path=file_path)\n        \n        # 1. Pattern detection via heuristics\n        detected_pattern, pattern_conf = self._detect_pattern_heuristic(name, props)\n        analysis.detected_pattern = detected_pattern\n        analysis.pattern_confidence = pattern_conf\n        \n        # 2. Role detection (from existing classification)\n        if \"type\" in props:\n            analysis.detected_role = props[\"type\"]\n            analysis.role_confidence = props.get(\"confidence\", 50) / 100.0\n        \n        # 3. Smell detection\n        smells = self._detect_smells_heuristic(name, props)\n        analysis.detected_smells = smells\n        analysis.smell_severity = len(smells) * 0.3  # Simple severity\n        \n        # 4. Optional: LLM enrichment for low-confidence cases\n        if self.llm_classifier and pattern_conf < 0.5:\n            llm_result = self._escalate_to_llm(sid, file_contents)\n            if llm_result:\n                if llm_result.get(\"pattern\"):\n                    analysis.detected_pattern = llm_result[\"pattern\"]\n                    analysis.pattern_confidence = llm_result.get(\"confidence\", 0.7)\n                analysis.reasoning = llm_result.get(\"reasoning\", \"\")\n        \n        return analysis",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector._detect_pattern_heuristic",
      "name": "IntentDetector._detect_pattern_heuristic",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 172,
      "end_line": 188,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "props",
          "type": "Dict"
        }
      ],
      "return_type": "Tuple[Optional[str], float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect pattern using name-based heuristics.",
      "signature": "def _detect_pattern_heuristic(self, name: str, props: Dict) -> Tuple[Optional[str], float]:",
      "body_source": "    def _detect_pattern_heuristic(self, name: str, props: Dict) -> Tuple[Optional[str], float]:\n        \"\"\"Detect pattern using name-based heuristics.\"\"\"\n        name_lower = name.lower()\n        \n        for pattern, hints in PATTERNS.items():\n            name_hints = hints.get(\"name_hints\", [])\n            if any(hint in name_lower for hint in name_hints):\n                confidence = 0.7\n                \n                # Boost confidence if method patterns match\n                methods = hints.get(\"methods\", [])\n                if methods and any(m in str(props) for m in methods):\n                    confidence = 0.9\n                \n                return pattern, confidence\n        \n        return None, 0.0",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector._detect_smells_heuristic",
      "name": "IntentDetector._detect_smells_heuristic",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 190,
      "end_line": 207,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "props",
          "type": "Dict"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect code smells using heuristics.",
      "signature": "def _detect_smells_heuristic(self, name: str, props: Dict) -> List[str]:",
      "body_source": "    def _detect_smells_heuristic(self, name: str, props: Dict) -> List[str]:\n        \"\"\"Detect code smells using heuristics.\"\"\"\n        smells = []\n        \n        # God Class detection\n        method_count = props.get(\"methods\", 0)\n        lines = props.get(\"lines\", 0)\n        \n        if method_count > SMELLS[\"GodClass\"][\"method_count_threshold\"]:\n            smells.append(\"GodClass\")\n        elif lines > SMELLS[\"GodClass\"][\"line_count_threshold\"]:\n            smells.append(\"GodClass\")\n        \n        # Long Method detection\n        if lines > SMELLS[\"LongMethod\"][\"line_count_threshold\"]:\n            smells.append(\"LongMethod\")\n        \n        return smells",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:IntentDetector._escalate_to_llm",
      "name": "IntentDetector._escalate_to_llm",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 209,
      "end_line": 216,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "sid"
        },
        {
          "name": "file_contents",
          "type": "Dict",
          "default": "None"
        }
      ],
      "return_type": "Optional[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Escalate to LLM for intent analysis.",
      "signature": "def _escalate_to_llm(self, sid, file_contents: Dict = None) -> Optional[Dict]:",
      "body_source": "    def _escalate_to_llm(self, sid, file_contents: Dict = None) -> Optional[Dict]:\n        \"\"\"Escalate to LLM for intent analysis.\"\"\"\n        if not self.llm_classifier:\n            return None\n        \n        # This would call the existing LLM classifier\n        # For now, return None to indicate escalation not performed\n        return None",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:_enrich_with_why",
      "name": "_enrich_with_why",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 220,
      "end_line": 245,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "engine"
        },
        {
          "name": "semantic_ids",
          "type": "List"
        },
        {
          "name": "intent_data",
          "type": "Dict"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enrich semantic IDs with WHY dimension (intent/patterns).\n\nArgs:\n    engine: LearningEngine instance\n    semantic_ids: List of SemanticID objects\n    intent_data: Result from IntentDetector.analyze()",
      "signature": "def _enrich_with_why(engine, semantic_ids: List, intent_data: Dict):",
      "body_source": "def _enrich_with_why(engine, semantic_ids: List, intent_data: Dict):\n    \"\"\"\n    Enrich semantic IDs with WHY dimension (intent/patterns).\n    \n    Args:\n        engine: LearningEngine instance\n        semantic_ids: List of SemanticID objects\n        intent_data: Result from IntentDetector.analyze()\n    \"\"\"\n    analyses = intent_data.get(\"analyses\", [])\n    \n    # Build lookup by name\n    analysis_by_name = {a.name: a for a in analyses}\n    \n    for sid in semantic_ids:\n        analysis = analysis_by_name.get(sid.name)\n        if analysis:\n            # Add detected pattern\n            if analysis.detected_pattern:\n                sid.properties[\"pattern\"] = analysis.detected_pattern\n                sid.properties[\"pattern_confidence\"] = analysis.pattern_confidence\n            \n            # Add detected smells\n            if analysis.detected_smells:\n                sid.properties[\"smells\"] = \",\".join(analysis.detected_smells)\n                sid.properties[\"smell_severity\"] = analysis.smell_severity",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:MockSID",
      "name": "MockSID",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 255,
      "end_line": 255,
      "role": "DTO",
      "role_confidence": 70.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class MockSID:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py:MockSID.__init__",
      "name": "MockSID.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/intent_detector.py",
      "start_line": 256,
      "end_line": 259,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name"
        },
        {
          "name": "props"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, name, props):",
      "body_source": "        def __init__(self, name, props):\n            self.name = name\n            self.properties = props\n            self.module_path = \"test.module\"",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_runner.py:run_health_check",
      "name": "run_health_check",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/newman_runner.py",
      "start_line": 15,
      "end_line": 51,
      "role": "Validator",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "exit_on_fail",
          "default": "False"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Run all health checks and print report.",
      "signature": "def run_health_check(exit_on_fail=False):",
      "body_source": "def run_health_check(exit_on_fail=False):\n    \"\"\"Run all health checks and print report.\"\"\"\n    print(\"\ud83d\udd2c NEWMAN PIPELINE VALIDATION\")\n    print(\"=\" * 60)\n    print(f\"Timestamp: {datetime.now().isoformat()}\")\n    print(\"-\" * 60)\n    \n    suite = NewmanSuite()\n    results = suite.run_all()\n    \n    all_passed = True\n    \n    for r in results:\n        status_icon = \"\u2705\"\n        if r.status == \"FAIL\":\n            status_icon = \"\u274c\"\n            all_passed = False\n        elif r.status == \"WARN\":\n            status_icon = \"\u26a0\ufe0f \"\n        elif r.status == \"SKIP\":\n            status_icon = \"\u23ed\ufe0f \"\n            \n        print(f\"{status_icon} [{r.component}]\".ljust(40) + f\"{r.status} ({r.latency_ms:.1f}ms)\")\n        print(f\"    \u2514\u2500 {r.details}\")\n        if r.error:\n            print(f\"    \u274c Error: {r.error}\")\n        print()\n        \n    print(\"=\" * 60)\n    if all_passed:\n        print(f\"\u2728 SYSTEM HEALTHY - {len(results)} Checks Passed\")\n        return 0\n    else:\n        print(\"\ud83d\udea8 SYSTEM ISSUES DETECTED\")\n        if exit_on_fail:\n            return 1\n        return 0",
      "complexity": 0,
      "lines_of_code": 36,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState",
      "name": "CodebaseState",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 17,
      "end_line": 17,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class CodebaseState:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.__init__",
      "name": "CodebaseState.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 25,
      "end_line": 36,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "target_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, target_path: str):",
      "body_source": "    def __init__(self, target_path: str):\n        self.target_path = target_path\n        self.nodes: Dict[str, Dict[str, Any]] = {}\n        self.edges: List[Dict[str, Any]] = []\n        self.metadata: Dict[str, Any] = {\n            \"created_at\": datetime.now().isoformat(),\n            \"target\": target_path,\n            \"layers_activated\": []\n        }\n        \n        # Indices for fast lookup\n        self._node_lookup: Dict[str, Dict[str, Any]] = {}",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.load_initial_graph",
      "name": "CodebaseState.load_initial_graph",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 38,
      "end_line": 76,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "List[Any]"
        },
        {
          "name": "edges",
          "type": "List[Any]"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load the initial AST/Ref graph from Stage 1-2.\nAccepts dicts or objects (Dataclasses).",
      "signature": "def load_initial_graph(self, nodes: List[Any], edges: List[Any]):",
      "body_source": "    def load_initial_graph(self, nodes: List[Any], edges: List[Any]):\n        \"\"\"\n        Load the initial AST/Ref graph from Stage 1-2.\n        Accepts dicts or objects (Dataclasses).\n        \"\"\"\n        # clear existing\n        self.nodes = {}\n        self.edges = []\n        \n        # Ingest nodes\n        for n in nodes:\n            # Normalize to dict\n            if hasattr(n, 'to_dict'):\n                n_dict = n.to_dict()\n            elif hasattr(n, '__dict__'):\n                n_dict = vars(n)\n            else:\n                n_dict = dict(n)\n                \n            # Ensure ID\n            nid = n_dict.get('id')\n            if not nid:\n                continue # Skip invalid nodes\n                \n            self.nodes[nid] = n_dict\n            self._node_lookup[nid] = n_dict\n            \n        # Ingest edges\n        for e in edges:\n            if hasattr(e, 'to_dict'):\n                e_dict = e.to_dict()\n            elif hasattr(e, '__dict__'):\n                e_dict = vars(e)\n            else:\n                e_dict = dict(e)\n            \n            self.edges.append(e_dict)\n            \n        print(f\"  [State] Loaded {len(self.nodes)} nodes and {len(self.edges)} edges.\")",
      "complexity": 0,
      "lines_of_code": 38,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.enrich_node",
      "name": "CodebaseState.enrich_node",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 78,
      "end_line": 100,
      "role": "Transformer",
      "role_confidence": 80.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node_id",
          "type": "str"
        },
        {
          "name": "layer_name",
          "type": "str"
        },
        {
          "name": "**attributes"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enrich a specific node with semantic data from a layer.\n\nArgs:\n    node_id: The ID of the node to enrich.\n    layer_name: The source of this data (e.g., \"purpose\", \"flow\").\n    **attributes: Key-value pairs to merge into the node.",
      "signature": "def enrich_node(self, node_id: str, layer_name: str, **attributes):",
      "body_source": "    def enrich_node(self, node_id: str, layer_name: str, **attributes):\n        \"\"\"\n        Enrich a specific node with semantic data from a layer.\n        \n        Args:\n            node_id: The ID of the node to enrich.\n            layer_name: The source of this data (e.g., \"purpose\", \"flow\").\n            **attributes: Key-value pairs to merge into the node.\n        \"\"\"\n        if node_id not in self.nodes:\n            # We silently skip enrichment for non-existent nodes to avoid crashing \n            # on transient or virtual nodes created by some sub-systems\n            return\n            \n        node = self.nodes[node_id]\n        \n        # Merge attributes\n        for k, v in attributes.items():\n            node[k] = v\n            \n        # Track layer activation\n        if layer_name not in self.metadata[\"layers_activated\"]:\n            self.metadata[\"layers_activated\"].append(layer_name)",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.get_node",
      "name": "CodebaseState.get_node",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 102,
      "end_line": 103,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node_id",
          "type": "str"
        }
      ],
      "return_type": "Optional[Dict[str, Any]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:",
      "body_source": "    def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:\n        return self.nodes.get(node_id)",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.validate",
      "name": "CodebaseState.validate",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 105,
      "end_line": 122,
      "role": "Validator",
      "role_confidence": 88.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check for referential integrity violations.\nReturns a list of error messages.",
      "signature": "def validate(self) -> List[str]:",
      "body_source": "    def validate(self) -> List[str]:\n        \"\"\"\n        Check for referential integrity violations.\n        Returns a list of error messages.\n        \"\"\"\n        errors = []\n        \n        # 1. Edge integrity\n        for i, edge in enumerate(self.edges):\n            src = edge.get('source')\n            tgt = edge.get('target')\n            \n            if src not in self.nodes:\n                errors.append(f\"Edge {i} source not found: {src}\")\n            if tgt not in self.nodes:\n                errors.append(f\"Edge {i} target not found: {tgt}\")\n                \n        return errors",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py:CodebaseState.export",
      "name": "CodebaseState.export",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/data_management.py",
      "start_line": 124,
      "end_line": 133,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export the fully reconciled state as a unified dictionary\nready for visualization or JSON dump.",
      "signature": "def export(self) -> Dict[str, Any]:",
      "body_source": "    def export(self) -> Dict[str, Any]:\n        \"\"\"\n        Export the fully reconciled state as a unified dictionary\n        ready for visualization or JSON dump.\n        \"\"\"\n        return {\n            \"nodes\": list(self.nodes.values()),\n            \"edges\": self.edges,\n            \"metadata\": self.metadata\n        }",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:NodeStats",
      "name": "NodeStats",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 40,
      "end_line": 40,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class NodeStats:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:GraphAnalysisResult",
      "name": "GraphAnalysisResult",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 54,
      "end_line": 54,
      "role": "DTO",
      "role_confidence": 78.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class GraphAnalysisResult:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:load_graph",
      "name": "load_graph",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 65,
      "end_line": 104,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "path",
          "type": "str | Path"
        }
      ],
      "return_type": "'nx.DiGraph'",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load graph.json into a NetworkX DiGraph.",
      "signature": "def load_graph(path: str | Path) -> \"nx.DiGraph\":",
      "body_source": "def load_graph(path: str | Path) -> \"nx.DiGraph\":\n    \"\"\"Load graph.json into a NetworkX DiGraph.\"\"\"\n    if nx is None:\n        raise ImportError(\"NetworkX not installed. Run: pip install networkx\")\n    \n    path = Path(path)\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    \n    G = nx.DiGraph()\n    \n    # Add nodes from components\n    components = data.get(\"components\", {})\n    for node_id, info in components.items():\n        G.add_node(\n            node_id,\n            name=info.get(\"name\", node_id),\n            kind=info.get(\"kind\", \"unknown\"),\n            file=info.get(\"file\", \"\"),\n            role=info.get(\"role\"),\n        )\n    \n    # Add edges - try multiple possible formats\n    edges = data.get(\"edges\", data.get(\"relationships\", []))\n    edge_count = 0\n    for rel in edges:\n        # Try different key names\n        src = rel.get(\"source\") or rel.get(\"from\") or rel.get(\"src\")\n        tgt = rel.get(\"target\") or rel.get(\"to\") or rel.get(\"dst\")\n        if src and tgt:\n            # Add nodes if they don't exist (edge-first format)\n            if not G.has_node(src):\n                G.add_node(src, name=src.split(\"|\")[-2] if \"|\" in src else src, kind=\"unknown\", file=\"\")\n            if not G.has_node(tgt):\n                G.add_node(tgt, name=tgt.split(\"|\")[-2] if \"|\" in tgt else tgt, kind=\"unknown\", file=\"\")\n            G.add_edge(src, tgt, kind=rel.get(\"kind\", \"calls\"))\n            edge_count += 1\n    \n    print(f\"   Loaded {len(G.nodes)} nodes, {edge_count} edges\")\n    return G",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_bottlenecks",
      "name": "find_bottlenecks",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 107,
      "end_line": 139,
      "role": "Test",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "top_n",
          "type": "int",
          "default": "20"
        },
        {
          "name": "sample_size",
          "type": "int",
          "default": "500"
        }
      ],
      "return_type": "list[NodeStats]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find bottleneck nodes using betweenness centrality.\n\nHigh betweenness = many shortest paths go through this node.\nThese are coupling hotspots / God Functions.\n\nUses sampling for large graphs (>1000 nodes) for performance.",
      "signature": "def find_bottlenecks(G: \"nx.DiGraph\", top_n: int = 20, sample_size: int = 500) -> list[NodeStats]:",
      "body_source": "def find_bottlenecks(G: \"nx.DiGraph\", top_n: int = 20, sample_size: int = 500) -> list[NodeStats]:\n    \"\"\"\n    Find bottleneck nodes using betweenness centrality.\n    \n    High betweenness = many shortest paths go through this node.\n    These are coupling hotspots / God Functions.\n    \n    Uses sampling for large graphs (>1000 nodes) for performance.\n    \"\"\"\n    if nx is None:\n        return []\n    \n    # Sample for large graphs\n    k = min(sample_size, len(G.nodes)) if len(G.nodes) > 1000 else None\n    bc = nx.betweenness_centrality(G, k=k, normalized=True)\n    \n    # Sort by centrality\n    sorted_nodes = sorted(bc.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    \n    results = []\n    for node_id, centrality in sorted_nodes:\n        info = G.nodes.get(node_id, {})\n        results.append(NodeStats(\n            id=node_id,\n            name=info.get(\"name\", node_id.split(\"|\")[-2] if \"|\" in node_id else node_id),\n            kind=info.get(\"kind\", \"\"),\n            file=info.get(\"file\", \"\"),\n            in_degree=G.in_degree(node_id),\n            out_degree=G.out_degree(node_id),\n            betweenness=centrality,\n        ))\n    \n    return results",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_pagerank",
      "name": "find_pagerank",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 142,
      "end_line": 177,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "top_n",
          "type": "int",
          "default": "20"
        }
      ],
      "return_type": "list[NodeStats]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Rank nodes by PageRank (importance based on incoming links).\n\nHigh PageRank = many important nodes depend on this one.\nUses power iteration method (no scipy required).",
      "signature": "def find_pagerank(G: \"nx.DiGraph\", top_n: int = 20) -> list[NodeStats]:",
      "body_source": "def find_pagerank(G: \"nx.DiGraph\", top_n: int = 20) -> list[NodeStats]:\n    \"\"\"\n    Rank nodes by PageRank (importance based on incoming links).\n    \n    High PageRank = many important nodes depend on this one.\n    Uses power iteration method (no scipy required).\n    \"\"\"\n    if nx is None:\n        return []\n    \n    try:\n        # Try scipy-based first (faster for large graphs)\n        pr = nx.pagerank(G, alpha=0.85)\n    except ImportError:\n        # Fallback to power iteration (no scipy)\n        pr = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-6)\n    except Exception:\n        # If graph has issues, return empty\n        return []\n    \n    sorted_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    \n    results = []\n    for node_id, rank in sorted_nodes:\n        info = G.nodes.get(node_id, {})\n        results.append(NodeStats(\n            id=node_id,\n            name=info.get(\"name\", node_id),\n            kind=info.get(\"kind\", \"\"),\n            file=info.get(\"file\", \"\"),\n            in_degree=G.in_degree(node_id),\n            out_degree=G.out_degree(node_id),\n            pagerank=rank,\n        ))\n    \n    return results",
      "complexity": 0,
      "lines_of_code": 35,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_communities_leiden",
      "name": "find_communities_leiden",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 180,
      "end_line": 222,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "resolution",
          "type": "float",
          "default": "1.0"
        }
      ],
      "return_type": "dict[int, list[str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect communities using the Leiden algorithm (preferred).\n\nLeiden is an improvement over Louvain that guarantees connected communities\nand typically finds higher quality partitions.\n\nArgs:\n    G: NetworkX directed graph\n    resolution: Higher values = more smaller communities\n\nReturns dict: community_id -> list of node IDs.",
      "signature": "def find_communities_leiden(G: \"nx.DiGraph\", resolution: float = 1.0) -> dict[int, list[str]]:",
      "body_source": "def find_communities_leiden(G: \"nx.DiGraph\", resolution: float = 1.0) -> dict[int, list[str]]:\n    \"\"\"\n    Detect communities using the Leiden algorithm (preferred).\n    \n    Leiden is an improvement over Louvain that guarantees connected communities\n    and typically finds higher quality partitions.\n    \n    Args:\n        G: NetworkX directed graph\n        resolution: Higher values = more smaller communities\n    \n    Returns dict: community_id -> list of node IDs.\n    \"\"\"\n    if not HAS_LEIDEN:\n        print(\"   \u26a0\ufe0f  Leiden not available, falling back to Louvain\")\n        return find_communities_louvain(G)\n    \n    # Convert NetworkX to igraph\n    # igraph works with integer node IDs, so we need a mapping\n    node_list = list(G.nodes())\n    node_to_idx = {node: i for i, node in enumerate(node_list)}\n    \n    # Create igraph from edges\n    edges = [(node_to_idx[u], node_to_idx[v]) for u, v in G.edges() \n             if u in node_to_idx and v in node_to_idx]\n    \n    ig_graph = ig.Graph(n=len(node_list), edges=edges, directed=False)\n    \n    # Run Leiden algorithm with Modularity optimization\n    partition = leidenalg.find_partition(\n        ig_graph, \n        leidenalg.ModularityVertexPartition,\n        seed=42  # reproducible\n    )\n    \n    # Convert back to our format: community_id -> [node_ids]\n    communities = {}\n    for comm_id, members in enumerate(partition):\n        node_ids = [node_list[idx] for idx in members]\n        if node_ids:  # skip empty communities\n            communities[comm_id] = node_ids\n    \n    return communities",
      "complexity": 0,
      "lines_of_code": 42,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_communities_louvain",
      "name": "find_communities_louvain",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 225,
      "end_line": 250,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        }
      ],
      "return_type": "dict[int, list[str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect communities using Louvain algorithm (fallback).\n\nReturns dict: community_id -> list of node IDs.",
      "signature": "def find_communities_louvain(G: \"nx.DiGraph\") -> dict[int, list[str]]:",
      "body_source": "def find_communities_louvain(G: \"nx.DiGraph\") -> dict[int, list[str]]:\n    \"\"\"\n    Detect communities using Louvain algorithm (fallback).\n    \n    Returns dict: community_id -> list of node IDs.\n    \"\"\"\n    if community_louvain is None:\n        # Ultimate fallback: use connected components\n        undirected = G.to_undirected()\n        communities = {}\n        for i, component in enumerate(nx.connected_components(undirected)):\n            communities[i] = list(component)\n        return communities\n    \n    # Louvain works on undirected graphs\n    undirected = G.to_undirected()\n    partition = community_louvain.best_partition(undirected)\n    \n    # Invert: community_id -> [nodes]\n    communities = {}\n    for node, comm_id in partition.items():\n        if comm_id not in communities:\n            communities[comm_id] = []\n        communities[comm_id].append(node)\n    \n    return communities",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_communities",
      "name": "find_communities",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 253,
      "end_line": 266,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "algorithm",
          "type": "str",
          "default": "'auto'"
        }
      ],
      "return_type": "dict[int, list[str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect communities using the best available algorithm.\n\nArgs:\n    G: NetworkX directed graph\n    algorithm: \"leiden\", \"louvain\", or \"auto\" (default, uses Leiden if available)\n\nReturns dict: community_id -> list of node IDs.",
      "signature": "def find_communities(G: \"nx.DiGraph\", algorithm: str = \"auto\") -> dict[int, list[str]]:",
      "body_source": "def find_communities(G: \"nx.DiGraph\", algorithm: str = \"auto\") -> dict[int, list[str]]:\n    \"\"\"\n    Detect communities using the best available algorithm.\n    \n    Args:\n        G: NetworkX directed graph\n        algorithm: \"leiden\", \"louvain\", or \"auto\" (default, uses Leiden if available)\n    \n    Returns dict: community_id -> list of node IDs.\n    \"\"\"\n    if algorithm == \"leiden\" or (algorithm == \"auto\" and HAS_LEIDEN):\n        return find_communities_leiden(G)\n    else:\n        return find_communities_louvain(G)",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:find_bridges",
      "name": "find_bridges",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 269,
      "end_line": 281,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "limit",
          "type": "int",
          "default": "50"
        }
      ],
      "return_type": "list[tuple[str, str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find bridge edges whose removal would disconnect the graph.\n\nThese are critical coupling points - potential refactoring targets.",
      "signature": "def find_bridges(G: \"nx.DiGraph\", limit: int = 50) -> list[tuple[str, str]]:",
      "body_source": "def find_bridges(G: \"nx.DiGraph\", limit: int = 50) -> list[tuple[str, str]]:\n    \"\"\"\n    Find bridge edges whose removal would disconnect the graph.\n    \n    These are critical coupling points - potential refactoring targets.\n    \"\"\"\n    if nx is None:\n        return []\n    \n    # Bridges only defined for undirected graphs\n    undirected = G.to_undirected()\n    bridges = list(nx.bridges(undirected))\n    return bridges[:limit]",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:shortest_path",
      "name": "shortest_path",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 284,
      "end_line": 315,
      "role": "Test",
      "role_confidence": 90.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "source",
          "type": "str"
        },
        {
          "name": "target",
          "type": "str"
        }
      ],
      "return_type": "list[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Find shortest path between two nodes.\n\nAccepts either full ID or just function name (will search).",
      "signature": "def shortest_path(G: \"nx.DiGraph\", source: str, target: str) -> list[str]:",
      "body_source": "def shortest_path(G: \"nx.DiGraph\", source: str, target: str) -> list[str]:\n    \"\"\"\n    Find shortest path between two nodes.\n    \n    Accepts either full ID or just function name (will search).\n    \"\"\"\n    if nx is None:\n        return []\n    \n    # Resolve partial names to full IDs\n    def resolve(name):\n        if name in G.nodes:\n            return name\n        for node_id in G.nodes:\n            if f\"|{name}|\" in node_id or node_id.endswith(f\"|{name}\"):\n                return node_id\n        # Try name attribute\n        for node_id, info in G.nodes(data=True):\n            if info.get(\"name\") == name:\n                return node_id\n        return None\n    \n    src = resolve(source)\n    tgt = resolve(target)\n    \n    if not src or not tgt:\n        return []\n    \n    try:\n        return nx.shortest_path(G, src, tgt)\n    except nx.NetworkXNoPath:\n        return []",
      "complexity": 0,
      "lines_of_code": 31,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:shortest_path.resolve",
      "name": "shortest_path.resolve",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 294,
      "end_line": 304,
      "role": "Test",
      "role_confidence": 90.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "name"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def resolve(name):",
      "body_source": "    def resolve(name):\n        if name in G.nodes:\n            return name\n        for node_id in G.nodes:\n            if f\"|{name}|\" in node_id or node_id.endswith(f\"|{name}\"):\n                return node_id\n        # Try name attribute\n        for node_id, info in G.nodes(data=True):\n            if info.get(\"name\") == name:\n                return node_id\n        return None",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:suggest_refactoring_cuts",
      "name": "suggest_refactoring_cuts",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 318,
      "end_line": 347,
      "role": "Test",
      "role_confidence": 60.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "G",
          "type": "'nx.DiGraph'"
        },
        {
          "name": "top_n",
          "type": "int",
          "default": "10"
        }
      ],
      "return_type": "list[dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Suggest edges to remove that would best decouple the graph.\n\nUses edge betweenness: edges that appear on many shortest paths.\nRemoving these would create more isolated clusters.",
      "signature": "def suggest_refactoring_cuts(G: \"nx.DiGraph\", top_n: int = 10) -> list[dict]:",
      "body_source": "def suggest_refactoring_cuts(G: \"nx.DiGraph\", top_n: int = 10) -> list[dict]:\n    \"\"\"\n    Suggest edges to remove that would best decouple the graph.\n    \n    Uses edge betweenness: edges that appear on many shortest paths.\n    Removing these would create more isolated clusters.\n    \"\"\"\n    if nx is None:\n        return []\n    \n    # Sample for performance\n    k = min(200, len(G.nodes)) if len(G.nodes) > 500 else None\n    eb = nx.edge_betweenness_centrality(G, k=k, normalized=True)\n    \n    sorted_edges = sorted(eb.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    \n    results = []\n    for (src, tgt), centrality in sorted_edges:\n        src_name = G.nodes.get(src, {}).get(\"name\", src)\n        tgt_name = G.nodes.get(tgt, {}).get(\"name\", tgt)\n        results.append({\n            \"source\": src,\n            \"target\": tgt,\n            \"source_name\": src_name,\n            \"target_name\": tgt_name,\n            \"centrality\": centrality,\n            \"recommendation\": f\"Consider extracting {tgt_name} or introducing interface\"\n        })\n    \n    return results",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:analyze_full",
      "name": "analyze_full",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 350,
      "end_line": 374,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "graph_path",
          "type": "str | Path"
        },
        {
          "name": "top_n",
          "type": "int",
          "default": "20"
        }
      ],
      "return_type": "GraphAnalysisResult",
      "base_classes": [],
      "decorators": [],
      "docstring": "Run full analysis on a graph.",
      "signature": "def analyze_full(graph_path: str | Path, top_n: int = 20) -> GraphAnalysisResult:",
      "body_source": "def analyze_full(graph_path: str | Path, top_n: int = 20) -> GraphAnalysisResult:\n    \"\"\"Run full analysis on a graph.\"\"\"\n    G = load_graph(graph_path)\n    \n    result = GraphAnalysisResult(\n        node_count=len(G.nodes),\n        edge_count=len(G.edges),\n    )\n    \n    print(f\"\ud83d\udcca Analyzing graph: {result.node_count} nodes, {result.edge_count} edges\")\n    \n    print(\"\ud83d\udd0d Finding bottlenecks (betweenness centrality)...\")\n    result.bottlenecks = find_bottlenecks(G, top_n=top_n)\n    \n    print(\"\ud83d\udcc8 Computing PageRank...\")\n    result.top_pagerank = find_pagerank(G, top_n=top_n)\n    \n    print(\"\ud83e\udde9 Detecting communities...\")\n    result.communities = find_communities(G)\n    print(f\"   Found {len(result.communities)} communities\")\n    \n    print(\"\ud83c\udf09 Finding bridge edges...\")\n    result.bridges = find_bridges(G)\n    \n    return result",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py:generate_report",
      "name": "generate_report",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/graph_analyzer.py",
      "start_line": 377,
      "end_line": 450,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "result",
          "type": "GraphAnalysisResult"
        },
        {
          "name": "output_path",
          "type": "str | Path",
          "default": "None"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate markdown report from analysis results.",
      "signature": "def generate_report(result: GraphAnalysisResult, output_path: str | Path = None) -> str:",
      "body_source": "def generate_report(result: GraphAnalysisResult, output_path: str | Path = None) -> str:\n    \"\"\"Generate markdown report from analysis results.\"\"\"\n    lines = [\n        \"# \ud83d\udd2c Graph Analysis Report\",\n        \"\",\n        f\"**Nodes:** {result.node_count} | **Edges:** {result.edge_count} | **Communities:** {len(result.communities)}\",\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83d\udea8 Bottleneck Functions (High Betweenness Centrality)\",\n        \"\",\n        \"These nodes appear on many shortest paths\u2014coupling hotspots that everything flows through.\",\n        \"\",\n        \"| Rank | Function | File | In | Out | Centrality |\",\n        \"|-----:|----------|------|---:|----:|-----------:|\",\n    ]\n    \n    for i, node in enumerate(result.bottlenecks[:15], 1):\n        lines.append(f\"| {i} | `{node.name}` | {node.file} | {node.in_degree} | {node.out_degree} | {node.betweenness:.4f} |\")\n    \n    lines.extend([\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83d\udcc8 Most Important Nodes (PageRank)\",\n        \"\",\n        \"Nodes that many other important nodes depend on.\",\n        \"\",\n        \"| Rank | Function | File | PageRank |\",\n        \"|-----:|----------|------|--------:|\",\n    ])\n    \n    for i, node in enumerate(result.top_pagerank[:15], 1):\n        lines.append(f\"| {i} | `{node.name}` | {node.file} | {node.pagerank:.6f} |\")\n    \n    lines.extend([\n        \"\",\n        \"---\",\n        \"\",\n        \"## \ud83e\udde9 Community Summary\",\n        \"\",\n        f\"Detected **{len(result.communities)}** natural clusters.\",\n        \"\",\n        \"| Community | Size | Sample Members |\",\n        \"|----------:|-----:|----------------|\",\n    ])\n    \n    for comm_id in sorted(result.communities.keys(), key=lambda k: len(result.communities[k]), reverse=True)[:10]:\n        members = result.communities[comm_id]\n        sample = [m.split(\"|\")[-2] if \"|\" in m else m[:30] for m in members[:3]]\n        lines.append(f\"| {comm_id} | {len(members)} | {', '.join(sample)} |\")\n    \n    if result.bridges:\n        lines.extend([\n            \"\",\n            \"---\",\n            \"\",\n            \"## \ud83c\udf09 Critical Bridge Edges\",\n            \"\",\n            \"Removing these edges would split the graph\u2014key coupling points.\",\n            \"\",\n        ])\n        for src, tgt in result.bridges[:10]:\n            src_short = src.split(\"|\")[-2] if \"|\" in src else src[:30]\n            tgt_short = tgt.split(\"|\")[-2] if \"|\" in tgt else tgt[:30]\n            lines.append(f\"- `{src_short}` \u2192 `{tgt_short}`\")\n    \n    report = \"\\n\".join(lines)\n    \n    if output_path:\n        Path(output_path).write_text(report, encoding=\"utf-8\")\n        print(f\"\ud83d\udcdd Report saved to: {output_path}\")\n    \n    return report",
      "complexity": 0,
      "lines_of_code": 73,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightType",
      "name": "InsightType",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 14,
      "end_line": 14,
      "role": "DTO",
      "role_confidence": 80.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class InsightType(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:Priority",
      "name": "Priority",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 24,
      "end_line": 24,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Priority(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:Insight",
      "name": "Insight",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 33,
      "end_line": 33,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Insight:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:OptimizationSchema",
      "name": "OptimizationSchema",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 46,
      "end_line": 46,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class OptimizationSchema:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine",
      "name": "InsightsEngine",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 234,
      "end_line": 234,
      "role": "DTO",
      "role_confidence": 82.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class InsightsEngine:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine.__init__",
      "name": "InsightsEngine.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 237,
      "end_line": 239,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.insights: List[Insight] = []\n        self.schemas = SCHEMAS",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine.analyze",
      "name": "InsightsEngine.analyze",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 241,
      "end_line": 274,
      "role": "Factory",
      "role_confidence": 88.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list",
          "default": "None"
        }
      ],
      "return_type": "List[Insight]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze Collider output and generate insights.\n\nArgs:\n    nodes: List of classified nodes from Collider\n    edges: List of edges (optional)\n\nReturns:\n    List of actionable insights",
      "signature": "def analyze(self, nodes: list, edges: list = None) -> List[Insight]:",
      "body_source": "    def analyze(self, nodes: list, edges: list = None) -> List[Insight]:\n        \"\"\"\n        Analyze Collider output and generate insights.\n        \n        Args:\n            nodes: List of classified nodes from Collider\n            edges: List of edges (optional)\n        \n        Returns:\n            List of actionable insights\n        \"\"\"\n        edges = edges or []\n        self.insights = []\n        \n        # Count roles\n        role_counts = Counter()\n        for node in nodes:\n            role = node.get('role', node.role if hasattr(node, 'role') else 'Unknown')\n            role_counts[role] += 1\n        \n        # Run all insight detectors\n        self._check_missing_repositories(role_counts)\n        self._check_missing_tests(role_counts, len(nodes))\n        self._check_service_layer(role_counts)\n        self._check_cqrs_opportunity(role_counts)\n        self._check_god_class_risk(nodes)\n        self._check_pure_function_opportunity(nodes)\n        self._check_layer_violations(nodes, edges)\n        \n        # Sort by priority\n        priority_order = {Priority.CRITICAL: 0, Priority.HIGH: 1, Priority.MEDIUM: 2, Priority.LOW: 3}\n        self.insights.sort(key=lambda x: priority_order[x.priority])\n        \n        return self.insights",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_missing_repositories",
      "name": "InsightsEngine._check_missing_repositories",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 276,
      "end_line": 292,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "role_counts",
          "type": "Counter"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect missing repository pattern",
      "signature": "def _check_missing_repositories(self, role_counts: Counter):",
      "body_source": "    def _check_missing_repositories(self, role_counts: Counter):\n        \"\"\"Detect missing repository pattern\"\"\"\n        entities = role_counts.get('Entity', 0) + role_counts.get('DTO', 0)\n        repos = role_counts.get('Repository', 0)\n        \n        if entities > 5 and repos < entities * 0.3:\n            missing = entities - repos\n            self.insights.append(Insight(\n                type=InsightType.ARCHITECTURE,\n                priority=Priority.MEDIUM,\n                title=\"Missing Repository Pattern\",\n                description=f\"Found {entities} data entities but only {repos} repositories.\",\n                affected_components=[f\"{missing} entities without repositories\"],\n                recommendation=\"Apply Repository Pattern to abstract data access\",\n                effort_estimate=\"medium\",\n                schema=\"REPOSITORY_PATTERN\"\n            ))",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_missing_tests",
      "name": "InsightsEngine._check_missing_tests",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 294,
      "end_line": 314,
      "role": "Test",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "role_counts",
          "type": "Counter"
        },
        {
          "name": "total_nodes",
          "type": "int"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect test coverage gaps",
      "signature": "def _check_missing_tests(self, role_counts: Counter, total_nodes: int):",
      "body_source": "    def _check_missing_tests(self, role_counts: Counter, total_nodes: int):\n        \"\"\"Detect test coverage gaps\"\"\"\n        tests = role_counts.get('Test', 0)\n        logic = (role_counts.get('Service', 0) + \n                 role_counts.get('Command', 0) + \n                 role_counts.get('ApplicationService', 0) +\n                 role_counts.get('UseCase', 0))\n        \n        test_ratio = tests / logic if logic > 0 else 1.0\n        \n        if logic > 10 and test_ratio < 0.5:\n            self.insights.append(Insight(\n                type=InsightType.TESTING,\n                priority=Priority.HIGH,\n                title=\"Low Test Coverage\",\n                description=f\"Only {tests} tests for {logic} logic components ({test_ratio:.0%} ratio).\",\n                affected_components=[f\"{logic - tests} untested components\"],\n                recommendation=\"Add tests for critical business logic\",\n                effort_estimate=\"high\",\n                schema=\"TEST_COVERAGE\"\n            ))",
      "complexity": 0,
      "lines_of_code": 20,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_service_layer",
      "name": "InsightsEngine._check_service_layer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 316,
      "end_line": 331,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "role_counts",
          "type": "Counter"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect missing service layer",
      "signature": "def _check_service_layer(self, role_counts: Counter):",
      "body_source": "    def _check_service_layer(self, role_counts: Counter):\n        \"\"\"Detect missing service layer\"\"\"\n        controllers = role_counts.get('Controller', 0)\n        services = role_counts.get('Service', 0) + role_counts.get('ApplicationService', 0)\n        \n        if controllers > 5 and services < controllers * 0.5:\n            self.insights.append(Insight(\n                type=InsightType.REFACTORING,\n                priority=Priority.MEDIUM,\n                title=\"Thin Service Layer\",\n                description=f\"Found {controllers} controllers but only {services} services.\",\n                affected_components=[\"Controllers may contain business logic\"],\n                recommendation=\"Extract business logic into service layer\",\n                effort_estimate=\"medium\",\n                schema=\"SERVICE_EXTRACTION\"\n            ))",
      "complexity": 0,
      "lines_of_code": 15,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_cqrs_opportunity",
      "name": "InsightsEngine._check_cqrs_opportunity",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 333,
      "end_line": 350,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "role_counts",
          "type": "Counter"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect CQRS opportunity",
      "signature": "def _check_cqrs_opportunity(self, role_counts: Counter):",
      "body_source": "    def _check_cqrs_opportunity(self, role_counts: Counter):\n        \"\"\"Detect CQRS opportunity\"\"\"\n        queries = role_counts.get('Query', 0)\n        commands = role_counts.get('Command', 0)\n        \n        if queries > 20 and commands > 20:\n            ratio = queries / commands if commands > 0 else 1\n            if 0.3 < ratio < 3:  # Balanced mix suggests CQRS opportunity\n                self.insights.append(Insight(\n                    type=InsightType.ARCHITECTURE,\n                    priority=Priority.LOW,\n                    title=\"CQRS Opportunity\",\n                    description=f\"Balanced Query/Command mix ({queries} queries, {commands} commands).\",\n                    affected_components=[\"Query and Command operations\"],\n                    recommendation=\"Consider CQRS pattern for scalability\",\n                    effort_estimate=\"high\",\n                    schema=\"CQRS_SEPARATION\"\n                ))",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_god_class_risk",
      "name": "InsightsEngine._check_god_class_risk",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 352,
      "end_line": 375,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect potential God classes",
      "signature": "def _check_god_class_risk(self, nodes: list):",
      "body_source": "    def _check_god_class_risk(self, nodes: list):\n        \"\"\"Detect potential God classes\"\"\"\n        # Count methods per class\n        class_methods = Counter()\n        for node in nodes:\n            name = node.get('name', node.name if hasattr(node, 'name') else '')\n            if '.' in name:\n                class_name = name.rsplit('.', 1)[0]\n                class_methods[class_name] += 1\n        \n        # Find classes with many methods\n        god_classes = [cls for cls, count in class_methods.items() if count > 20]\n        \n        if god_classes:\n            self.insights.append(Insight(\n                type=InsightType.REFACTORING,\n                priority=Priority.HIGH,\n                title=\"God Class Detected\",\n                description=f\"Found {len(god_classes)} classes with 20+ methods.\",\n                affected_components=god_classes[:5],\n                recommendation=\"Decompose into smaller, focused classes\",\n                effort_estimate=\"high\",\n                schema=\"GOD_CLASS_DECOMPOSITION\"\n            ))",
      "complexity": 0,
      "lines_of_code": 23,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_pure_function_opportunity",
      "name": "InsightsEngine._check_pure_function_opportunity",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 377,
      "end_line": 393,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect pure function extraction opportunities",
      "signature": "def _check_pure_function_opportunity(self, nodes: list):",
      "body_source": "    def _check_pure_function_opportunity(self, nodes: list):\n        \"\"\"Detect pure function extraction opportunities\"\"\"\n        queries = [n for n in nodes if n.get('role', getattr(n, 'role', '')) == 'Query']\n        utilities = [n for n in nodes if n.get('role', getattr(n, 'role', '')) == 'Utility']\n        \n        pure_candidates = len(queries) + len(utilities)\n        if pure_candidates > 30:\n            self.insights.append(Insight(\n                type=InsightType.PERFORMANCE,\n                priority=Priority.LOW,\n                title=\"Pure Function Optimization\",\n                description=f\"Found {pure_candidates} potentially pure functions (Queries + Utilities).\",\n                affected_components=[f\"{len(queries)} queries, {len(utilities)} utilities\"],\n                recommendation=\"Verify purity and add caching where beneficial\",\n                effort_estimate=\"low\",\n                schema=\"PURE_FUNCTION_EXTRACTION\"\n            ))",
      "complexity": 0,
      "lines_of_code": 16,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine._check_layer_violations",
      "name": "InsightsEngine._check_layer_violations",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 395,
      "end_line": 467,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Detect layer violations from edges",
      "signature": "def _check_layer_violations(self, nodes: list, edges: list):",
      "body_source": "    def _check_layer_violations(self, nodes: list, edges: list):\n        \"\"\"Detect layer violations from edges\"\"\"\n        # Layer order (higher = deeper in architecture)\n        ROLE_TO_LAYER = {\n            'Controller': 'presentation',\n            'View': 'presentation',\n            \n            'ApplicationService': 'application',\n            'UseCase': 'application',\n            'Service': 'application',\n            \n            'Entity': 'domain',\n            'ValueObject': 'domain',\n            'DomainService': 'domain',\n            'Policy': 'domain',\n            'Specification': 'domain',\n            \n            'Repository': 'infrastructure',\n            'RepositoryImpl': 'infrastructure',\n            'Gateway': 'infrastructure',\n            'Adapter': 'infrastructure',\n            'Configuration': 'infrastructure',\n        }\n        \n        LAYER_ORDER = {\n            'presentation': 0,\n            'application': 1,\n            'domain': 2,\n            'infrastructure': 3,\n        }\n        \n        # Build node lookup\n        node_lookup = {}\n        for node in nodes:\n            name = node.get('name', getattr(node, 'name', ''))\n            role = node.get('role', getattr(node, 'role', 'Unknown'))\n            node_lookup[name] = role\n        \n        violations = []\n        for edge in edges:\n            # Handle dict or tuple edges\n            if isinstance(edge, dict):\n                source = edge.get('source', edge.get('from', ''))\n                target = edge.get('target', edge.get('to', ''))\n            elif isinstance(edge, (list, tuple)) and len(edge) >= 2:\n                source, target = edge[0], edge[1]\n            else:\n                continue\n            \n            source_role = node_lookup.get(source, 'Unknown')\n            target_role = node_lookup.get(target, 'Unknown')\n            \n            source_layer = ROLE_TO_LAYER.get(source_role, 'unknown')\n            target_layer = ROLE_TO_LAYER.get(target_role, 'unknown')\n            \n            source_order = LAYER_ORDER.get(source_layer, 99)\n            target_order = LAYER_ORDER.get(target_layer, 99)\n            \n            # Violation: deeper layer calling shallower layer\n            if source_order > target_order and source_layer != 'unknown' and target_layer != 'unknown':\n                violations.append(f\"{source} ({source_layer}) \u2192 {target} ({target_layer})\")\n        \n        if len(violations) > 5:\n            self.insights.append(Insight(\n                type=InsightType.ARCHITECTURE,\n                priority=Priority.HIGH,\n                title=\"Layer Boundary Violations\",\n                description=f\"Found {len(violations)} cross-layer violations (infrastructure calling presentation).\",\n                affected_components=violations[:5],\n                recommendation=\"Enforce layer boundaries - dependencies should point inward\",\n                effort_estimate=\"medium\",\n                schema=\"LAYER_ENFORCEMENT\"\n            ))",
      "complexity": 0,
      "lines_of_code": 72,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:InsightsEngine.get_report",
      "name": "InsightsEngine.get_report",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 469,
      "end_line": 502,
      "role": "Factory",
      "role_confidence": 95.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate human-readable report",
      "signature": "def get_report(self) -> str:",
      "body_source": "    def get_report(self) -> str:\n        \"\"\"Generate human-readable report\"\"\"\n        if not self.insights:\n            return \"\u2705 No significant issues detected.\"\n        \n        lines = [\"# Actionable Insights Report\\n\"]\n        \n        # Group by type\n        by_type = {}\n        for insight in self.insights:\n            key = insight.type.value\n            if key not in by_type:\n                by_type[key] = []\n            by_type[key].append(insight)\n        \n        for type_name, insights in by_type.items():\n            lines.append(f\"\\n## {type_name.upper()}\\n\")\n            for i, insight in enumerate(insights, 1):\n                lines.append(f\"### {i}. {insight.title} [{insight.priority.value.upper()}]\")\n                lines.append(f\"\\n{insight.description}\\n\")\n                lines.append(f\"**Affected:** {', '.join(insight.affected_components[:3])}\")\n                lines.append(f\"\\n**Recommendation:** {insight.recommendation}\")\n                lines.append(f\"\\n**Effort:** {insight.effort_estimate}\")\n                if insight.schema:\n                    schema = self.schemas.get(insight.schema)\n                    if schema:\n                        lines.append(f\"\\n\\n**Optimization Schema: {schema.name}**\")\n                        lines.append(f\"\\n{schema.description}\")\n                        lines.append(f\"\\n\\nSteps:\")\n                        for step in schema.steps:\n                            lines.append(f\"\\n- {step}\")\n                lines.append(\"\\n---\\n\")\n        \n        return \"\\n\".join(lines)",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py:generate_insights",
      "name": "generate_insights",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/insights_engine.py",
      "start_line": 505,
      "end_line": 519,
      "role": "Factory",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "nodes",
          "type": "list"
        },
        {
          "name": "edges",
          "type": "list",
          "default": "None"
        }
      ],
      "return_type": "Tuple[List[Insight], str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to generate insights.\n\nUsage:\n    from insights_engine import generate_insights\n    \n    result = analyze(path)\n    insights, report = generate_insights(result.nodes, result.edges)\n    print(report)",
      "signature": "def generate_insights(nodes: list, edges: list = None) -> Tuple[List[Insight], str]:",
      "body_source": "def generate_insights(nodes: list, edges: list = None) -> Tuple[List[Insight], str]:\n    \"\"\"\n    Convenience function to generate insights.\n    \n    Usage:\n        from insights_engine import generate_insights\n        \n        result = analyze(path)\n        insights, report = generate_insights(result.nodes, result.edges)\n        print(report)\n    \"\"\"\n    engine = InsightsEngine()\n    insights = engine.analyze(nodes, edges)\n    report = engine.get_report()\n    return insights, report",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py:HealthStatus",
      "name": "HealthStatus",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py",
      "start_line": 14,
      "end_line": 14,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class HealthStatus:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py:SystemHealth",
      "name": "SystemHealth",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py",
      "start_line": 19,
      "end_line": 19,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SystemHealth:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py:SystemHealth.check_all",
      "name": "SystemHealth.check_all",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py",
      "start_line": 32,
      "end_line": 71,
      "role": "Validator",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "HealthStatus",
      "base_classes": [],
      "decorators": [
        "staticmethod"
      ],
      "docstring": "",
      "signature": "def check_all() -> HealthStatus:",
      "body_source": "    def check_all() -> HealthStatus:\n        checks = []\n        warnings = []\n        all_passed = True\n        \n        # 1. Check Python Version\n        py_ver = sys.version_info\n        if py_ver.major == 3 and py_ver.minor >= 8:\n            checks.append((\"Python Version\", True, f\"{platform.python_version()}\"))\n        else:\n            checks.append((\"Python Version\", False, f\"Found {platform.python_version()}, need 3.8+\"))\n            all_passed = False\n            \n        # 2. Check Core Bindings\n        for pkg, desc in SystemHealth.REQUIRED_PACKAGES.items():\n            is_critical = pkg in [\"tree_sitter\", \"tree_sitter_python\"]\n            \n            try:\n                importlib.import_module(pkg)\n                checks.append((f\"{desc} ({pkg})\", True, \"Installed\"))\n            except ImportError:\n                if not is_critical:\n                    checks.append((f\"{desc} ({pkg})\", False, \"Missing (Optional)\"))\n                    warnings.append(f\"Missing optional binding: {pkg}\")\n                else:\n                    checks.append((f\"{desc} ({pkg})\", False, \"MISSING - CRITICAL\"))\n                    all_passed = False\n        \n        # 3. File System Check\n        try:\n            with open(\".health_check_tmp\", \"w\") as f:\n                f.write(\"test\")\n            import os\n            os.remove(\".health_check_tmp\")\n            checks.append((\"File System Write\", True, \"Writable\"))\n        except Exception as e:\n            checks.append((\"File System Write\", False, f\"Failed: {e}\"))\n            all_passed = False\n            \n        return HealthStatus(is_healthy=all_passed, checks=checks, warnings=warnings)",
      "complexity": 0,
      "lines_of_code": 39,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py:SystemHealth.print_checklist",
      "name": "SystemHealth.print_checklist",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/system_health.py",
      "start_line": 74,
      "end_line": 102,
      "role": "Validator",
      "role_confidence": 0.8,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "exit_on_fail",
          "type": "bool",
          "default": "False"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "staticmethod"
      ],
      "docstring": "Run checks and print ASCII checklist.",
      "signature": "def print_checklist(exit_on_fail: bool = False):",
      "body_source": "    def print_checklist(exit_on_fail: bool = False):\n        \"\"\"Run checks and print ASCII checklist.\"\"\"\n        print(f\"\\n\ud83d\udee1\ufe0f  PRE-FLIGHT SYSTEM CHECK\")\n        print(\"=\" * 60)\n        \n        status = SystemHealth.check_all()\n        \n        for name, passed, msg in status.checks:\n            icon = \"\u2705\" if passed else \"\u274c\"\n            # Special case for optional missing\n            if not passed and (\"Optional\" in msg or \"Skipped\" in msg):\n                icon = \"\u26a0\ufe0f \"\n            \n            print(f\"{icon} {name:<30} : {msg}\")\n            \n        print(\"-\" * 60)\n        \n        if status.warnings:\n            print(\"\\n\u26a0\ufe0f  WARNINGS:\")\n            for w in status.warnings:\n                print(f\"   - {w}\")\n                \n        if not status.is_healthy:\n            print(\"\\n\u274c SYSTEM UNHEALTHY. ABORTING LAUNCH.\")\n            print(\"Please install missing dependencies.\")\n            if exit_on_fail:\n                sys.exit(1)\n        else:\n            print(\"\ud83d\ude80 SYSTEM READY. ENGINES IGNITED.\\n\")",
      "complexity": 0,
      "lines_of_code": 28,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:FunctionBody",
      "name": "FunctionBody",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 21,
      "end_line": 21,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class FunctionBody:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:ClassBody",
      "name": "ClassBody",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 56,
      "end_line": 56,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ClassBody:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteCodebase",
      "name": "CompleteCodebase",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 82,
      "end_line": 82,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class CompleteCodebase:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteCodebase.get_stats",
      "name": "CompleteCodebase.get_stats",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 104,
      "end_line": 111,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def get_stats(self) -> Dict:",
      "body_source": "    def get_stats(self) -> Dict:\n        return {\n            \"files\": len(self.files),\n            \"functions\": len(self.functions),\n            \"classes\": len(self.classes),\n            \"total_lines\": sum(s.count('\\n') + 1 for s in self.files.values()),\n            \"total_bytes\": sum(len(s) for s in self.files.values()),\n        }",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor",
      "name": "CompleteExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 114,
      "end_line": 114,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class CompleteExtractor:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor.__init__",
      "name": "CompleteExtractor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 119,
      "end_line": 121,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.parsers = {}\n        self._init_parsers()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._init_parsers",
      "name": "CompleteExtractor._init_parsers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 123,
      "end_line": 125,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def _init_parsers(self):",
      "body_source": "    def _init_parsers(self):\n        from core.language_loader import LanguageLoader\n        self.parsers, self.languages, self.extensions = LanguageLoader.load_all()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor.extract",
      "name": "CompleteExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 127,
      "end_line": 163,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str",
          "default": "'python'"
        }
      ],
      "return_type": "CompleteCodebase",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract complete codebase representation.",
      "signature": "def extract(self, repo_path: str, language: str = \"python\") -> CompleteCodebase:",
      "body_source": "    def extract(self, repo_path: str, language: str = \"python\") -> CompleteCodebase:\n        \"\"\"Extract complete codebase representation.\"\"\"\n        path = Path(repo_path)\n        codebase = CompleteCodebase()\n        \n        parser = self.parsers.get(language)\n        if not parser:\n            raise ValueError(f\"Unsupported language: {language}\")\n        \n        # Use extensions from loader\n        patterns = self.extensions.get(language, [\"*.py\"])\n        found_files = []\n        for pattern in patterns:\n            # Ensure pattern is a glob (e.g. \".ts\" -> \"*.ts\")\n            glob_pat = pattern if pattern.startswith(\"*\") else f\"*{pattern}\"\n            found_files.extend(list(path.rglob(glob_pat)))\n\n        for py_file in found_files:\n            if any(x in str(py_file) for x in [\"__pycache__\", \"node_modules\", \".git\", \".venv\", \"dddlint_env\", \"output/\"]):\n                continue\n            \n            try:\n                rel_path = str(py_file.relative_to(path))\n                code = py_file.read_text(errors='replace')\n                code_bytes = code.encode()\n                tree = parser.parse(code_bytes)\n                \n                # Store raw source\n                codebase.files[rel_path] = code\n                \n                # Extract structured components\n                self._extract_file(codebase, rel_path, code, code_bytes, tree)\n                \n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n        \n        return codebase",
      "complexity": 0,
      "lines_of_code": 36,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._extract_file",
      "name": "CompleteExtractor._extract_file",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 165,
      "end_line": 210,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "codebase",
          "type": "CompleteCodebase"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "code",
          "type": "str"
        },
        {
          "name": "code_bytes",
          "type": "bytes"
        },
        {
          "name": "tree"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract all components from a file.",
      "signature": "def _extract_file(self, codebase: CompleteCodebase, file_path: str,",
      "body_source": "    def _extract_file(self, codebase: CompleteCodebase, file_path: str, \n                      code: str, code_bytes: bytes, tree):\n        \"\"\"Extract all components from a file.\"\"\"\n        \n        root = tree.root_node\n        \n        # Track imports\n        codebase.imports[file_path] = []\n        codebase.string_literals[file_path] = []\n        codebase.numeric_constants[file_path] = []\n        \n        # Polyglot AST Mapping\n        FUNCTION_TYPES = {\n            \"function_definition\", \"async_function_definition\", # Python\n            \"function_declaration\", \"method_definition\", \"arrow_function\", # TS/JS\n            \"func_literal\", \"function_declaration\", \"method_declaration\", # Go\n            \"method_declaration\", \"constructor_declaration\" # Java\n        }\n        CLASS_TYPES = {\n            \"class_definition\", # Python\n            \"class_declaration\", # TS/JS/Java\n            \"type_spec\", # Go (structs)\n        }\n        \n        for child in root.children:\n            if child.type in [\"import_statement\", \"import_from_statement\", \"import_declaration\"]:\n                codebase.imports[file_path].append(child.text.decode())\n            elif child.type in CLASS_TYPES:\n                class_body = self._extract_class(child, file_path, code)\n                codebase.classes[class_body.id] = class_body\n            elif child.type in FUNCTION_TYPES:\n                func_body = self._extract_function(child, file_path, code)\n                codebase.functions[func_body.id] = func_body\n            \n            # Recurse for nested (e.g. export const class ...)\n            if child.type in [\"export_statement\", \"lexical_declaration\"]:\n                for sub in child.children:\n                    if sub.type in CLASS_TYPES:\n                        class_body = self._extract_class(sub, file_path, code)\n                        codebase.classes[class_body.id] = class_body\n                    elif sub.type in FUNCTION_TYPES:\n                        func_body = self._extract_function(sub, file_path, code)\n                        codebase.functions[func_body.id] = func_body\n        \n        # Extract all literals from the file\n        self._extract_literals(root, codebase, file_path)",
      "complexity": 0,
      "lines_of_code": 45,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._extract_function",
      "name": "CompleteExtractor._extract_function",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 212,
      "end_line": 320,
      "role": "Transformer",
      "role_confidence": 78.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "code",
          "type": "str"
        },
        {
          "name": "parent_class",
          "type": "str",
          "default": "''"
        }
      ],
      "return_type": "FunctionBody",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract complete function body.",
      "signature": "def _extract_function(self, node, file_path: str, code: str,",
      "body_source": "    def _extract_function(self, node, file_path: str, code: str, \n                          parent_class: str = \"\") -> FunctionBody:\n        \"\"\"Extract complete function body.\"\"\"\n        \n        # Get function name\n        func_name = \"\"\n        for child in node.children:\n            if child.type == \"identifier\":\n                func_name = child.text.decode()\n                break\n        \n        # Get source code\n        start = node.start_byte\n        end = node.end_byte\n        source = code[start:end] if isinstance(code, str) else code.decode()[start:end]\n        \n        # Parse parameters\n        parameters = []\n        for child in node.children:\n            if child.type in [\"parameters\", \"formal_parameters\", \"parameter_list\"]:\n                parameters = self._parse_parameters(child)\n                break\n        \n        # Get return type\n        return_type = None\n        for child in node.children:\n            if child.type == \"type\":\n                return_type = child.text.decode()\n                break\n        \n        # Get decorators\n        decorators = []\n        # Check previous siblings for decorators\n        \n        # Get docstring\n        docstring = None\n        for child in node.children:\n            if child.type == \"block\":\n                for stmt in child.children:\n                    if stmt.type == \"expression_statement\":\n                        for expr in stmt.children:\n                            if expr.type == \"string\":\n                                docstring = expr.text.decode().strip('\\\"\\'')\n                                break\n                        break\n                break\n        \n        # Analyze body\n        local_vars = []\n        calls = []\n        string_literals = []\n        numeric_literals = []\n        \n        def analyze_body(n):\n            if n.type == \"assignment\":\n                for c in n.children:\n                    if c.type == \"identifier\":\n                        local_vars.append(c.text.decode())\n                        break\n            elif n.type in [\"call\", \"call_expression\", \"method_invocation\"]:\n                callee = self._get_callee(n)\n                if callee:\n                    calls.append(callee)\n            elif n.type == \"string\":\n                string_literals.append(n.text.decode())\n            elif n.type == \"integer\":\n                try:\n                    numeric_literals.append(int(n.text.decode()))\n                except:\n                    pass\n            elif n.type == \"float\":\n                try:\n                    numeric_literals.append(float(n.text.decode()))\n                except:\n                    pass\n            \n            for c in n.children:\n                analyze_body(c)\n        \n        analyze_body(node)\n        \n        # Compute AST hash\n        ast_hash = hashlib.md5(source.encode()).hexdigest()[:12]\n        \n        # Determine function type\n        is_async = node.type == \"async_function_definition\"\n        is_generator = \"yield\" in source\n        \n        func_id = f\"{file_path}:{parent_class}.{func_name}\" if parent_class else f\"{file_path}:{func_name}\"\n        \n        return FunctionBody(\n            id=func_id,\n            name=func_name,\n            file=file_path,\n            start_line=node.start_point[0] + 1,\n            end_line=node.end_point[0] + 1,\n            source_code=source,\n            parameters=parameters,\n            return_type=return_type,\n            decorators=decorators,\n            docstring=docstring,\n            local_vars=list(set(local_vars)),\n            calls=list(set(calls)),\n            string_literals=string_literals[:10],  # Limit\n            numeric_literals=numeric_literals[:10],\n            ast_hash=ast_hash,\n            is_async=is_async,\n            is_generator=is_generator,\n        )",
      "complexity": 0,
      "lines_of_code": 108,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor.analyze_body",
      "name": "CompleteExtractor.analyze_body",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 265,
      "end_line": 289,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "n"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def analyze_body(n):",
      "body_source": "        def analyze_body(n):\n            if n.type == \"assignment\":\n                for c in n.children:\n                    if c.type == \"identifier\":\n                        local_vars.append(c.text.decode())\n                        break\n            elif n.type in [\"call\", \"call_expression\", \"method_invocation\"]:\n                callee = self._get_callee(n)\n                if callee:\n                    calls.append(callee)\n            elif n.type == \"string\":\n                string_literals.append(n.text.decode())\n            elif n.type == \"integer\":\n                try:\n                    numeric_literals.append(int(n.text.decode()))\n                except:\n                    pass\n            elif n.type == \"float\":\n                try:\n                    numeric_literals.append(float(n.text.decode()))\n                except:\n                    pass\n            \n            for c in n.children:\n                analyze_body(c)",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._extract_class",
      "name": "CompleteExtractor._extract_class",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 322,
      "end_line": 402,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "code",
          "type": "str"
        }
      ],
      "return_type": "ClassBody",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract complete class body.",
      "signature": "def _extract_class(self, node, file_path: str, code: str) -> ClassBody:",
      "body_source": "    def _extract_class(self, node, file_path: str, code: str) -> ClassBody:\n        \"\"\"Extract complete class body.\"\"\"\n        \n        # Get class name\n        class_name = \"\"\n        for child in node.children:\n            if child.type == \"identifier\":\n                class_name = child.text.decode()\n                break\n        \n        # Get source code\n        start = node.start_byte\n        end = node.end_byte\n        source = code[start:end] if isinstance(code, str) else code.decode()[start:end]\n        \n        # Get base classes\n        bases = []\n        for child in node.children:\n            if child.type == \"argument_list\":\n                for arg in child.children:\n                    if arg.type == \"identifier\":\n                        bases.append(arg.text.decode())\n                    elif arg.type == \"attribute\":\n                        bases.append(arg.text.decode())\n        \n        # Analyze class body\n        methods = []\n        class_vars = []\n        instance_vars = []\n        properties = []\n        dunder_methods = []\n        docstring = None\n        \n        for child in node.children:\n            if child.type == \"block\":\n                for stmt in child.children:\n                    if stmt.type in (\"function_definition\", \"async_function_definition\"):\n                        method_name = \"\"\n                        for c in stmt.children:\n                            if c.type == \"identifier\":\n                                method_name = c.text.decode()\n                                break\n                        methods.append(method_name)\n                        \n                        if method_name.startswith(\"__\") and method_name.endswith(\"__\"):\n                            dunder_methods.append(method_name)\n                        \n                        # Check for @property\n                        # Extract instance vars from __init__\n                        if method_name == \"__init__\":\n                            for c in stmt.children:\n                                if c.type == \"block\":\n                                    self._extract_instance_vars(c, instance_vars)\n                    \n                    elif stmt.type == \"expression_statement\":\n                        # Class-level assignments or docstring\n                        for expr in stmt.children:\n                            if expr.type == \"string\" and docstring is None:\n                                docstring = expr.text.decode().strip('\\\"\\'')\n                            elif expr.type == \"assignment\":\n                                for c in expr.children:\n                                    if c.type == \"identifier\":\n                                        class_vars.append(c.text.decode())\n                                        break\n        \n        return ClassBody(\n            id=f\"{file_path}:{class_name}\",\n            name=class_name,\n            file=file_path,\n            start_line=node.start_point[0] + 1,\n            end_line=node.end_point[0] + 1,\n            source_code=source,\n            bases=bases,\n            decorators=[],\n            docstring=docstring,\n            methods=methods,\n            class_vars=class_vars,\n            instance_vars=list(set(instance_vars)),\n            properties=properties,\n            dunder_methods=dunder_methods,\n        )",
      "complexity": 0,
      "lines_of_code": 80,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._extract_instance_vars",
      "name": "CompleteExtractor._extract_instance_vars",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 404,
      "end_line": 416,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "block_node"
        },
        {
          "name": "instance_vars",
          "type": "List[str]"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract self.x assignments from __init__.",
      "signature": "def _extract_instance_vars(self, block_node, instance_vars: List[str]):",
      "body_source": "    def _extract_instance_vars(self, block_node, instance_vars: List[str]):\n        \"\"\"Extract self.x assignments from __init__.\"\"\"\n        def visit(node):\n            if node.type == \"assignment\":\n                for child in node.children:\n                    if child.type == \"attribute\":\n                        text = child.text.decode()\n                        if text.startswith(\"self.\"):\n                            instance_vars.append(text[5:])  # Remove \"self.\"\n                        break\n            for child in node.children:\n                visit(child)\n        visit(block_node)",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor.visit",
      "name": "CompleteExtractor.visit",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 406,
      "end_line": 415,
      "role": "Service",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "node"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit(node):",
      "body_source": "        def visit(node):\n            if node.type == \"assignment\":\n                for child in node.children:\n                    if child.type == \"attribute\":\n                        text = child.text.decode()\n                        if text.startswith(\"self.\"):\n                            instance_vars.append(text[5:])  # Remove \"self.\"\n                        break\n            for child in node.children:\n                visit(child)",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._parse_parameters",
      "name": "CompleteExtractor._parse_parameters",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 418,
      "end_line": 442,
      "role": "Query",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "params_node"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Parse function parameters.",
      "signature": "def _parse_parameters(self, params_node) -> List[Dict]:",
      "body_source": "    def _parse_parameters(self, params_node) -> List[Dict]:\n        \"\"\"Parse function parameters.\"\"\"\n        params = []\n        for child in params_node.children:\n            if child.type == \"identifier\":\n                params.append({\"name\": child.text.decode(), \"type\": None, \"default\": None})\n            elif child.type == \"typed_parameter\":\n                name = \"\"\n                type_ann = None\n                for c in child.children:\n                    if c.type == \"identifier\":\n                        name = c.text.decode()\n                    elif c.type == \"type\":\n                        type_ann = c.text.decode()\n                params.append({\"name\": name, \"type\": type_ann, \"default\": None})\n            elif child.type == \"default_parameter\":\n                name = \"\"\n                default = None\n                for c in child.children:\n                    if c.type == \"identifier\":\n                        name = c.text.decode()\n                    else:\n                        default = c.text.decode()\n                params.append({\"name\": name, \"type\": None, \"default\": default})\n        return params",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._get_callee",
      "name": "CompleteExtractor._get_callee",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 444,
      "end_line": 458,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "call_node"
        }
      ],
      "return_type": "Optional[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the name of the function being called (Polyglot).",
      "signature": "def _get_callee(self, call_node) -> Optional[str]:",
      "body_source": "    def _get_callee(self, call_node) -> Optional[str]:\n        \"\"\"Get the name of the function being called (Polyglot).\"\"\"\n        # Try finding the function name directly\n        for child in call_node.children:\n            # Simple calls: func()\n            if child.type == \"identifier\":\n                return child.text.decode()\n            \n            # Member calls: obj.method()\n            if child.type in [\"attribute\", \"field_expression\", \"selector_expression\", \"member_expression\", \"field_access\"]:\n                 # Usually the method name is the last part/child of this expression\n                 # But simplistic approach: just return the whole text (obj.method)\n                 return child.text.decode()\n                 \n        return None",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor._extract_literals",
      "name": "CompleteExtractor._extract_literals",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 460,
      "end_line": 478,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "codebase",
          "type": "CompleteCodebase"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract all literal values from a file.",
      "signature": "def _extract_literals(self, node, codebase: CompleteCodebase, file_path: str):",
      "body_source": "    def _extract_literals(self, node, codebase: CompleteCodebase, file_path: str):\n        \"\"\"Extract all literal values from a file.\"\"\"\n        if node.type == \"string\":\n            val = node.text.decode()\n            if len(val) > 3:  # Skip empty strings\n                codebase.string_literals[file_path].append(val[:100])  # Limit length\n        elif node.type == \"integer\":\n            try:\n                codebase.numeric_constants[file_path].append(int(node.text.decode()))\n            except:\n                pass\n        elif node.type == \"float\":\n            try:\n                codebase.numeric_constants[file_path].append(float(node.text.decode()))\n            except:\n                pass\n        \n        for child in node.children:\n            self._extract_literals(child, codebase, file_path)",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py:CompleteExtractor.export_json",
      "name": "CompleteExtractor.export_json",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/complete_extractor.py",
      "start_line": 480,
      "end_line": 522,
      "role": "Command",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "codebase",
          "type": "CompleteCodebase"
        },
        {
          "name": "path",
          "type": "str"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Export complete codebase to JSON.",
      "signature": "def export_json(self, codebase: CompleteCodebase, path: str):",
      "body_source": "    def export_json(self, codebase: CompleteCodebase, path: str):\n        \"\"\"Export complete codebase to JSON.\"\"\"\n        data = {\n            \"stats\": codebase.get_stats(),\n            \"files\": list(codebase.files.keys()),\n            \"functions\": {\n                fid: {\n                    \"id\": f.id,\n                    \"name\": f.name,\n                    \"file\": f.file,\n                    \"start_line\": f.start_line,\n                    \"end_line\": f.end_line,\n                    \"source_code\": f.source_code,\n                    \"parameters\": f.parameters,\n                    \"return_type\": f.return_type,\n                    \"docstring\": f.docstring,\n                    \"local_vars\": f.local_vars,\n                    \"calls\": f.calls,\n                    \"is_async\": f.is_async,\n                    \"ast_hash\": f.ast_hash,\n                } for fid, f in codebase.functions.items()\n            },\n            \"classes\": {\n                cid: {\n                    \"id\": c.id,\n                    \"name\": c.name,\n                    \"file\": c.file,\n                    \"start_line\": c.start_line,\n                    \"end_line\": c.end_line,\n                    \"source_code\": c.source_code,\n                    \"bases\": c.bases,\n                    \"docstring\": c.docstring,\n                    \"methods\": c.methods,\n                    \"class_vars\": c.class_vars,\n                    \"instance_vars\": c.instance_vars,\n                    \"dunder_methods\": c.dunder_methods,\n                } for cid, c in codebase.classes.items()\n            },\n            \"imports\": codebase.imports,\n        }\n        \n        with open(path, 'w') as f:\n            json.dump(data, f, indent=2)",
      "complexity": 0,
      "lines_of_code": 42,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityIssue",
      "name": "PurityIssue",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 16,
      "end_line": 16,
      "role": "DTO",
      "role_confidence": 70.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PurityIssue:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector",
      "name": "PurityDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 26,
      "end_line": 26,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class PurityDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector.__init__",
      "name": "PurityDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 55,
      "end_line": 56,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.available = self._check_availability()",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._check_availability",
      "name": "PurityDetector._check_availability",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 58,
      "end_line": 69,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if bandit is available.",
      "signature": "def _check_availability(self) -> bool:",
      "body_source": "    def _check_availability(self) -> bool:\n        \"\"\"Check if bandit is available.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"bandit\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=5\n            )\n            return result.returncode == 0\n        except (FileNotFoundError, subprocess.TimeoutExpired):\n            return False",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector.analyze",
      "name": "PurityDetector.analyze",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 71,
      "end_line": 112,
      "role": "Analyzer",
      "role_confidence": 88.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a repository for purity violations.\n\nReturns:\n    {\n        \"available\": bool,\n        \"issues\": List[PurityIssue],\n        \"purity_map\": Dict[str, bool]  # file -> is_pure\n    }",
      "signature": "def analyze(self, repo_path: str) -> Dict:",
      "body_source": "    def analyze(self, repo_path: str) -> Dict:\n        \"\"\"\n        Analyze a repository for purity violations.\n        \n        Returns:\n            {\n                \"available\": bool,\n                \"issues\": List[PurityIssue],\n                \"purity_map\": Dict[str, bool]  # file -> is_pure\n            }\n        \"\"\"\n        if not self.available:\n            return self._fallback_analysis(repo_path)\n        \n        try:\n            # Run bandit in JSON mode\n            output_file = Path(repo_path) / \".bandit_output.json\"\n            \n            subprocess.run(\n                [\"bandit\", \"-r\", repo_path, \"-f\", \"json\", \"-o\", str(output_file)],\n                capture_output=True,\n                timeout=60\n            )\n            \n            if output_file.exists():\n                with open(output_file) as f:\n                    bandit_data = json.load(f)\n                \n                output_file.unlink()  # Clean up\n                \n                issues = self._parse_bandit_output(bandit_data)\n                purity_map = self._build_purity_map(repo_path, issues)\n                \n                return {\n                    \"available\": True,\n                    \"issues\": issues,\n                    \"purity_map\": purity_map\n                }\n        \n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Purity detection failed: {e}\")\n            return self._fallback_analysis(repo_path)",
      "complexity": 0,
      "lines_of_code": 41,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._parse_bandit_output",
      "name": "PurityDetector._parse_bandit_output",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 114,
      "end_line": 134,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "data",
          "type": "Dict"
        }
      ],
      "return_type": "List[PurityIssue]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Parse bandit JSON output into structured issues.",
      "signature": "def _parse_bandit_output(self, data: Dict) -> List[PurityIssue]:",
      "body_source": "    def _parse_bandit_output(self, data: Dict) -> List[PurityIssue]:\n        \"\"\"Parse bandit JSON output into structured issues.\"\"\"\n        issues = []\n        \n        for result in data.get(\"results\", []):\n            test_id = result.get(\"test_id\", \"\")\n            \n            # Only keep side-effect related tests\n            if test_id in self.SIDE_EFFECT_TESTS:\n                issue_type = self._categorize_issue(test_id, result.get(\"issue_text\", \"\"))\n                \n                issues.append(PurityIssue(\n                    file_path=result.get(\"filename\", \"\"),\n                    line_number=result.get(\"line_number\", 0),\n                    issue_type=issue_type,\n                    severity=result.get(\"issue_severity\", \"\").lower(),\n                    confidence=result.get(\"issue_confidence\", \"\").lower(),\n                    description=result.get(\"issue_text\", \"\")\n                ))\n        \n        return issues",
      "complexity": 0,
      "lines_of_code": 20,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._categorize_issue",
      "name": "PurityDetector._categorize_issue",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 136,
      "end_line": 149,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "test_id",
          "type": "str"
        },
        {
          "name": "text",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Categorize the type of side effect.",
      "signature": "def _categorize_issue(self, test_id: str, text: str) -> str:",
      "body_source": "    def _categorize_issue(self, test_id: str, text: str) -> str:\n        \"\"\"Categorize the type of side effect.\"\"\"\n        text_lower = text.lower()\n        \n        if any(x in text_lower for x in [\"file\", \"open\", \"read\", \"write\"]):\n            return \"file_io\"\n        elif any(x in text_lower for x in [\"network\", \"socket\", \"url\", \"http\", \"ftp\"]):\n            return \"network\"\n        elif \"exec\" in text_lower or \"eval\" in text_lower:\n            return \"eval\"\n        elif \"subprocess\" in text_lower or \"process\" in text_lower:\n            return \"process_io\"\n        else:\n            return \"mutation\"",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._build_purity_map",
      "name": "PurityDetector._build_purity_map",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 151,
      "end_line": 171,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        },
        {
          "name": "issues",
          "type": "List[PurityIssue]"
        }
      ],
      "return_type": "Dict[str, bool]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build a map of files to purity status.\n\nReturns: {file_path: is_pure}",
      "signature": "def _build_purity_map(self, repo_path: str, issues: List[PurityIssue]) -> Dict[str, bool]:",
      "body_source": "    def _build_purity_map(self, repo_path: str, issues: List[PurityIssue]) -> Dict[str, bool]:\n        \"\"\"\n        Build a map of files to purity status.\n        \n        Returns: {file_path: is_pure}\n        \"\"\"\n        # Files with issues are impure\n        impure_files = {issue.file_path for issue in issues}\n        \n        # Map all Python files\n        purity_map = {}\n        repo = Path(repo_path)\n        \n        for py_file in repo.rglob(\"*.py\"):\n            if any(x in str(py_file) for x in [\"__pycache__\", \".venv\", \"venv\"]):\n                continue\n            \n            rel_path = str(py_file.relative_to(repo))\n            purity_map[rel_path] = rel_path not in impure_files\n        \n        return purity_map",
      "complexity": 0,
      "lines_of_code": 20,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._fallback_analysis",
      "name": "PurityDetector._fallback_analysis",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 173,
      "end_line": 199,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Heuristic-based purity detection (when bandit unavailable).",
      "signature": "def _fallback_analysis(self, repo_path: str) -> Dict:",
      "body_source": "    def _fallback_analysis(self, repo_path: str) -> Dict:\n        \"\"\"\n        Heuristic-based purity detection (when bandit unavailable).\n        \"\"\"\n        print(\"\u26a0\ufe0f  Using heuristic purity detection (bandit not available)\")\n        \n        purity_map = {}\n        repo = Path(repo_path)\n        \n        for py_file in repo.rglob(\"*.py\"):\n            if any(x in str(py_file) for x in [\"__pycache__\", \".venv\", \"venv\"]):\n                continue\n            \n            try:\n                content = py_file.read_text()\n                is_pure = self._heuristic_check(content)\n                \n                rel_path = str(py_file.relative_to(repo))\n                purity_map[rel_path] = is_pure\n            except:\n                pass\n        \n        return {\n            \"available\": False,\n            \"issues\": [],\n            \"purity_map\": purity_map\n        }",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py:PurityDetector._heuristic_check",
      "name": "PurityDetector._heuristic_check",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/purity_detector.py",
      "start_line": 201,
      "end_line": 219,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "code",
          "type": "str"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Simple heuristic: check for common impure patterns.",
      "signature": "def _heuristic_check(self, code: str) -> bool:",
      "body_source": "    def _heuristic_check(self, code: str) -> bool:\n        \"\"\"Simple heuristic: check for common impure patterns.\"\"\"\n        impure_markers = [\n            \"open(\",\n            \"file(\",\n            \"print(\",\n            \"input(\",\n            \"requests.\",\n            \"urllib.\",\n            \"socket.\",\n            \"os.system\",\n            \"subprocess.\",\n            \"eval(\",\n            \"exec(\",\n            \"global \",\n        ]\n        \n        code_lower = code.lower()\n        return not any(marker.lower() in code_lower for marker in impure_markers)",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier",
      "name": "ParticleClassifier",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 12,
      "end_line": 12,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class ParticleClassifier:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier.__init__",
      "name": "ParticleClassifier.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 15,
      "end_line": 24,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        # Load particle definitions\n        particles_file = Path(__file__).parent.parent / 'patterns' / 'particle_defs.json'\n        with open(particles_file) as f:\n            self.particle_defs = json.load(f)\n\n        # Create lookup dict\n        self.particle_lookup = {\n            p['id']: p for p in self.particle_defs['particle_types']\n        }",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier.classify_particle",
      "name": "ParticleClassifier.classify_particle",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 26,
      "end_line": 51,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particle_data",
          "type": "Dict"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify particle with RPBL scores",
      "signature": "def classify_particle(self, particle_data: Dict) -> Dict[str, Any]:",
      "body_source": "    def classify_particle(self, particle_data: Dict) -> Dict[str, Any]:\n        \"\"\"Classify particle with RPBL scores\"\"\"\n        particle_type = particle_data.get('type', 'Unknown')\n\n        if particle_type in self.particle_lookup:\n            definition = self.particle_lookup[particle_type]\n            return {\n                **particle_data,\n                'responsibility': definition['responsibility'],\n                'purity': definition['purity'],\n                'boundary': definition['boundary'],\n                'lifecycle': definition['lifecycle'],\n                'rpbl_score': self._calculate_rpbl_score(definition),\n                'description': definition['description']\n            }\n        else:\n            # Default scores for unknown particles\n            return {\n                **particle_data,\n                'responsibility': 5,\n                'purity': 5,\n                'boundary': 5,\n                'lifecycle': 5,\n                'rpbl_score': 5.0,\n                'description': 'Unknown particle type'\n            }",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier._calculate_rpbl_score",
      "name": "ParticleClassifier._calculate_rpbl_score",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 53,
      "end_line": 61,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "definition",
          "type": "Dict"
        }
      ],
      "return_type": "float",
      "base_classes": [],
      "decorators": [],
      "docstring": "Calculate overall RPBL score",
      "signature": "def _calculate_rpbl_score(self, definition: Dict) -> float:",
      "body_source": "    def _calculate_rpbl_score(self, definition: Dict) -> float:\n        \"\"\"Calculate overall RPBL score\"\"\"\n        scores = [\n            definition['responsibility'],\n            definition['purity'],\n            definition['boundary'],\n            definition['lifecycle']\n        ]\n        return sum(scores) / len(scores)",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier.get_all_particle_types",
      "name": "ParticleClassifier.get_all_particle_types",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 63,
      "end_line": 65,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[Dict]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all defined particle types",
      "signature": "def get_all_particle_types(self) -> List[Dict]:",
      "body_source": "    def get_all_particle_types(self) -> List[Dict]:\n        \"\"\"Get all defined particle types\"\"\"\n        return self.particle_defs['particle_types']",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py:ParticleClassifier.analyze_particle_distribution",
      "name": "ParticleClassifier.analyze_particle_distribution",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/particle_classifier.py",
      "start_line": 67,
      "end_line": 91,
      "role": "Analyzer",
      "role_confidence": 83.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        }
      ],
      "return_type": "Dict[str, Any]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze distribution of detected particles",
      "signature": "def analyze_particle_distribution(self, particles: List[Dict]) -> Dict[str, Any]:",
      "body_source": "    def analyze_particle_distribution(self, particles: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Analyze distribution of detected particles\"\"\"\n        type_counts = {}\n        rpbl_scores = []\n\n        for particle in particles:\n            particle_type = particle.get('type', 'Unknown')\n            type_counts[particle_type] = type_counts.get(particle_type, 0) + 1\n\n            if 'rpbl_score' in particle:\n                rpbl_scores.append(particle['rpbl_score'])\n\n        # Calculate statistics\n        total_particles = len(particles)\n        unique_types = len(type_counts)\n        avg_rpbl = sum(rpbl_scores) / len(rpbl_scores) if rpbl_scores else 0\n\n        return {\n            'total_particles': total_particles,\n            'unique_types': unique_types,\n            'type_distribution': type_counts,\n            'average_rpbl_score': avg_rpbl,\n            'highest_rpbl': max(rpbl_scores) if rpbl_scores else 0,\n            'lowest_rpbl': min(rpbl_scores) if rpbl_scores else 0\n        }",
      "complexity": 0,
      "lines_of_code": 24,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/audit_runner.py:_print_health",
      "name": "_print_health",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/audit_runner.py",
      "start_line": 25,
      "end_line": 54,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "results"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Pretty-print health results and return boolean success.",
      "signature": "def _print_health(results):",
      "body_source": "def _print_health(results):\n    \"\"\"Pretty-print health results and return boolean success.\"\"\"\n    print(\"\\n\ud83d\udd2c NEWMAN PIPELINE VALIDATION\")\n    print(\"=\" * 60)\n    print(f\"Timestamp: {datetime.now().isoformat()}\")\n    print(\"-\" * 60)\n\n    all_passed = True\n\n    for r in results:\n        status_icon = \"\u2705\"\n        if r.status == \"FAIL\":\n            status_icon = \"\u274c\"\n            all_passed = False\n        elif r.status == \"WARN\":\n            status_icon = \"\u26a0\ufe0f \"\n        elif r.status == \"SKIP\":\n            status_icon = \"\u23ed\ufe0f \"\n\n        print(f\"{status_icon} [{r.component}]\".ljust(40) + f\"{r.status} ({r.latency_ms:.1f}ms)\")\n        print(f\"    \u2514\u2500 {r.details}\")\n        if r.error:\n            print(f\"    \u274c Error: {r.error}\")\n        print()\n\n    if all_passed:\n        print(\"\u2728 HEALTH SUITE PASSED\")\n    else:\n        print(\"\ud83d\udea8 HEALTH SUITE DETECTED ISSUES\")\n    return all_passed",
      "complexity": 0,
      "lines_of_code": 29,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/audit_runner.py:run_full_audit",
      "name": "run_full_audit",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/audit_runner.py",
      "start_line": 57,
      "end_line": 92,
      "role": "Validator",
      "role_confidence": 80.0,
      "discovery_method": "structural:docstring_validator",
      "params": [
        {
          "name": "target_path",
          "type": "str",
          "default": "'.'"
        },
        {
          "name": "output_dir",
          "type": "str",
          "default": "'output/audit'"
        }
      ],
      "return_type": "int",
      "base_classes": [],
      "decorators": [],
      "docstring": "Run health checks and a quick analysis to validate the toolchain.",
      "signature": "def run_full_audit(target_path: str = \".\", output_dir: str = \"output/audit\") -> int:",
      "body_source": "def run_full_audit(target_path: str = \".\", output_dir: str = \"output/audit\") -> int:\n    \"\"\"Run health checks and a quick analysis to validate the toolchain.\"\"\"\n    repo_path = Path(target_path).resolve()\n    print(\"\\n\ud83e\uddfe SPECTROMETER FULL AUDIT\")\n    print(\"=\" * 60)\n    print(f\"Target: {repo_path}\")\n    print(f\"Output: {output_dir}\")\n\n    if not repo_path.exists():\n        print(f\"\u274c Target path not found: {repo_path}\")\n        return 1\n\n    health_results = NewmanSuite().run_all()\n    health_ok = _print_health(health_results)\n\n    print(\"\\n\ud83e\udde0 RUNNING ANALYSIS\")\n\n    analysis_ok = True\n    try:\n        run_proof(str(repo_path), output_dir=output_dir)\n    except SystemExit as e:\n        analysis_ok = (e.code == 0)\n    except Exception as exc:  # noqa: BLE001\n        analysis_ok = False\n        print(f\"\u274c Analysis failed: {exc}\")\n\n    print(\"\\n\" + \"=\" * 60)\n    if health_ok and analysis_ok:\n        print(\"\u2705 AUDIT COMPLETE: pipeline is operational\")\n        return 0\n\n    if not health_ok:\n        print(\"\u274c Audit failed: health suite reported issues\")\n    if not analysis_ok:\n        print(\"\u274c Audit failed: analysis run did not complete cleanly\")\n    return 1",
      "complexity": 0,
      "lines_of_code": 35,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator",
      "name": "VisualizationGenerator",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 9,
      "end_line": 9,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class VisualizationGenerator:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator.__init__",
      "name": "VisualizationGenerator.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 14,
      "end_line": 18,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "template_path",
          "type": "str",
          "default": "'demos/collider_viz.html'"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, template_path: str = \"demos/collider_viz.html\"):",
      "body_source": "    def __init__(self, template_path: str = \"demos/collider_viz.html\"):\n        self.template_path = Path(template_path)\n        if not self.template_path.exists():\n            # Fallback to looking in source root if not found\n            self.template_path = Path(__file__).parent.parent / \"collider_viz.html\"",
      "complexity": 0,
      "lines_of_code": 4,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator.generate",
      "name": "VisualizationGenerator.generate",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 20,
      "end_line": 60,
      "role": "Factory",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph_path",
          "type": "str | Path"
        },
        {
          "name": "output_path",
          "type": "str | Path"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate HTML visualization from a graph.json file.",
      "signature": "def generate(self, graph_path: str | Path, output_path: str | Path):",
      "body_source": "    def generate(self, graph_path: str | Path, output_path: str | Path):\n        \"\"\"\n        Generate HTML visualization from a graph.json file.\n        \"\"\"\n        graph_path = Path(graph_path)\n        output_path = Path(output_path)\n        \n        if not graph_path.exists():\n            raise FileNotFoundError(f\"Graph file not found: {graph_path}\")\n\n        logger.info(f\"Loading graph from {graph_path}\")\n        with open(graph_path, 'r') as f:\n            graph_data = json.load(f)\n\n        # Process Data\n        particles, connections = self._process_graph(graph_data)\n        \n        node_count = len(particles)\n        edge_count = len(connections)\n        logger.info(f\"Processed {node_count} nodes and {edge_count} edges.\")\n\n        # Load Template\n        if not self.template_path.exists():\n             raise FileNotFoundError(f\"Template not found at {self.template_path}\")\n             \n        with open(self.template_path, 'r') as f:\n            html = f.read()\n\n        # Inject Data\n        html = self._inject_data(html, particles, connections)\n\n        # Optimize for Graph Size\n        html = self._apply_optimizations(html, node_count)\n\n        # Write Output\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(output_path, 'w') as f:\n            f.write(html)\n        \n        logger.info(f\"Visualization saved to {output_path}\")\n        return output_path",
      "complexity": 0,
      "lines_of_code": 40,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._process_graph",
      "name": "VisualizationGenerator._process_graph",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 62,
      "end_line": 133,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "graph_data",
          "type": "Dict[str, Any]"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convert Spectrometer graph format (UnifiedAnalysisOutput) to Vis.js format.\nNow expects 'nodes' list from CodebaseState export.",
      "signature": "def _process_graph(self, graph_data: Dict[str, Any]):",
      "body_source": "    def _process_graph(self, graph_data: Dict[str, Any]):\n        \"\"\"\n        Convert Spectrometer graph format (UnifiedAnalysisOutput) to Vis.js format.\n        Now expects 'nodes' list from CodebaseState export.\n        \"\"\"\n        nodes = graph_data.get(\"nodes\", [])\n        edges = graph_data.get(\"edges\", [])\n        \n        # Legacy fallback: 'components' dict\n        if not nodes and \"components\" in graph_data:\n            components = graph_data[\"components\"]\n            if isinstance(components, dict):\n                nodes = list(components.values())\n            elif isinstance(components, list):\n                nodes = components\n\n        # 1. Particles (Nodes)\n        particles = []\n        valid_ids = set()\n        \n        for node in nodes:\n            # Flexible ID/Name handling\n            nid = node.get(\"id\")\n            if not nid: continue\n            \n            # Use pre-calculated attributes from CodebaseState\n            layer = node.get(\"layer\", \"App\")\n            role = node.get(\"role\", \"Worker\")\n            \n            # Normalize layer case if needed\n            if hasattr(layer, \"capitalize\"):\n                layer = layer.capitalize()\n                \n            particles.append({\n                \"id\": nid,\n                \"label\": node.get(\"name\", nid[:20]),\n                \"layer\": layer,\n                \"role\": role,\n                \"file\": node.get(\"file_path\", node.get(\"file\", \"\")),\n                \"startLine\": node.get(\"start_line\", 0),\n                \"endLine\": node.get(\"end_line\", 0),\n                # Visual attributes\n                \"complexity\": node.get(\"complexity\", 1),\n                \"kind\": node.get(\"kind\", \"class\"),\n                # Pass through enrichment data\n                \"is_hotspot\": node.get(\"is_hotspot\", False),\n                \"is_orphan\": node.get(\"is_orphan\", False),\n                \"description\": node.get(\"docstring\", \"\")[:200],\n                \"intelligence\": node.get(\"metadata\", {}).get(\"intelligence\", None)\n            })\n            valid_ids.add(nid)\n\n        # 2. Connections (Edges)\n        connection_list = []\n        name_to_id = {p[\"label\"]: p[\"id\"] for p in particles}\n\n        for e in edges:\n            src = e.get(\"source\") or e.get(\"from\")\n            dst = e.get(\"target\") or e.get(\"to\")\n            \n            # Resolve IDs\n            final_src = src if src in valid_ids else name_to_id.get(src)\n            final_dst = dst if dst in valid_ids else name_to_id.get(dst)\n            \n            if final_src and final_dst:\n                connection_list.append({\n                    \"from\": final_src,\n                    \"to\": final_dst,\n                    \"type\": (e.get(\"edge_type\") or e.get(\"type\", \"CALLS\")).upper()\n                })\n\n        return particles, connection_list",
      "complexity": 0,
      "lines_of_code": 71,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._inject_data",
      "name": "VisualizationGenerator._inject_data",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 135,
      "end_line": 179,
      "role": "Entity",
      "role_confidence": 75.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "html",
          "type": "str"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        },
        {
          "name": "connections",
          "type": "List[Dict]"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Inject JSON data into the HTML template using strict markers.",
      "signature": "def _inject_data(self, html: str, particles: List[Dict], connections: List[Dict]) -> str:",
      "body_source": "    def _inject_data(self, html: str, particles: List[Dict], connections: List[Dict]) -> str:\n        \"\"\"Inject JSON data into the HTML template using strict markers.\"\"\"\n        \n        # Prepare JSON with HTML safety\n        def safe_json_dumps(obj):\n            return json.dumps(obj).replace('<', '\\\\u003c').replace('>', '\\\\u003e').replace('/', '\\\\u002f')\n            \n        particles_json = safe_json_dumps(particles)\n        connections_json = safe_json_dumps(connections)\n        \n        # Injection Block\n        injection_block = f\"\"\"\n        /* <!-- DATA_INJECTION_START --> */\n        const particles = {particles_json};\n        const connections = {connections_json};\n        \n        // IDE: Source Map & File Tree\n        const sourceMap = {self._generate_source_map(particles, None)};\n        const fileTree = {self._generate_file_tree(particles)};\n        /* <!-- DATA_INJECTION_END --> */\n        \"\"\"\n        \n        # Find Markers\n        start_marker = \"/* <!-- DATA_INJECTION_START --> */\"\n        end_marker = \"/* <!-- DATA_INJECTION_END --> */\"\n        \n        start_idx = html.find(start_marker)\n        end_idx = html.find(end_marker)\n        \n        if start_idx == -1 or end_idx == -1:\n            logger.warning(\"Injection markers not found in template! Using naive replacement.\")\n            # Fallback to simple variable replacement if markers missing (legacy support)\n            import re\n            html = re.sub(r\"const particles = \\[.*?\\];\", f\"const particles = {particles_json};\", html, flags=re.DOTALL)\n            html = re.sub(r\"const connections = \\[.*?\\];\", f\"const connections = {connections_json};\", html, flags=re.DOTALL)\n            return html\n\n        # Robust Replacement\n        # We replace everything from start_marker to end_marker (inclusive of end marker)\n        # with our new block.\n        \n        pre = html[:start_idx]\n        post = html[end_idx + len(end_marker):]\n        \n        return pre + injection_block + post",
      "complexity": 0,
      "lines_of_code": 44,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator.safe_json_dumps",
      "name": "VisualizationGenerator.safe_json_dumps",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 139,
      "end_line": 140,
      "role": "Mapper",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "obj"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def safe_json_dumps(obj):",
      "body_source": "        def safe_json_dumps(obj):\n            return json.dumps(obj).replace('<', '\\\\u003c').replace('>', '\\\\u003e').replace('/', '\\\\u002f')",
      "complexity": 0,
      "lines_of_code": 1,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._apply_optimizations",
      "name": "VisualizationGenerator._apply_optimizations",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 181,
      "end_line": 194,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "html",
          "type": "str"
        },
        {
          "name": "node_count",
          "type": "int"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Enable physics only for small graphs.",
      "signature": "def _apply_optimizations(self, html: str, node_count: int) -> str:",
      "body_source": "    def _apply_optimizations(self, html: str, node_count: int) -> str:\n        \"\"\"Enable physics only for small graphs.\"\"\"\n        \n        if node_count < 2000:\n            # Enable physics for small graphs\n            html = html.replace(\"let physicsEnabled = false;\", \"let physicsEnabled = true;\")\n            logger.info(\"Small graph detected: Enabled physics.\")\n        else:\n            logger.info(f\"Large graph ({node_count} nodes): Physics remain disabled.\")\n        \n        # Inject concept images\n        html = self._inject_concept_images(html)\n            \n        return html",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._inject_concept_images",
      "name": "VisualizationGenerator._inject_concept_images",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 196,
      "end_line": 249,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "html",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Inject concept images as base64 data URIs for tooltip display.",
      "signature": "def _inject_concept_images(self, html: str) -> str:",
      "body_source": "    def _inject_concept_images(self, html: str) -> str:\n        \"\"\"Inject concept images as base64 data URIs for tooltip display.\"\"\"\n        import base64\n        \n        # Find assets folder relative to template\n        assets_path = self.template_path.parent / \"assets\"\n        \n        concept_files = {\n            \"god_class\": \"concept_god_class.jpg\",\n            \"coupling\": \"concept_coupling.jpg\",\n            \"layer_violation\": \"concept_layer_violation.jpg\",\n            \"clustering\": \"concept_clustering.jpg\",\n            \"orphan\": \"concept_orphan.jpg\"\n        }\n        \n        concept_data = {}\n        for concept, filename in concept_files.items():\n            img_path = assets_path / filename\n            if img_path.exists():\n                with open(img_path, 'rb') as f:\n                    encoded = base64.b64encode(f.read()).decode('utf-8')\n                    concept_data[concept] = f\"data:image/jpeg;base64,{encoded}\"\n                logger.info(f\"Embedded concept image: {concept}\")\n            else:\n                concept_data[concept] = \"null\"\n                logger.warning(f\"Concept image not found: {img_path}\")\n        \n        # Build replacement block\n        replacement_block = \"// ___CONCEPT_IMAGES_START___\\n\"\n        entries = []\n        for concept, data_uri in concept_data.items():\n            if data_uri == \"null\":\n                entries.append(f'            {concept}: null')\n            else:\n                entries.append(f'            {concept}: \"{data_uri}\"')\n        replacement_block += \",\\n\".join(entries)\n        replacement_block += \"\\n            // ___CONCEPT_IMAGES_END___\"\n        \n        # Find and replace the marker section\n        start_marker = \"// ___CONCEPT_IMAGES_START___\"\n        end_marker = \"// ___CONCEPT_IMAGES_END___\"\n        \n        start_idx = html.find(start_marker)\n        end_idx = html.find(end_marker)\n        \n        if start_idx != -1 and end_idx != -1:\n            pre = html[:start_idx]\n            post = html[end_idx + len(end_marker):]\n            html = pre + replacement_block + post\n            logger.info(\"Injected concept images into template.\")\n        else:\n            logger.warning(\"Concept image markers not found in template.\")\n        \n        return html",
      "complexity": 0,
      "lines_of_code": 53,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._generate_source_map",
      "name": "VisualizationGenerator._generate_source_map",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 251,
      "end_line": 285,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        },
        {
          "name": "repo_path",
          "type": "str",
          "default": "None"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a source map for files in the graph.",
      "signature": "def _generate_source_map(self, particles: List[Dict], repo_path: str = None) -> str:",
      "body_source": "    def _generate_source_map(self, particles: List[Dict], repo_path: str = None) -> str:\n        \"\"\"Generate a source map for files in the graph.\"\"\"\n        import os\n        source_map = {}\n        \n        # Limit total source map size to avoid crashing browser (e.g. 5MB)\n        MAX_SIZE = 5 * 1024 * 1024 \n        current_size = 0\n        \n        # Collect unique file paths\n        files = {p.get(\"file\") for p in particles if p.get(\"file\")}\n        \n        for file_path in files:\n            # Resolve absolute path\n            abs_path = file_path\n            if repo_path and not file_path.startswith('/'):\n               abs_path = os.path.join(repo_path, file_path)\n               \n            if os.path.exists(abs_path) and os.path.isfile(abs_path):\n                try:\n                    with open(abs_path, 'r', encoding='utf-8', errors='ignore') as f:\n                        content = f.read()\n                        # Simple truncation for very large individual files\n                        if len(content) > 50000:\n                             content = content[:50000] + \"\\\\n... (truncated)\"\n                        \n                        source_map[file_path] = content\n                        current_size += len(content)\n                        \n                        if current_size > MAX_SIZE:\n                            break\n                except Exception as e:\n                    logger.warning(f\"Failed to read source for map: {abs_path} - {e}\")\n                    \n        return json.dumps(source_map)",
      "complexity": 0,
      "lines_of_code": 34,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator._generate_file_tree",
      "name": "VisualizationGenerator._generate_file_tree",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 287,
      "end_line": 327,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "particles",
          "type": "List[Dict]"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Generate a hierarchical file tree structure from particles.",
      "signature": "def _generate_file_tree(self, particles: List[Dict]) -> str:",
      "body_source": "    def _generate_file_tree(self, particles: List[Dict]) -> str:\n        \"\"\"Generate a hierarchical file tree structure from particles.\"\"\"\n        import json\n        tree = []\n        files = {p.get(\"file\") for p in particles if p.get(\"file\")}\n        \n        # Build tree structure\n        # Use a simple list of paths for now, frontend converts to tree?\n        # Or build a recursive structure here.\n        # Let's matching the expected 'nodes' format in renderTree:\n        # [{name, type: 'folder'|'file', path, children: []}, ...]\n        \n        def add_to_tree(path_parts, current_level, full_path):\n            if not path_parts:\n                return\n\n            name = path_parts[0]\n            is_file = len(path_parts) == 1\n            type_ = 'file' if is_file else 'folder'\n            \n            existing = next((item for item in current_level if item['name'] == name), None)\n            \n            if not existing:\n                existing = {\n                    'name': name,\n                    'type': type_,\n                    'path': full_path if is_file else None, # Only files usually have openable paths\n                    'children': []\n                }\n                current_level.append(existing)\n            \n            if not is_file:\n                add_to_tree(path_parts[1:], existing['children'], full_path)\n                \n        for f in files:\n            parts = f.split('/')\n            # Filter out empty parts\n            parts = [p for p in parts if p]\n            add_to_tree(parts, tree, f)\n            \n        return json.dumps(tree)",
      "complexity": 0,
      "lines_of_code": 40,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py:VisualizationGenerator.add_to_tree",
      "name": "VisualizationGenerator.add_to_tree",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/viz_generator.py",
      "start_line": 299,
      "end_line": 319,
      "role": "Command",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "path_parts"
        },
        {
          "name": "current_level"
        },
        {
          "name": "full_path"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def add_to_tree(path_parts, current_level, full_path):",
      "body_source": "        def add_to_tree(path_parts, current_level, full_path):\n            if not path_parts:\n                return\n\n            name = path_parts[0]\n            is_file = len(path_parts) == 1\n            type_ = 'file' if is_file else 'folder'\n            \n            existing = next((item for item in current_level if item['name'] == name), None)\n            \n            if not existing:\n                existing = {\n                    'name': name,\n                    'type': type_,\n                    'path': full_path if is_file else None, # Only files usually have openable paths\n                    'children': []\n                }\n                current_level.append(existing)\n            \n            if not is_file:\n                add_to_tree(path_parts[1:], existing['children'], full_path)",
      "complexity": 0,
      "lines_of_code": 20,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryViolation",
      "name": "BoundaryViolation",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 16,
      "end_line": 16,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class BoundaryViolation:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector",
      "name": "BoundaryDetector",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 25,
      "end_line": 25,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class BoundaryDetector:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector.__init__",
      "name": "BoundaryDetector.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 30,
      "end_line": 32,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "config_path",
          "type": "Optional[str]",
          "default": "None"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self, config_path: Optional[str] = None):",
      "body_source": "    def __init__(self, config_path: Optional[str] = None):\n        self.config_path = config_path or \".import-linter.ini\"\n        self.available = self._check_availability()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector._check_availability",
      "name": "BoundaryDetector._check_availability",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 34,
      "end_line": 45,
      "role": "Specification",
      "role_confidence": 70.0,
      "discovery_method": "structural:return_type_bool",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "bool",
      "base_classes": [],
      "decorators": [],
      "docstring": "Check if import-linter is available.",
      "signature": "def _check_availability(self) -> bool:",
      "body_source": "    def _check_availability(self) -> bool:\n        \"\"\"Check if import-linter is available.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"import-linter\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=5\n            )\n            return result.returncode == 0\n        except (FileNotFoundError, subprocess.TimeoutExpired):\n            return False",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector.analyze",
      "name": "BoundaryDetector.analyze",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 47,
      "end_line": 90,
      "role": "Analyzer",
      "role_confidence": 88.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "Analyze a repository for boundary violations.\n\nReturns:\n    {\n        \"available\": bool,\n        \"violations\": List[BoundaryViolation],\n        \"layer_map\": Dict[str, str]  # module -> layer\n    }",
      "signature": "def analyze(self, repo_path: str) -> Dict:",
      "body_source": "    def analyze(self, repo_path: str) -> Dict:\n        \"\"\"\n        Analyze a repository for boundary violations.\n        \n        Returns:\n            {\n                \"available\": bool,\n                \"violations\": List[BoundaryViolation],\n                \"layer_map\": Dict[str, str]  # module -> layer\n            }\n        \"\"\"\n        if not self.available:\n            return {\n                \"available\": False,\n                \"violations\": [],\n                \"layer_map\": {}\n            }\n        \n        # Run import-linter\n        try:\n            result = subprocess.run(\n                [\"import-linter\", \"--config\", self.config_path],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=30\n            )\n            \n            violations = self._parse_output(result.stdout)\n            layer_map = self._infer_layers(repo_path)\n            \n            return {\n                \"available\": True,\n                \"violations\": violations,\n                \"layer_map\": layer_map\n            }\n        \n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Boundary detection failed: {e}\")\n            return {\n                \"available\": False,\n                \"violations\": [],\n                \"layer_map\": {}\n            }",
      "complexity": 0,
      "lines_of_code": 43,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector._parse_output",
      "name": "BoundaryDetector._parse_output",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 92,
      "end_line": 114,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "output",
          "type": "str"
        }
      ],
      "return_type": "List[BoundaryViolation]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Parse import-linter output into structured violations.",
      "signature": "def _parse_output(self, output: str) -> List[BoundaryViolation]:",
      "body_source": "    def _parse_output(self, output: str) -> List[BoundaryViolation]:\n        \"\"\"Parse import-linter output into structured violations.\"\"\"\n        violations = []\n        \n        # import-linter output format:\n        # \"myapp.domain.entities imports myapp.infrastructure.db\"\n        \n        for line in output.split('\\n'):\n            if 'imports' in line and '->' not in line:\n                parts = line.strip().split()\n                if len(parts) >= 3:\n                    source = parts[0]\n                    target = parts[2]\n                    \n                    violations.append(BoundaryViolation(\n                        source_module=source,\n                        forbidden_module=target,\n                        contract_name=\"boundary-rule\",\n                        layer_source=self._extract_layer(source),\n                        layer_target=self._extract_layer(target)\n                    ))\n        \n        return violations",
      "complexity": 0,
      "lines_of_code": 22,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector._extract_layer",
      "name": "BoundaryDetector._extract_layer",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 116,
      "end_line": 129,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "module_path",
          "type": "str"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract architectural layer from module path.",
      "signature": "def _extract_layer(self, module_path: str) -> str:",
      "body_source": "    def _extract_layer(self, module_path: str) -> str:\n        \"\"\"Extract architectural layer from module path.\"\"\"\n        lower = module_path.lower()\n        \n        if 'domain' in lower or 'entities' in lower or 'model' in lower:\n            return \"domain\"\n        elif 'application' in lower or 'usecase' in lower or 'service' in lower:\n            return \"application\"\n        elif 'infrastructure' in lower or 'repository' in lower or 'gateway' in lower:\n            return \"infrastructure\"\n        elif 'presentation' in lower or 'api' in lower or 'controller' in lower:\n            return \"presentation\"\n        else:\n            return \"unknown\"",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py:BoundaryDetector._infer_layers",
      "name": "BoundaryDetector._infer_layers",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/boundary_detector.py",
      "start_line": 131,
      "end_line": 148,
      "role": "Factory",
      "role_confidence": 70.0,
      "discovery_method": "structural:docstring_factory",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "repo_path",
          "type": "str"
        }
      ],
      "return_type": "Dict[str, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Build a map of all Python files to their inferred layer.\n\nReturns: {file_path: layer}",
      "signature": "def _infer_layers(self, repo_path: str) -> Dict[str, str]:",
      "body_source": "    def _infer_layers(self, repo_path: str) -> Dict[str, str]:\n        \"\"\"\n        Build a map of all Python files to their inferred layer.\n        \n        Returns: {file_path: layer}\n        \"\"\"\n        layer_map = {}\n        repo = Path(repo_path)\n        \n        for py_file in repo.rglob(\"*.py\"):\n            if any(x in str(py_file) for x in [\"__pycache__\", \".venv\", \"venv\"]):\n                continue\n            \n            rel_path = str(py_file.relative_to(repo))\n            layer = self._extract_layer(rel_path)\n            layer_map[rel_path] = layer\n        \n        return layer_map",
      "complexity": 0,
      "lines_of_code": 17,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolKind",
      "name": "SymbolKind",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 12,
      "end_line": 12,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class SymbolKind(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:ClassifiedSymbol",
      "name": "ClassifiedSymbol",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 24,
      "end_line": 24,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class ClassifiedSymbol:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:ClassifiedSymbol.to_dict",
      "name": "ClassifiedSymbol.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 39,
      "end_line": 53,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def to_dict(self) -> dict:",
      "body_source": "    def to_dict(self) -> dict:\n        return {\n            \"name\": self.name,\n            \"kind\": self.kind.value,\n            \"role\": self.role,\n            \"confidence\": self.confidence,\n            \"file_path\": self.file_path,\n            \"line\": self.line,\n            \"end_line\": self.end_line,\n            \"parent\": self.parent,\n            \"evidence\": self.evidence,\n            \"decorators\": self.decorators,\n            \"base_classes\": self.base_classes,\n            \"docstring\": self.docstring,\n        }",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier",
      "name": "SymbolClassifier",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 120,
      "end_line": 120,
      "role": "DTO",
      "role_confidence": 92.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SymbolClassifier:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier.classify",
      "name": "SymbolClassifier.classify",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 199,
      "end_line": 241,
      "role": "Analyzer",
      "role_confidence": 90.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "kind",
          "type": "SymbolKind"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "line",
          "type": "int"
        },
        {
          "name": "decorators",
          "type": "List[str]",
          "default": "None"
        },
        {
          "name": "base_classes",
          "type": "List[str]",
          "default": "None"
        },
        {
          "name": "evidence",
          "type": "str",
          "default": "''"
        }
      ],
      "return_type": "ClassifiedSymbol",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a symbol based on all available evidence.",
      "signature": "def classify(",
      "body_source": "    def classify(\n        self,\n        name: str,\n        kind: SymbolKind,\n        file_path: str,\n        line: int,\n        decorators: List[str] = None,\n        base_classes: List[str] = None,\n        evidence: str = \"\",\n    ) -> ClassifiedSymbol:\n        \"\"\"Classify a symbol based on all available evidence.\"\"\"\n        decorators = decorators or []\n        base_classes = base_classes or []\n        \n        # Try classification in order of confidence\n        role, confidence = self._classify_by_inheritance(base_classes)\n        \n        if confidence < 90:\n            dec_role, dec_conf = self._classify_by_decorators(decorators)\n            if dec_conf > confidence:\n                role, confidence = dec_role, dec_conf\n        \n        if confidence < 85:\n            name_role, name_conf = self._classify_by_name(name, kind)\n            if name_conf > confidence:\n                role, confidence = name_role, name_conf\n        \n        if confidence < 70:\n            ev_role, ev_conf = self._classify_by_evidence(evidence)\n            if ev_conf > confidence:\n                role, confidence = ev_role, ev_conf\n        \n        return ClassifiedSymbol(\n            name=name,\n            kind=kind,\n            role=role,\n            confidence=confidence,\n            file_path=file_path,\n            line=line,\n            decorators=decorators,\n            base_classes=base_classes,\n            evidence=evidence,\n        )",
      "complexity": 0,
      "lines_of_code": 42,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier._classify_by_inheritance",
      "name": "SymbolClassifier._classify_by_inheritance",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 243,
      "end_line": 250,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "base_classes",
          "type": "List[str]"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by DDD base class inheritance (highest confidence).",
      "signature": "def _classify_by_inheritance(self, base_classes: List[str]) -> Tuple[str, float]:",
      "body_source": "    def _classify_by_inheritance(self, base_classes: List[str]) -> Tuple[str, float]:\n        \"\"\"Classify by DDD base class inheritance (highest confidence).\"\"\"\n        for base in base_classes:\n            # Handle full path like module.Entity -> Entity\n            simple_name = base.split('.')[-1]\n            if simple_name in DDD_BASE_CLASS_MAPPINGS:\n                return (DDD_BASE_CLASS_MAPPINGS[simple_name], 99)\n        return (\"Unknown\", 0)",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier._classify_by_decorators",
      "name": "SymbolClassifier._classify_by_decorators",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 252,
      "end_line": 265,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "decorators",
          "type": "List[str]"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by decorator patterns.",
      "signature": "def _classify_by_decorators(self, decorators: List[str]) -> Tuple[str, float]:",
      "body_source": "    def _classify_by_decorators(self, decorators: List[str]) -> Tuple[str, float]:\n        \"\"\"Classify by decorator patterns.\"\"\"\n        for dec in decorators:\n            # Handle full decorator path\n            simple_name = dec.split('.')[-1].lower()\n            if simple_name in DECORATOR_MAPPINGS:\n                return (DECORATOR_MAPPINGS[simple_name], 90)\n            \n            # Check partial matches\n            for pattern, role in DECORATOR_MAPPINGS.items():\n                if pattern in dec.lower():\n                    return (role, 85)\n        \n        return (\"Unknown\", 0)",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier._classify_by_name",
      "name": "SymbolClassifier._classify_by_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 267,
      "end_line": 293,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "kind",
          "type": "SymbolKind"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by naming conventions.",
      "signature": "def _classify_by_name(self, name: str, kind: SymbolKind) -> Tuple[str, float]:",
      "body_source": "    def _classify_by_name(self, name: str, kind: SymbolKind) -> Tuple[str, float]:\n        \"\"\"Classify by naming conventions.\"\"\"\n        name_lower = name.lower()\n        short_name = name.split('.')[-1] if '.' in name else name\n        short_lower = short_name.lower()\n        \n        # Check dunder methods\n        if short_lower in self.DUNDER_MAPPINGS:\n            return self.DUNDER_MAPPINGS[short_lower]\n        \n        # Check prefix patterns\n        for prefix, (role, conf) in self.PREFIX_PATTERNS.items():\n            if short_lower.startswith(prefix):\n                return (role, conf)\n        \n        # Check suffix patterns (case-insensitive)\n        for suffix, (role, conf) in self.SUFFIX_PATTERNS.items():\n            if short_name.endswith(suffix) or short_lower.endswith(suffix.lower()):\n                return (role, conf)\n        \n        # Default based on kind\n        if kind == SymbolKind.CLASS:\n            return (\"DTO\", 50)\n        elif kind == SymbolKind.FUNCTION:\n            return (\"Utility\", 50)\n        \n        return (\"Unknown\", 0)",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:SymbolClassifier._classify_by_evidence",
      "name": "SymbolClassifier._classify_by_evidence",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 295,
      "end_line": 314,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "evidence",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by code evidence (body patterns).",
      "signature": "def _classify_by_evidence(self, evidence: str) -> Tuple[str, float]:",
      "body_source": "    def _classify_by_evidence(self, evidence: str) -> Tuple[str, float]:\n        \"\"\"Classify by code evidence (body patterns).\"\"\"\n        if not evidence:\n            return (\"Unknown\", 0)\n        \n        evidence_lower = evidence.lower()\n        \n        # Database patterns\n        if any(x in evidence_lower for x in ['select', 'insert', 'update ', 'delete from']):\n            return (\"Repository\", 75)\n        \n        # HTTP patterns\n        if any(x in evidence_lower for x in ['request', 'response', 'http', 'json']):\n            return (\"Controller\", 70)\n        \n        # Validation patterns\n        if any(x in evidence_lower for x in ['raise', 'assert', 'error', 'exception']):\n            return (\"Validator\", 65)\n        \n        return (\"Unknown\", 0)",
      "complexity": 0,
      "lines_of_code": 19,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py:classify_symbol",
      "name": "classify_symbol",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/symbol_classifier.py",
      "start_line": 320,
      "end_line": 329,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "name",
          "type": "str"
        },
        {
          "name": "kind",
          "type": "str"
        },
        {
          "name": "file_path",
          "type": "str"
        },
        {
          "name": "line",
          "type": "int"
        },
        {
          "name": "**kwargs"
        }
      ],
      "return_type": "ClassifiedSymbol",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to classify a symbol.",
      "signature": "def classify_symbol(",
      "body_source": "def classify_symbol(\n    name: str,\n    kind: str,\n    file_path: str,\n    line: int,\n    **kwargs\n) -> ClassifiedSymbol:\n    \"\"\"Convenience function to classify a symbol.\"\"\"\n    kind_enum = SymbolKind(kind) if isinstance(kind, str) else kind\n    return _classifier.classify(name, kind_enum, file_path, line, **kwargs)",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonSymbol",
      "name": "PythonSymbol",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 13,
      "end_line": 13,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class PythonSymbol:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonSymbol.to_dict",
      "name": "PythonSymbol.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 27,
      "end_line": 39,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def to_dict(self) -> dict:",
      "body_source": "    def to_dict(self) -> dict:\n        return {\n            \"name\": self.name,\n            \"kind\": self.kind,\n            \"line\": self.line,\n            \"end_line\": self.end_line,\n            \"parent\": self.parent,\n            \"decorators\": self.decorators,\n            \"base_classes\": self.base_classes,\n            \"params\": self.params,\n            \"return_type\": self.return_type,\n            \"docstring\": self.docstring,\n        }",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser",
      "name": "PythonASTParser",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 42,
      "end_line": 42,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class PythonASTParser:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser.__init__",
      "name": "PythonASTParser.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 48,
      "end_line": 50,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self.symbols: List[PythonSymbol] = []\n        self.lines: List[str] = []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser.parse",
      "name": "PythonASTParser.parse",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 52,
      "end_line": 70,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[PythonSymbol]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Parse Python content and extract all symbols.",
      "signature": "def parse(self, content: str) -> List[PythonSymbol]:",
      "body_source": "    def parse(self, content: str) -> List[PythonSymbol]:\n        \"\"\"Parse Python content and extract all symbols.\"\"\"\n        self.symbols = []\n        self.lines = content.splitlines()\n        \n        try:\n            tree = ast.parse(content)\n        except SyntaxError:\n            return []\n        \n        # Measure depth to choose traversal strategy\n        depth = self._measure_depth(tree)\n        \n        if depth < self.MAX_RECURSIVE_DEPTH:\n            self._extract_recursive(tree)\n        else:\n            self._extract_iterative(tree)\n        \n        return self.symbols",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._measure_depth",
      "name": "PythonASTParser._measure_depth",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 72,
      "end_line": 84,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        }
      ],
      "return_type": "int",
      "base_classes": [],
      "decorators": [],
      "docstring": "Measure AST depth without recursion.",
      "signature": "def _measure_depth(self, tree: ast.AST) -> int:",
      "body_source": "    def _measure_depth(self, tree: ast.AST) -> int:\n        \"\"\"Measure AST depth without recursion.\"\"\"\n        max_depth = 0\n        stack = [(tree, 0)]\n        \n        while stack:\n            node, depth = stack.pop()\n            max_depth = max(max_depth, depth)\n            \n            for child in ast.iter_child_nodes(node):\n                stack.append((child, depth + 1))\n        \n        return max_depth",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._extract_recursive",
      "name": "PythonASTParser._extract_recursive",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 86,
      "end_line": 116,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        },
        {
          "name": "parent",
          "type": "str",
          "default": "''"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Recursive extraction for normal-depth trees.",
      "signature": "def _extract_recursive(self, tree: ast.AST, parent: str = \"\"):",
      "body_source": "    def _extract_recursive(self, tree: ast.AST, parent: str = \"\"):\n        \"\"\"Recursive extraction for normal-depth trees.\"\"\"\n        \n        class Visitor(ast.NodeVisitor):\n            def __init__(self_v, parser, parent_name):\n                self_v.parser = parser\n                self_v.parent = parent_name\n            \n            def visit_ClassDef(self_v, node: ast.ClassDef):\n                symbol = self_v.parser._extract_class(node, self_v.parent)\n                self_v.parser.symbols.append(symbol)\n                \n                # Visit methods with class as parent\n                for child in node.body:\n                    if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                        method_symbol = self_v.parser._extract_function(child, node.name)\n                        self_v.parser.symbols.append(method_symbol)\n                    elif isinstance(child, ast.ClassDef):\n                        Visitor(self_v.parser, node.name).visit(child)\n            \n            def visit_FunctionDef(self_v, node: ast.FunctionDef):\n                if not self_v.parent:  # Only top-level functions\n                    symbol = self_v.parser._extract_function(node, \"\")\n                    self_v.parser.symbols.append(symbol)\n            \n            def visit_AsyncFunctionDef(self_v, node: ast.AsyncFunctionDef):\n                if not self_v.parent:\n                    symbol = self_v.parser._extract_function(node, \"\")\n                    self_v.parser.symbols.append(symbol)\n        \n        Visitor(self, parent).visit(tree)",
      "complexity": 0,
      "lines_of_code": 30,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:Visitor",
      "name": "Visitor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 89,
      "end_line": 89,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "NodeVisitor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Visitor(ast.NodeVisitor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:Visitor.__init__",
      "name": "Visitor.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 90,
      "end_line": 92,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self_v"
        },
        {
          "name": "parser"
        },
        {
          "name": "parent_name"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self_v, parser, parent_name):",
      "body_source": "            def __init__(self_v, parser, parent_name):\n                self_v.parser = parser\n                self_v.parent = parent_name",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:Visitor.visit_ClassDef",
      "name": "Visitor.visit_ClassDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 94,
      "end_line": 104,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self_v"
        },
        {
          "name": "node",
          "type": "ast.ClassDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_ClassDef(self_v, node: ast.ClassDef):",
      "body_source": "            def visit_ClassDef(self_v, node: ast.ClassDef):\n                symbol = self_v.parser._extract_class(node, self_v.parent)\n                self_v.parser.symbols.append(symbol)\n                \n                # Visit methods with class as parent\n                for child in node.body:\n                    if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                        method_symbol = self_v.parser._extract_function(child, node.name)\n                        self_v.parser.symbols.append(method_symbol)\n                    elif isinstance(child, ast.ClassDef):\n                        Visitor(self_v.parser, node.name).visit(child)",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:Visitor.visit_FunctionDef",
      "name": "Visitor.visit_FunctionDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 106,
      "end_line": 109,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self_v"
        },
        {
          "name": "node",
          "type": "ast.FunctionDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_FunctionDef(self_v, node: ast.FunctionDef):",
      "body_source": "            def visit_FunctionDef(self_v, node: ast.FunctionDef):\n                if not self_v.parent:  # Only top-level functions\n                    symbol = self_v.parser._extract_function(node, \"\")\n                    self_v.parser.symbols.append(symbol)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:Visitor.visit_AsyncFunctionDef",
      "name": "Visitor.visit_AsyncFunctionDef",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 111,
      "end_line": 114,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self_v"
        },
        {
          "name": "node",
          "type": "ast.AsyncFunctionDef"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def visit_AsyncFunctionDef(self_v, node: ast.AsyncFunctionDef):",
      "body_source": "            def visit_AsyncFunctionDef(self_v, node: ast.AsyncFunctionDef):\n                if not self_v.parent:\n                    symbol = self_v.parser._extract_function(node, \"\")\n                    self_v.parser.symbols.append(symbol)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._extract_iterative",
      "name": "PythonASTParser._extract_iterative",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 118,
      "end_line": 144,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "tree",
          "type": "ast.AST"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Iterative extraction for deep trees (avoids recursion limits).",
      "signature": "def _extract_iterative(self, tree: ast.AST):",
      "body_source": "    def _extract_iterative(self, tree: ast.AST):\n        \"\"\"Iterative extraction for deep trees (avoids recursion limits).\"\"\"\n        stack = [(tree, \"\")]\n        \n        while stack:\n            node, parent = stack.pop()\n            \n            if isinstance(node, ast.ClassDef):\n                symbol = self._extract_class(node, parent)\n                self.symbols.append(symbol)\n                \n                # Add children with class as parent\n                for child in reversed(node.body):\n                    if isinstance(child, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                        method_symbol = self._extract_function(child, node.name)\n                        self.symbols.append(method_symbol)\n                    elif isinstance(child, ast.ClassDef):\n                        stack.append((child, node.name))\n            \n            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                if not parent:  # Only top-level\n                    symbol = self._extract_function(node, \"\")\n                    self.symbols.append(symbol)\n            \n            elif isinstance(node, ast.Module):\n                for child in reversed(node.body):\n                    stack.append((child, parent))",
      "complexity": 0,
      "lines_of_code": 26,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._extract_class",
      "name": "PythonASTParser._extract_class",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 146,
      "end_line": 157,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.ClassDef"
        },
        {
          "name": "parent",
          "type": "str"
        }
      ],
      "return_type": "PythonSymbol",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract a class symbol.",
      "signature": "def _extract_class(self, node: ast.ClassDef, parent: str) -> PythonSymbol:",
      "body_source": "    def _extract_class(self, node: ast.ClassDef, parent: str) -> PythonSymbol:\n        \"\"\"Extract a class symbol.\"\"\"\n        return PythonSymbol(\n            name=node.name,\n            kind=\"class\",\n            line=node.lineno,\n            end_line=getattr(node, 'end_lineno', node.lineno),\n            parent=parent,\n            decorators=self._get_decorators(node),\n            base_classes=self._get_base_classes(node),\n            docstring=self._get_docstring(node),\n        )",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._extract_function",
      "name": "PythonASTParser._extract_function",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 159,
      "end_line": 172,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        },
        {
          "name": "parent",
          "type": "str"
        }
      ],
      "return_type": "PythonSymbol",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract a function/method symbol.",
      "signature": "def _extract_function(self, node, parent: str) -> PythonSymbol:",
      "body_source": "    def _extract_function(self, node, parent: str) -> PythonSymbol:\n        \"\"\"Extract a function/method symbol.\"\"\"\n        return PythonSymbol(\n            name=node.name,\n            kind=\"method\" if parent else \"function\",\n            line=node.lineno,\n            end_line=getattr(node, 'end_lineno', node.lineno),\n            parent=parent,\n            decorators=self._get_decorators(node),\n            params=self._get_params(node),\n            return_type=self._get_return_type(node),\n            docstring=self._get_docstring(node),\n            body_source=self._get_body_source(node),\n        )",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_decorators",
      "name": "PythonASTParser._get_decorators",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 174,
      "end_line": 187,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract decorator names.",
      "signature": "def _get_decorators(self, node) -> List[str]:",
      "body_source": "    def _get_decorators(self, node) -> List[str]:\n        \"\"\"Extract decorator names.\"\"\"\n        decorators = []\n        for dec in node.decorator_list:\n            if isinstance(dec, ast.Name):\n                decorators.append(dec.id)\n            elif isinstance(dec, ast.Attribute):\n                decorators.append(f\"{self._get_attribute_name(dec)}\")\n            elif isinstance(dec, ast.Call):\n                if isinstance(dec.func, ast.Name):\n                    decorators.append(dec.func.id)\n                elif isinstance(dec.func, ast.Attribute):\n                    decorators.append(self._get_attribute_name(dec.func))\n        return decorators",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_attribute_name",
      "name": "PythonASTParser._get_attribute_name",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 189,
      "end_line": 198,
      "role": "Test",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_test",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.Attribute"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get full attribute name like pytest.mark.parametrize.",
      "signature": "def _get_attribute_name(self, node: ast.Attribute) -> str:",
      "body_source": "    def _get_attribute_name(self, node: ast.Attribute) -> str:\n        \"\"\"Get full attribute name like pytest.mark.parametrize.\"\"\"\n        parts = [node.attr]\n        current = node.value\n        while isinstance(current, ast.Attribute):\n            parts.append(current.attr)\n            current = current.value\n        if isinstance(current, ast.Name):\n            parts.append(current.id)\n        return '.'.join(reversed(parts))",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_base_classes",
      "name": "PythonASTParser._get_base_classes",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 200,
      "end_line": 208,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node",
          "type": "ast.ClassDef"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract base class names.",
      "signature": "def _get_base_classes(self, node: ast.ClassDef) -> List[str]:",
      "body_source": "    def _get_base_classes(self, node: ast.ClassDef) -> List[str]:\n        \"\"\"Extract base class names.\"\"\"\n        bases = []\n        for base in node.bases:\n            if isinstance(base, ast.Name):\n                bases.append(base.id)\n            elif isinstance(base, ast.Attribute):\n                bases.append(self._get_attribute_name(base))\n        return bases",
      "complexity": 0,
      "lines_of_code": 8,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_params",
      "name": "PythonASTParser._get_params",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 210,
      "end_line": 243,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "List[Dict[str, str]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract function parameters with types and defaults.",
      "signature": "def _get_params(self, node) -> List[Dict[str, str]]:",
      "body_source": "    def _get_params(self, node) -> List[Dict[str, str]]:\n        \"\"\"Extract function parameters with types and defaults.\"\"\"\n        params = []\n        args = node.args\n        \n        # Calculate defaults offset\n        defaults_offset = len(args.args) - len(args.defaults)\n        \n        for i, arg in enumerate(args.args):\n            param = {\"name\": arg.arg}\n            \n            # Type annotation\n            if arg.annotation:\n                param[\"type\"] = ast.unparse(arg.annotation) if hasattr(ast, 'unparse') else \"\"\n            \n            # Default value\n            default_idx = i - defaults_offset\n            if default_idx >= 0 and default_idx < len(args.defaults):\n                try:\n                    param[\"default\"] = ast.unparse(args.defaults[default_idx]) if hasattr(ast, 'unparse') else \"...\"\n                except:\n                    param[\"default\"] = \"...\"\n            \n            params.append(param)\n        \n        # *args\n        if args.vararg:\n            params.append({\"name\": f\"*{args.vararg.arg}\"})\n        \n        # **kwargs\n        if args.kwarg:\n            params.append({\"name\": f\"**{args.kwarg.arg}\"})\n        \n        return params",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_return_type",
      "name": "PythonASTParser._get_return_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 245,
      "end_line": 252,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract return type annotation.",
      "signature": "def _get_return_type(self, node) -> str:",
      "body_source": "    def _get_return_type(self, node) -> str:\n        \"\"\"Extract return type annotation.\"\"\"\n        if node.returns:\n            try:\n                return ast.unparse(node.returns) if hasattr(ast, 'unparse') else \"\"\n            except:\n                return \"\"\n        return \"\"",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_docstring",
      "name": "PythonASTParser._get_docstring",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 254,
      "end_line": 256,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract docstring from function or class.",
      "signature": "def _get_docstring(self, node) -> str:",
      "body_source": "    def _get_docstring(self, node) -> str:\n        \"\"\"Extract docstring from function or class.\"\"\"\n        return ast.get_docstring(node) or \"\"",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:PythonASTParser._get_body_source",
      "name": "PythonASTParser._get_body_source",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 258,
      "end_line": 268,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "node"
        }
      ],
      "return_type": "str",
      "base_classes": [],
      "decorators": [],
      "docstring": "Extract body source code.",
      "signature": "def _get_body_source(self, node) -> str:",
      "body_source": "    def _get_body_source(self, node) -> str:\n        \"\"\"Extract body source code.\"\"\"\n        if not self.lines:\n            return \"\"\n        \n        try:\n            start = node.lineno - 1\n            end = getattr(node, 'end_lineno', start + 1)\n            return '\\n'.join(self.lines[start:end])\n        except:\n            return \"\"",
      "complexity": 0,
      "lines_of_code": 10,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py:parse_python",
      "name": "parse_python",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/python_parser.py",
      "start_line": 271,
      "end_line": 274,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "structural:docstring_parser",
      "params": [
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[PythonSymbol]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to parse Python content.",
      "signature": "def parse_python(content: str) -> List[PythonSymbol]:",
      "body_source": "def parse_python(content: str) -> List[PythonSymbol]:\n    \"\"\"Convenience function to parse Python content.\"\"\"\n    parser = PythonASTParser()\n    return parser.parse(content)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:Import",
      "name": "Import",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 14,
      "end_line": 14,
      "role": "DTO",
      "role_confidence": 85.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class Import:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:Import.__post_init__",
      "name": "Import.__post_init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 21,
      "end_line": 23,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __post_init__(self):",
      "body_source": "    def __post_init__(self):\n        if self.items is None:\n            self.items = []",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:ImportExtractor",
      "name": "ImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 26,
      "end_line": 26,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ABC"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class ImportExtractor(ABC):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:ImportExtractor.extract",
      "name": "ImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 30,
      "end_line": 32,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [
        "abstractmethod"
      ],
      "docstring": "Extract imports from source content.",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        \"\"\"Extract imports from source content.\"\"\"\n        pass",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:PythonImportExtractor",
      "name": "PythonImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 35,
      "end_line": 35,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class PythonImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:PythonImportExtractor.extract",
      "name": "PythonImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 38,
      "end_line": 71,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        # Standard imports: import x, import x as y\n        for match in re.finditer(r'^import\\s+(\\S+)(?:\\s+as\\s+(\\S+))?', content, re.MULTILINE):\n            imports.append(Import(\n                module=match.group(1),\n                alias=match.group(2) or \"\"\n            ))\n        \n        # From imports: from x import y, z\n        for match in re.finditer(r'^from\\s+(\\S+)\\s+import\\s+(.+?)(?:\\s*$|\\s*#)', content, re.MULTILINE):\n            module = match.group(1)\n            is_relative = module.startswith('.')\n            items_str = match.group(2)\n            \n            # Parse items (handle multiline with backslash)\n            if '(' in items_str:\n                # Handle parenthesized imports - find closing paren\n                start = content.find(items_str)\n                end = content.find(')', start)\n                if end > start:\n                    items_str = content[start:end]\n            \n            items = [item.strip().split(' as ')[0] for item in items_str.split(',')]\n            items = [item.strip() for item in items if item.strip() and item.strip() != '(']\n            \n            imports.append(Import(\n                module=module,\n                items=items,\n                is_relative=is_relative\n            ))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 33,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:JavaScriptImportExtractor",
      "name": "JavaScriptImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 74,
      "end_line": 74,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class JavaScriptImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:JavaScriptImportExtractor.extract",
      "name": "JavaScriptImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 77,
      "end_line": 109,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        # ES6 imports: import x from 'y'\n        for match in re.finditer(\n            r'import\\s+(?:(\\w+)(?:\\s*,\\s*)?)?(?:\\{([^}]+)\\})?\\s*from\\s*[\\'\"]([^\\'\"]+)[\\'\"]',\n            content\n        ):\n            default_import = match.group(1)\n            named_imports = match.group(2)\n            module = match.group(3)\n            \n            items = []\n            if named_imports:\n                items = [item.strip().split(' as ')[0].strip() \n                        for item in named_imports.split(',')]\n            \n            imports.append(Import(\n                module=module,\n                alias=default_import or \"\",\n                items=items,\n                is_relative=module.startswith('.')\n            ))\n        \n        # CommonJS: require('x')\n        for match in re.finditer(r'require\\s*\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)', content):\n            imports.append(Import(module=match.group(1)))\n        \n        # Dynamic imports: import('x')\n        for match in re.finditer(r'import\\s*\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)', content):\n            imports.append(Import(module=match.group(1)))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 32,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:JavaImportExtractor",
      "name": "JavaImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 112,
      "end_line": 112,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class JavaImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:JavaImportExtractor.extract",
      "name": "JavaImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 115,
      "end_line": 126,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        for match in re.finditer(r'^import\\s+(static\\s+)?([a-zA-Z0-9_.]+)(?:\\.\\*)?;', content, re.MULTILINE):\n            is_static = bool(match.group(1))\n            module = match.group(2)\n            imports.append(Import(\n                module=module,\n                alias=\"static\" if is_static else \"\"\n            ))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 11,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:GoImportExtractor",
      "name": "GoImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 129,
      "end_line": 129,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class GoImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:GoImportExtractor.extract",
      "name": "GoImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 132,
      "end_line": 157,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        # Single import: import \"fmt\"\n        for match in re.finditer(r'^import\\s+\"([^\"]+)\"', content, re.MULTILINE):\n            imports.append(Import(module=match.group(1)))\n        \n        # Grouped imports: import ( ... )\n        for match in re.finditer(r'import\\s*\\((.*?)\\)', content, re.DOTALL):\n            block = match.group(1)\n            for line in block.split('\\n'):\n                line = line.strip()\n                if line and not line.startswith('//'):\n                    # Handle alias: alias \"path\"\n                    alias_match = re.match(r'(\\w+)\\s+\"([^\"]+)\"', line)\n                    if alias_match:\n                        imports.append(Import(\n                            module=alias_match.group(2),\n                            alias=alias_match.group(1)\n                        ))\n                    else:\n                        path_match = re.match(r'\"([^\"]+)\"', line)\n                        if path_match:\n                            imports.append(Import(module=path_match.group(1)))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 25,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:RustImportExtractor",
      "name": "RustImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 160,
      "end_line": 160,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class RustImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:RustImportExtractor.extract",
      "name": "RustImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 163,
      "end_line": 169,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        for match in re.finditer(r'use\\s+([a-zA-Z0-9_:{}]+);', content):\n            imports.append(Import(module=match.group(1)))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:CSharpImportExtractor",
      "name": "CSharpImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 172,
      "end_line": 172,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class CSharpImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:CSharpImportExtractor.extract",
      "name": "CSharpImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 175,
      "end_line": 181,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        for match in re.finditer(r'^using\\s+(?:static\\s+)?([a-zA-Z0-9_.]+)\\s*;', content, re.MULTILINE):\n            imports.append(Import(module=match.group(1)))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 6,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:RubyImportExtractor",
      "name": "RubyImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 184,
      "end_line": 184,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class RubyImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:RubyImportExtractor.extract",
      "name": "RubyImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 187,
      "end_line": 196,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        for match in re.finditer(r\"require\\s*['\\\"]([^'\\\"]+)['\\\"]\", content):\n            imports.append(Import(module=match.group(1)))\n        \n        for match in re.finditer(r\"require_relative\\s*['\\\"]([^'\\\"]+)['\\\"]\", content):\n            imports.append(Import(module=match.group(1), is_relative=True))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:PHPImportExtractor",
      "name": "PHPImportExtractor",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 199,
      "end_line": 199,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "ImportExtractor"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class PHPImportExtractor(ImportExtractor):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:PHPImportExtractor.extract",
      "name": "PHPImportExtractor.extract",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 202,
      "end_line": 214,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "content",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def extract(self, content: str) -> List[Import]:",
      "body_source": "    def extract(self, content: str) -> List[Import]:\n        imports = []\n        \n        for match in re.finditer(r'use\\s+([a-zA-Z0-9_\\\\]+)(?:\\s+as\\s+(\\w+))?;', content):\n            imports.append(Import(\n                module=match.group(1),\n                alias=match.group(2) or \"\"\n            ))\n        \n        for match in re.finditer(r\"(?:include|require)(?:_once)?\\s*['\\\"]([^'\\\"]+)['\\\"]\", content):\n            imports.append(Import(module=match.group(1)))\n        \n        return imports",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:get_import_extractor",
      "name": "get_import_extractor",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 232,
      "end_line": 237,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "ImportExtractor",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the appropriate import extractor for a language.",
      "signature": "def get_import_extractor(language: str) -> ImportExtractor:",
      "body_source": "def get_import_extractor(language: str) -> ImportExtractor:\n    \"\"\"Get the appropriate import extractor for a language.\"\"\"\n    extractor_class = IMPORT_EXTRACTORS.get(language.lower())\n    if extractor_class:\n        return extractor_class()\n    return PythonImportExtractor()  # Default fallback",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py:extract_imports",
      "name": "extract_imports",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/parser/import_extractor.py",
      "start_line": 240,
      "end_line": 243,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "content",
          "type": "str"
        },
        {
          "name": "language",
          "type": "str"
        }
      ],
      "return_type": "List[Import]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Convenience function to extract imports from content.",
      "signature": "def extract_imports(content: str, language: str) -> List[Import]:",
      "body_source": "def extract_imports(content: str, language: str) -> List[Import]:\n    \"\"\"Convenience function to extract imports from content.\"\"\"\n    extractor = get_import_extractor(language)\n    return extractor.extract(content)",
      "complexity": 0,
      "lines_of_code": 3,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:RolePattern",
      "name": "RolePattern",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 14,
      "end_line": 14,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class RolePattern:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository",
      "name": "PatternRepository",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 28,
      "end_line": 28,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class PatternRepository:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.__init__",
      "name": "PatternRepository.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 36,
      "end_line": 48,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self._prefix_patterns: Dict[str, Tuple[str, float]] = {}\n        self._suffix_patterns: Dict[str, Tuple[str, float]] = {}\n        self._path_patterns: Dict[str, Tuple[str, float]] = {}\n        self._dunder_patterns: Dict[str, Tuple[str, float]] = {}\n        self._decorator_patterns: Dict[str, str] = {}\n        self._inheritance_patterns: Dict[str, str] = {}\n        \n        # Try to load from canonical file first\n        if CANONICAL_PATTERNS_PATH.exists():\n            self._load_from_canonical()\n        else:\n            self._load_default_patterns()",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository._load_from_canonical",
      "name": "PatternRepository._load_from_canonical",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 50,
      "end_line": 90,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load patterns from canonical/learned/patterns.json.",
      "signature": "def _load_from_canonical(self):",
      "body_source": "    def _load_from_canonical(self):\n        \"\"\"Load patterns from canonical/learned/patterns.json.\"\"\"\n        with open(CANONICAL_PATTERNS_PATH) as f:\n            data = json.load(f)\n        \n        # Load prefix patterns\n        for pattern, info in data.get(\"prefix_patterns\", {}).items():\n            role = info.get(\"role\", \"Unknown\")\n            confidence = info.get(\"confidence\", 75)\n            self._prefix_patterns[pattern] = (role, confidence)\n        \n        # Load suffix patterns\n        for pattern, info in data.get(\"suffix_patterns\", {}).items():\n            role = info.get(\"role\", \"Unknown\")\n            confidence = info.get(\"confidence\", 75)\n            self._suffix_patterns[pattern] = (role, confidence)\n        \n        # Load path patterns\n        for pattern, info in data.get(\"path_patterns\", {}).items():\n            role = info.get(\"role\", \"Unknown\")\n            confidence = info.get(\"confidence\", 75)\n            self._path_patterns[pattern] = (role, confidence)\n        \n        # Load parameter type patterns (STRUCTURAL ANCHORS - Go framework types)\n        self._param_type_patterns: Dict[str, Tuple[str, float]] = {}\n        for pattern, info in data.get(\"parameter_type_patterns\", {}).items():\n            role = info.get(\"role\", \"Unknown\")\n            confidence = info.get(\"confidence\", 90)\n            self._param_type_patterns[pattern] = (role, confidence)\n        \n        # Load import patterns (STRUCTURAL ANCHORS - JS/TS imports)\n        self._import_patterns: Dict[str, Tuple[str, float]] = {}\n        for pattern, info in data.get(\"import_patterns\", {}).items():\n            role = info.get(\"role\", \"Unknown\")\n            confidence = info.get(\"confidence\", 85)\n            self._import_patterns[pattern] = (role, confidence)\n        \n        # Still need to load dunder/decorator/inheritance from defaults\n        self._load_dunder_patterns()\n        self._load_decorator_patterns()\n        self._load_inheritance_patterns()",
      "complexity": 0,
      "lines_of_code": 40,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository._load_dunder_patterns",
      "name": "PatternRepository._load_dunder_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 92,
      "end_line": 110,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load Python dunder method patterns.",
      "signature": "def _load_dunder_patterns(self):",
      "body_source": "    def _load_dunder_patterns(self):\n        \"\"\"Load Python dunder method patterns.\"\"\"\n        self._dunder_patterns = {\n            '__init__': ('Lifecycle', 95),\n            '__new__': ('Factory', 95),\n            '__del__': ('Lifecycle', 95),\n            '__str__': ('Utility', 90),\n            '__repr__': ('Utility', 90),\n            '__eq__': ('Specification', 90),\n            '__hash__': ('Utility', 90),\n            '__len__': ('Query', 90),\n            '__iter__': ('Iterator', 90),\n            '__next__': ('Iterator', 90),\n            '__getitem__': ('Query', 90),\n            '__setitem__': ('Command', 90),\n            '__call__': ('Command', 90),\n            '__enter__': ('Lifecycle', 90),\n            '__exit__': ('Lifecycle', 90),\n        }",
      "complexity": 0,
      "lines_of_code": 18,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository._load_decorator_patterns",
      "name": "PatternRepository._load_decorator_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 112,
      "end_line": 127,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load decorator patterns.",
      "signature": "def _load_decorator_patterns(self):",
      "body_source": "    def _load_decorator_patterns(self):\n        \"\"\"Load decorator patterns.\"\"\"\n        self._decorator_patterns = {\n            'staticmethod': 'Utility',\n            'classmethod': 'Factory',\n            'property': 'Query',\n            'abstractmethod': 'Specification',\n            'pytest.fixture': 'Fixture',\n            'fixture': 'Fixture',\n            'get': 'Controller',\n            'post': 'Controller',\n            'put': 'Controller',\n            'delete': 'Controller',\n            'route': 'Controller',\n            'test': 'Test',\n        }",
      "complexity": 0,
      "lines_of_code": 15,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository._load_inheritance_patterns",
      "name": "PatternRepository._load_inheritance_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 129,
      "end_line": 141,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load DDD inheritance patterns.",
      "signature": "def _load_inheritance_patterns(self):",
      "body_source": "    def _load_inheritance_patterns(self):\n        \"\"\"Load DDD inheritance patterns.\"\"\"\n        self._inheritance_patterns = {\n            'Entity': 'Entity',\n            'BaseEntity': 'Entity',\n            'ValueObject': 'ValueObject',\n            'AggregateRoot': 'AggregateRoot',\n            'Repository': 'Repository',\n            'BaseRepository': 'Repository',\n            'Command': 'Command',\n            'Query': 'Query',\n            'BaseSettings': 'Configuration',\n        }",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository._load_default_patterns",
      "name": "PatternRepository._load_default_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 143,
      "end_line": 396,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load all default patterns (fallback if JSON not found).",
      "signature": "def _load_default_patterns(self):",
      "body_source": "    def _load_default_patterns(self):\n        \"\"\"Load all default patterns (fallback if JSON not found).\"\"\"\n        # Prefix patterns (function names)\n        self._prefix_patterns = {\n            # Test patterns\n            'test_': ('Test', 90),\n            'test': ('Test', 85),  # Java: testUserLogin\n            'should': ('Test', 85),  # BDD: shouldReturnUser\n            'given': ('Test', 80),\n            'when': ('Test', 80),\n            'then': ('Test', 80),\n            \n            # Query patterns\n            'get_': ('Query', 95), # LEARNED: Boosted from 85\n            'fetch_': ('Query', 85),\n            'find_': ('Query', 85),\n            'load_': ('Query', 85),\n            'read_': ('Query', 85),\n            'search_': ('Query', 85),\n            'query_': ('Query', 85),\n            'list_': ('Query', 85),\n            \n            # Command patterns\n            'set_': ('Command', 85),\n            'update_': ('Command', 85),\n            'delete_': ('Command', 85),\n            'remove_': ('Command', 85),\n            'add_': ('Command', 85),\n            'save_': ('Command', 85),\n            'login': ('Command', 85),  # LEARNED: Tier 1 adjudication\n            'insert_': ('Command', 85),\n            'create_': ('Factory', 95),  # LEARNED: Boosted from 85\n            'do_': ('Command', 80),\n            'run_': ('Command', 80),\n            'execute_': ('Service', 60),  \n            'process_': ('Service', 60),  \n            \n            # Factory patterns\n            'build_': ('Factory', 85),\n            'make_': ('Factory', 90),     # LEARNED\n            'from_': ('Factory', 80),\n            \n            # Specification patterns\n            'is_': ('Specification', 85),\n            'has_': ('Specification', 85),\n            'can_': ('Specification', 85),\n            'should_': ('Specification', 85),\n            'validate_': ('Validator', 85),\n            'check_': ('Validator', 85),\n            \n            # Event patterns\n            'handle_': ('EventHandler', 85),\n            'on_': ('EventHandler', 85),\n            'watch_': ('Observer', 80),  # INFERRED (Tier 2)\n            '_should_': ('Specification', 85),  # INFERRED (Tier 3)\n            \n            # Mapper patterns\n            'convert_': ('Transformer', 90), # LEARNED (was Mapper)\n            'transform_': ('Transformer', 90), # LEARNED\n            'to_': ('Transformer', 85),\n            'as_': ('Transformer', 80),\n            'parse_': ('Utility', 80),\n            'format_': ('Utility', 80),\n            \n            # Lifecycle patterns\n            'init_': ('Lifecycle', 80),\n            'setup_': ('Lifecycle', 80),\n            'teardown_': ('Lifecycle', 80),\n            'cleanup_': ('Lifecycle', 80),\n            \n            # Internal\n            '_': ('Internal', 70),\n            \n            # Go/TypeScript specific\n            'Test': ('Test', 85),  # Go: TestUserLogin\n            'Benchmark': ('Test', 80),\n            'Example': ('Test', 75),\n            'describe': ('Test', 85),\n            'it': ('Test', 85),\n            'beforeEach': ('Fixture', 85),\n            'afterEach': ('Fixture', 85),\n        }\n        \n        # Suffix patterns (class names)\n        self._suffix_patterns = {\n            # Architecture roles\n            'Service': ('Service', 90),\n            'Repository': ('Repository', 90),\n            'Controller': ('Controller', 90),\n            'Handler': ('EventHandler', 85),\n            'Listener': ('EventHandler', 85),\n            \n            # Creation patterns\n            'Factory': ('Factory', 85),\n            'Builder': ('Builder', 85),\n            'Provider': ('Provider', 85),\n            \n            # Data patterns\n            'Mixin': ('Adapter', 85),  # INFERRED (Tier 2)\n            'Config': ('Configuration', 90),  # INFERRED (Tier 2)\n            'Configuration': ('Configuration', 90),\n            \n            # Testing patterns (Tier 2)\n            'Mock': ('TestDouble', 95),  # INFERRED\n            'Mocker': ('TestDouble', 95),  # INFERRED\n            'Stub': ('TestDouble', 90),  # INFERRED\n            'Fake': ('TestDouble', 90),\n            \n            # DDD patterns (Tier 2)\n            'Specifier': ('Specification', 80),  # INFERRED\n            \n            # Data patterns cont'd\n            'Mapper': ('Mapper', 85),\n            'Converter': ('Mapper', 80),\n            'Transformer': ('Mapper', 80),\n            'Serializer': ('Mapper', 80),\n            'DTO': ('DTO', 85),\n            'Entity': ('Entity', 85),\n            \n            # Validation patterns\n            'Validator': ('Validator', 85),\n            'Specification': ('Specification', 85),\n            \n            # Domain patterns\n            'Command': ('Command', 85),\n            'Query': ('Query', 85),\n            'Event': ('DomainEvent', 85),\n            'Policy': ('Policy', 85),\n            \n            # Infrastructure\n            'Adapter': ('Adapter', 85),\n            'Gateway': ('Gateway', 85),\n            'Client': ('Client', 85),\n            'Impl': ('RepositoryImpl', 80),\n            \n            # Configuration\n            'Config': ('Configuration', 80),\n            'Settings': ('Configuration', 80),\n            'Options': ('Configuration', 75),\n            \n            # Testing\n            'Test': ('Test', 85),\n            'Mock': ('Fixture', 80),\n            'Stub': ('Fixture', 80),\n            'Fake': ('Fixture', 80),\n            \n            # Errors\n            'Exception': ('Exception', 85),\n            'Error': ('Exception', 80),\n            \n            # Utilities\n            'Helper': ('Utility', 75),\n            'Utils': ('Utility', 75),\n            'Util': ('Utility', 75),\n            'Manager': ('Service', 75),\n            'Processor': ('Service', 75),\n            'Middleware': ('Service', 80),\n        }\n        \n        # Dunder methods (Python magic methods)\n        self._dunder_patterns = {\n            '__init__': ('Lifecycle', 95),\n            '__new__': ('Factory', 95),\n            '__del__': ('Lifecycle', 95),\n            '__str__': ('Utility', 90),\n            '__repr__': ('Utility', 90),\n            '__eq__': ('Specification', 90),\n            '__ne__': ('Specification', 90),\n            '__lt__': ('Specification', 90),\n            '__le__': ('Specification', 90),\n            '__gt__': ('Specification', 90),\n            '__ge__': ('Specification', 90),\n            '__hash__': ('Utility', 90),\n            '__bool__': ('Specification', 90),\n            '__len__': ('Query', 90),\n            '__iter__': ('Iterator', 90),\n            '__next__': ('Iterator', 90),\n            '__getitem__': ('Query', 90),\n            '__setitem__': ('Command', 90),\n            '__delitem__': ('Command', 90),\n            '__contains__': ('Specification', 90),\n            '__call__': ('Command', 90),\n            '__enter__': ('Lifecycle', 90),\n            '__exit__': ('Lifecycle', 90),\n            '__getattr__': ('Query', 85),\n            '__setattr__': ('Command', 85),\n        }\n        \n        # Decorator patterns\n        self._decorator_patterns = {\n            # Python decorators\n            'staticmethod': 'Utility',\n            'classmethod': 'Factory',\n            'property': 'Query',\n            'abstractmethod': 'Specification',\n            'pytest.fixture': 'Fixture',\n            'fixture': 'Fixture',\n            \n            # Web framework decorators\n            'get': 'Controller',\n            'post': 'Controller',\n            'put': 'Controller',\n            'delete': 'Controller',\n            'patch': 'Controller',\n            'route': 'Controller',\n            'api': 'Controller',\n            'app.route': 'Controller',\n            \n            # Django\n            'login_required': 'Controller',\n            'permission_required': 'Policy',\n            \n            # Testing\n            'test': 'Test',\n            'pytest.mark': 'Test',\n            'mock': 'Fixture',\n            'patch': 'Fixture',\n            \n            # Java/Spring\n            '@Service': 'Service',\n            '@Repository': 'Repository',\n            '@Controller': 'Controller',\n            '@RestController': 'Controller',\n            '@Component': 'Service',\n            '@Test': 'Test',\n            \n            # NestJS/Angular\n            '@Injectable': 'Service',\n            '@Component': 'Controller',\n            '@Pipe': 'Mapper',\n            '@Directive': 'Service',\n        }\n        \n        # DDD inheritance patterns (99% confidence)\n        self._inheritance_patterns = {\n            'BaseEntity': 'Entity',\n            'Entity': 'Entity',\n            'AbstractEntity': 'Entity',\n            'ValueObject': 'ValueObject',\n            'BaseValueObject': 'ValueObject',\n            'AggregateRoot': 'AggregateRoot',\n            'BaseAggregateRoot': 'AggregateRoot',\n            'Repository': 'Repository',\n            'BaseRepository': 'Repository',\n            'AbstractRepository': 'Repository',\n            'Command': 'Command',\n            'BaseCommand': 'Command',\n            'Query': 'Query',\n            'BaseQuery': 'Query',\n            'DomainService': 'DomainService',\n            'ApplicationService': 'ApplicationService',\n            'BaseSettings': 'Configuration',\n            'Settings': 'Configuration',\n        }",
      "complexity": 0,
      "lines_of_code": 253,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_path_patterns",
      "name": "PatternRepository.get_path_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 402,
      "end_line": 404,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Tuple[str, float]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all path patterns.",
      "signature": "def get_path_patterns(self) -> Dict[str, Tuple[str, float]]:",
      "body_source": "    def get_path_patterns(self) -> Dict[str, Tuple[str, float]]:\n        \"\"\"Get all path patterns.\"\"\"\n        return self._path_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_prefix_patterns",
      "name": "PatternRepository.get_prefix_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 406,
      "end_line": 408,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Tuple[str, float]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all prefix patterns.",
      "signature": "def get_prefix_patterns(self) -> Dict[str, Tuple[str, float]]:",
      "body_source": "    def get_prefix_patterns(self) -> Dict[str, Tuple[str, float]]:\n        \"\"\"Get all prefix patterns.\"\"\"\n        return self._prefix_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_suffix_patterns",
      "name": "PatternRepository.get_suffix_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 410,
      "end_line": 412,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Tuple[str, float]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all suffix patterns.",
      "signature": "def get_suffix_patterns(self) -> Dict[str, Tuple[str, float]]:",
      "body_source": "    def get_suffix_patterns(self) -> Dict[str, Tuple[str, float]]:\n        \"\"\"Get all suffix patterns.\"\"\"\n        return self._suffix_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_dunder_patterns",
      "name": "PatternRepository.get_dunder_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 414,
      "end_line": 416,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, Tuple[str, float]]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all dunder method patterns.",
      "signature": "def get_dunder_patterns(self) -> Dict[str, Tuple[str, float]]:",
      "body_source": "    def get_dunder_patterns(self) -> Dict[str, Tuple[str, float]]:\n        \"\"\"Get all dunder method patterns.\"\"\"\n        return self._dunder_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_decorator_patterns",
      "name": "PatternRepository.get_decorator_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 418,
      "end_line": 420,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all decorator patterns.",
      "signature": "def get_decorator_patterns(self) -> Dict[str, str]:",
      "body_source": "    def get_decorator_patterns(self) -> Dict[str, str]:\n        \"\"\"Get all decorator patterns.\"\"\"\n        return self._decorator_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_inheritance_patterns",
      "name": "PatternRepository.get_inheritance_patterns",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 422,
      "end_line": 424,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Dict[str, str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all inheritance patterns.",
      "signature": "def get_inheritance_patterns(self) -> Dict[str, str]:",
      "body_source": "    def get_inheritance_patterns(self) -> Dict[str, str]:\n        \"\"\"Get all inheritance patterns.\"\"\"\n        return self._inheritance_patterns.copy()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.get_all_roles",
      "name": "PatternRepository.get_all_roles",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 426,
      "end_line": 439,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "Set[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all known role names.",
      "signature": "def get_all_roles(self) -> Set[str]:",
      "body_source": "    def get_all_roles(self) -> Set[str]:\n        \"\"\"Get all known role names.\"\"\"\n        roles = set()\n        for _, (role, _) in self._prefix_patterns.items():\n            roles.add(role)\n        for _, (role, _) in self._suffix_patterns.items():\n            roles.add(role)\n        for _, (role, _) in self._dunder_patterns.items():\n            roles.add(role)\n        for _, role in self._decorator_patterns.items():\n            roles.add(role)\n        for _, role in self._inheritance_patterns.items():\n            roles.add(role)\n        return roles",
      "complexity": 0,
      "lines_of_code": 13,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_prefix",
      "name": "PatternRepository.classify_by_prefix",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 441,
      "end_line": 488,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a name by prefix patterns.\n\nUses camelCase/snake_case boundary detection to avoid false positives.\nE.g., 'use' should match 'useEffect' but NOT 'UserService'.",
      "signature": "def classify_by_prefix(self, name: str) -> Tuple[str, float]:",
      "body_source": "    def classify_by_prefix(self, name: str) -> Tuple[str, float]:\n        \"\"\"Classify a name by prefix patterns.\n        \n        Uses camelCase/snake_case boundary detection to avoid false positives.\n        E.g., 'use' should match 'useEffect' but NOT 'UserService'.\n        \"\"\"\n        short_name = name.split('.')[-1] if '.' in name else name\n        short_lower = short_name.lower()\n        \n        # Sort patterns by length (longest first) to match most specific\n        sorted_patterns = sorted(self._prefix_patterns.items(), \n                                  key=lambda x: len(x[0]), reverse=True)\n        \n        for prefix, (role, conf) in sorted_patterns:\n            prefix_lower = prefix.lower()\n            prefix_len = len(prefix)\n            \n            # Check if name starts with prefix (case-insensitive)\n            if not short_lower.startswith(prefix_lower):\n                continue\n            \n            # If exact match, it's valid\n            if len(short_name) == prefix_len:\n                return (role, conf)\n            \n            # Get the character after the prefix\n            next_char = short_name[prefix_len]\n            \n            # Check for valid word boundary:\n            # 1. snake_case: prefix ends with _ (e.g., 'get_' matches 'get_user')\n            # 2. camelCase: next char is uppercase (e.g., 'use' matches 'useEffect')\n            # 3. Underscore separator (e.g., 'test' matches 'test_user')\n            \n            if prefix.endswith('_'):\n                # snake_case patterns like 'get_' - just need prefix match\n                return (role, conf)\n            elif next_char.isupper():\n                # camelCase: 'useEffect' matches 'use' + 'E'\n                # But 'UserService' should NOT match 'use' because 'u' != 'U'\n                # Check case-sensitive prefix match for camelCase\n                if short_name.startswith(prefix):\n                    return (role, conf)\n            elif next_char == '_':\n                # 'test_user' matches 'test' + '_'\n                if short_name[:prefix_len].lower() == prefix_lower:\n                    return (role, conf - 5)  # Slightly lower confidence\n        \n        return ('Unknown', 0)",
      "complexity": 0,
      "lines_of_code": 47,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_suffix",
      "name": "PatternRepository.classify_by_suffix",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 490,
      "end_line": 499,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a name by suffix patterns.",
      "signature": "def classify_by_suffix(self, name: str) -> Tuple[str, float]:",
      "body_source": "    def classify_by_suffix(self, name: str) -> Tuple[str, float]:\n        \"\"\"Classify a name by suffix patterns.\"\"\"\n        short_name = name.split('.')[-1] if '.' in name else name\n        short_lower = short_name.lower()\n        \n        for suffix, (role, conf) in self._suffix_patterns.items():\n            if short_name.endswith(suffix) or short_lower.endswith(suffix.lower()):\n                return (role, conf)\n        \n        return ('Unknown', 0)",
      "complexity": 0,
      "lines_of_code": 9,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_dunder",
      "name": "PatternRepository.classify_by_dunder",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 501,
      "end_line": 508,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "name",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify a dunder method.",
      "signature": "def classify_by_dunder(self, name: str) -> Tuple[str, float]:",
      "body_source": "    def classify_by_dunder(self, name: str) -> Tuple[str, float]:\n        \"\"\"Classify a dunder method.\"\"\"\n        short_name = name.split('.')[-1] if '.' in name else name\n        \n        if short_name in self._dunder_patterns:\n            return self._dunder_patterns[short_name]\n        \n        return ('Unknown', 0)",
      "complexity": 0,
      "lines_of_code": 7,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_param_type",
      "name": "PatternRepository.classify_by_param_type",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 515,
      "end_line": 536,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "param_types",
          "type": "List[str]"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by function parameter types (Go framework anchors).\n\nExample: gin.Context \u2192 Controller (95% confidence)",
      "signature": "def classify_by_param_type(self, param_types: List[str]) -> Tuple[str, float]:",
      "body_source": "    def classify_by_param_type(self, param_types: List[str]) -> Tuple[str, float]:\n        \"\"\"Classify by function parameter types (Go framework anchors).\n        \n        Example: gin.Context \u2192 Controller (95% confidence)\n        \"\"\"\n        if not hasattr(self, '_param_type_patterns'):\n            return ('Unknown', 0)\n        \n        for param_type in param_types:\n            # Normalize: remove leading * for pointers\n            normalized = param_type.lstrip('*').strip()\n            \n            # Try exact match first\n            if normalized in self._param_type_patterns:\n                return self._param_type_patterns[normalized]\n            \n            # Try partial match (e.g., \"c *gin.Context\" contains \"gin.Context\")\n            for pattern, (role, conf) in self._param_type_patterns.items():\n                if pattern in param_type:\n                    return (role, conf)\n        \n        return ('Unknown', 0)",
      "complexity": 0,
      "lines_of_code": 21,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_import",
      "name": "PatternRepository.classify_by_import",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 538,
      "end_line": 565,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "imports",
          "type": "List[str]"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by imports (JS/TS library anchors).\n\nExample: 'react' import \u2192 UIComponent (95% confidence)",
      "signature": "def classify_by_import(self, imports: List[str]) -> Tuple[str, float]:",
      "body_source": "    def classify_by_import(self, imports: List[str]) -> Tuple[str, float]:\n        \"\"\"Classify by imports (JS/TS library anchors).\n        \n        Example: 'react' import \u2192 UIComponent (95% confidence)\n        \"\"\"\n        if not hasattr(self, '_import_patterns'):\n            return ('Unknown', 0)\n        \n        best_match = ('Unknown', 0)\n        \n        for imp in imports:\n            # Extract package name from import path\n            package = imp.split('/')[-1] if '/' in imp else imp\n            package = package.split('/')[0]  # Handle scoped packages like @prisma/client\n            \n            # Check full import\n            if imp in self._import_patterns:\n                role, conf = self._import_patterns[imp]\n                if conf > best_match[1]:\n                    best_match = (role, conf)\n            \n            # Check package name\n            elif package in self._import_patterns:\n                role, conf = self._import_patterns[package]\n                if conf > best_match[1]:\n                    best_match = (role, conf)\n        \n        return best_match",
      "complexity": 0,
      "lines_of_code": 27,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:PatternRepository.classify_by_path",
      "name": "PatternRepository.classify_by_path",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 567,
      "end_line": 581,
      "role": "Analyzer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "file_path",
          "type": "str"
        }
      ],
      "return_type": "Tuple[str, float]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Classify by file path patterns (universal directory anchors).\n\nExample: 'src/controllers/user.go' \u2192 Controller (90% confidence)",
      "signature": "def classify_by_path(self, file_path: str) -> Tuple[str, float]:",
      "body_source": "    def classify_by_path(self, file_path: str) -> Tuple[str, float]:\n        \"\"\"Classify by file path patterns (universal directory anchors).\n        \n        Example: 'src/controllers/user.go' \u2192 Controller (90% confidence)\n        \"\"\"\n        normalized = file_path.replace('\\\\', '/').lower()\n        \n        best_match = ('Unknown', 0)\n        \n        for pattern, (role, conf) in self._path_patterns.items():\n            if pattern.lower() in normalized:\n                if conf > best_match[1]:\n                    best_match = (role, conf)\n        \n        return best_match",
      "complexity": 0,
      "lines_of_code": 14,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py:get_pattern_repository",
      "name": "get_pattern_repository",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/pattern_repository.py",
      "start_line": 587,
      "end_line": 592,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "PatternRepository",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the singleton pattern repository.",
      "signature": "def get_pattern_repository() -> PatternRepository:",
      "body_source": "def get_pattern_repository() -> PatternRepository:\n    \"\"\"Get the singleton pattern repository.\"\"\"\n    global _pattern_repository\n    if _pattern_repository is None:\n        _pattern_repository = PatternRepository()\n    return _pattern_repository",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaCategory",
      "name": "SchemaCategory",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 12,
      "end_line": 12,
      "role": "DTO",
      "role_confidence": 75.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class SchemaCategory(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:Effort",
      "name": "Effort",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 21,
      "end_line": 21,
      "role": "DTO",
      "role_confidence": 65.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [
        "Enum"
      ],
      "decorators": [],
      "docstring": "",
      "signature": "class Effort(Enum):",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:OptimizationSchema",
      "name": "OptimizationSchema",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 29,
      "end_line": 29,
      "role": "DTO",
      "role_confidence": 88.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [
        "dataclass"
      ],
      "docstring": "",
      "signature": "class OptimizationSchema:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:OptimizationSchema.to_dict",
      "name": "OptimizationSchema.to_dict",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 43,
      "end_line": 55,
      "role": "Transformer",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "dict",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def to_dict(self) -> dict:",
      "body_source": "    def to_dict(self) -> dict:\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"category\": self.category.value,\n            \"description\": self.description,\n            \"when_to_apply\": self.when_to_apply,\n            \"effort\": self.effort.value,\n            \"steps\": self.steps,\n            \"benefits\": self.benefits,\n            \"risks\": self.risks,\n            \"related_schemas\": self.related_schemas,\n        }",
      "complexity": 0,
      "lines_of_code": 12,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository",
      "name": "SchemaRepository",
      "kind": "class",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 58,
      "end_line": 58,
      "role": "DTO",
      "role_confidence": 90.0,
      "discovery_method": "structural:leaf_class",
      "params": [],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "class SchemaRepository:",
      "body_source": "",
      "complexity": 0,
      "lines_of_code": 0,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.__init__",
      "name": "SchemaRepository.__init__",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 69,
      "end_line": 71,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "",
      "signature": "def __init__(self):",
      "body_source": "    def __init__(self):\n        self._schemas: Dict[str, OptimizationSchema] = {}\n        self._load_default_schemas()",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository._load_default_schemas",
      "name": "SchemaRepository._load_default_schemas",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 73,
      "end_line": 397,
      "role": "Internal",
      "role_confidence": 70.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "",
      "base_classes": [],
      "decorators": [],
      "docstring": "Load all default optimization schemas.",
      "signature": "def _load_default_schemas(self):",
      "body_source": "    def _load_default_schemas(self):\n        \"\"\"Load all default optimization schemas.\"\"\"\n        \n        # REPOSITORY_PATTERN\n        self._schemas[\"REPOSITORY_PATTERN\"] = OptimizationSchema(\n            id=\"REPOSITORY_PATTERN\",\n            name=\"Repository Pattern\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Abstract data access behind repository interfaces\",\n            when_to_apply=\"Entities without corresponding repositories\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Create repository interface (IEntityRepository)\",\n                \"Define CRUD methods: get, save, delete, find_all\",\n                \"Implement concrete repository class\",\n                \"Inject repository into services\",\n                \"Replace direct data access with repository calls\",\n            ],\n            benefits=[\n                \"Decouples business logic from data access\",\n                \"Enables easy testing with mocks\",\n                \"Supports swapping storage implementations\",\n            ],\n            risks=[\n                \"Over-abstraction for simple cases\",\n                \"Performance overhead if misused\",\n            ],\n            related_schemas=[\"CQRS_SEPARATION\", \"DEPENDENCY_INJECTION\"],\n        )\n        \n        # TEST_COVERAGE\n        self._schemas[\"TEST_COVERAGE\"] = OptimizationSchema(\n            id=\"TEST_COVERAGE\",\n            name=\"Test Coverage\",\n            category=SchemaCategory.TESTING,\n            description=\"Add unit tests for untested components\",\n            when_to_apply=\"Test-to-logic ratio below 1:3\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Identify critical paths (commands, services)\",\n                \"Create test file following naming convention\",\n                \"Write tests for happy path first\",\n                \"Add edge cases and error scenarios\",\n                \"Aim for 80% coverage on critical paths\",\n            ],\n            benefits=[\n                \"Catches regressions early\",\n                \"Documents expected behavior\",\n                \"Enables confident refactoring\",\n            ],\n            risks=[\n                \"Brittle tests if too coupled to implementation\",\n                \"Maintenance overhead for trivial tests\",\n            ],\n            related_schemas=[\"GOD_CLASS_DECOMPOSITION\"],\n        )\n        \n        # CQRS_SEPARATION\n        self._schemas[\"CQRS_SEPARATION\"] = OptimizationSchema(\n            id=\"CQRS_SEPARATION\",\n            name=\"CQRS Separation\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Separate read (query) and write (command) operations\",\n            when_to_apply=\"Mixed read/write operations in same class\",\n            effort=Effort.HIGH,\n            steps=[\n                \"Identify query methods (no side effects, return data)\",\n                \"Identify command methods (mutate state, may return void)\",\n                \"Create QueryHandler interface\",\n                \"Create CommandHandler interface\",\n                \"Move methods to appropriate handlers\",\n                \"Update callers to use correct handler\",\n            ],\n            benefits=[\n                \"Enables read/write scaling independently\",\n                \"Clearer intent in code\",\n                \"Better separation of concerns\",\n            ],\n            risks=[\n                \"Increases complexity for simple cases\",\n                \"Eventual consistency challenges\",\n            ],\n            related_schemas=[\"EVENT_SOURCING\", \"REPOSITORY_PATTERN\"],\n        )\n        \n        # GOD_CLASS_DECOMPOSITION\n        self._schemas[\"GOD_CLASS_DECOMPOSITION\"] = OptimizationSchema(\n            id=\"GOD_CLASS_DECOMPOSITION\",\n            name=\"God Class Decomposition\",\n            category=SchemaCategory.REFACTORING,\n            description=\"Split large classes into smaller, focused components\",\n            when_to_apply=\"Classes with 20+ methods or 500+ lines\",\n            effort=Effort.HIGH,\n            steps=[\n                \"Identify cohesive method groups (by functionality)\",\n                \"Create new class for each group\",\n                \"Move methods to new classes\",\n                \"Use composition in original class\",\n                \"Update callers to use new classes\",\n            ],\n            benefits=[\n                \"Single Responsibility Principle\",\n                \"Easier to test individual components\",\n                \"Reduced cognitive load\",\n            ],\n            risks=[\n                \"May break existing interfaces\",\n                \"Increased file count\",\n            ],\n            related_schemas=[\"SERVICE_EXTRACTION\", \"STRATEGY_PATTERN\"],\n        )\n        \n        # SERVICE_EXTRACTION\n        self._schemas[\"SERVICE_EXTRACTION\"] = OptimizationSchema(\n            id=\"SERVICE_EXTRACTION\",\n            name=\"Service Extraction\",\n            category=SchemaCategory.REFACTORING,\n            description=\"Extract business logic into dedicated service classes\",\n            when_to_apply=\"Controllers with embedded business logic\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Identify business logic in controller\",\n                \"Create service class with domain methods\",\n                \"Move logic to service\",\n                \"Inject service into controller\",\n                \"Controller becomes thin orchestrator\",\n            ],\n            benefits=[\n                \"Reusable business logic\",\n                \"Testable without HTTP concerns\",\n                \"Clear separation of concerns\",\n            ],\n            risks=[\"Over-engineering simple endpoints\"],\n            related_schemas=[\"DEPENDENCY_INJECTION\"],\n        )\n        \n        # LAYER_ENFORCEMENT\n        self._schemas[\"LAYER_ENFORCEMENT\"] = OptimizationSchema(\n            id=\"LAYER_ENFORCEMENT\",\n            name=\"Layer Enforcement\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Enforce clean layer boundaries in architecture\",\n            when_to_apply=\"Cross-layer violations detected\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Define layer boundaries (presentation, application, domain, infrastructure)\",\n                \"Add lint rules or tests for layer violations\",\n                \"Refactor violating imports\",\n                \"Use interfaces for cross-layer communication\",\n            ],\n            benefits=[\n                \"Clean Architecture compliance\",\n                \"Independent layer evolution\",\n                \"Testability at each layer\",\n            ],\n            risks=[\"Too rigid for small projects\"],\n            related_schemas=[\"DEPENDENCY_INJECTION\"],\n        )\n        \n        # PURE_FUNCTION_EXTRACTION\n        self._schemas[\"PURE_FUNCTION_EXTRACTION\"] = OptimizationSchema(\n            id=\"PURE_FUNCTION_EXTRACTION\",\n            name=\"Pure Function Extraction\",\n            category=SchemaCategory.PERFORMANCE,\n            description=\"Extract pure functions for caching and parallelization\",\n            when_to_apply=\"Functions mixing pure computation with side effects\",\n            effort=Effort.LOW,\n            steps=[\n                \"Identify pure computations (no I/O, no state mutation)\",\n                \"Extract to standalone pure functions\",\n                \"Add @lru_cache or similar memoization\",\n                \"Parallelize where beneficial\",\n            ],\n            benefits=[\n                \"Easy to cache results\",\n                \"Safe for parallelization\",\n                \"Easier to test\",\n            ],\n            risks=[\"Cache invalidation complexity\"],\n            related_schemas=[],\n        )\n        \n        # EVENT_SOURCING\n        self._schemas[\"EVENT_SOURCING\"] = OptimizationSchema(\n            id=\"EVENT_SOURCING\",\n            name=\"Event Sourcing\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Store state as sequence of events\",\n            when_to_apply=\"Audit requirements or complex state history needs\",\n            effort=Effort.HIGH,\n            steps=[\n                \"Define domain events for state changes\",\n                \"Create event store\",\n                \"Implement aggregate reconstruction from events\",\n                \"Add projections for read models\",\n            ],\n            benefits=[\n                \"Complete audit trail\",\n                \"Temporal queries\",\n                \"Replay for debugging\",\n            ],\n            risks=[\n                \"Significant paradigm shift\",\n                \"Eventual consistency\",\n            ],\n            related_schemas=[\"CQRS_SEPARATION\", \"SAGA_PATTERN\"],\n        )\n        \n        # SAGA_PATTERN\n        self._schemas[\"SAGA_PATTERN\"] = OptimizationSchema(\n            id=\"SAGA_PATTERN\",\n            name=\"Saga Pattern\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Manage distributed transactions with compensating actions\",\n            when_to_apply=\"Distributed transactions across services\",\n            effort=Effort.HIGH,\n            steps=[\n                \"Identify transaction boundaries\",\n                \"Define compensating actions for each step\",\n                \"Implement saga orchestrator or choreography\",\n                \"Handle partial failures gracefully\",\n            ],\n            benefits=[\n                \"Maintains consistency across services\",\n                \"No distributed locks needed\",\n            ],\n            risks=[\n                \"Complex error handling\",\n                \"Eventually consistent\",\n            ],\n            related_schemas=[\"EVENT_SOURCING\"],\n        )\n        \n        # FACTORY_METHOD\n        self._schemas[\"FACTORY_METHOD\"] = OptimizationSchema(\n            id=\"FACTORY_METHOD\",\n            name=\"Factory Method\",\n            category=SchemaCategory.REFACTORING,\n            description=\"Centralize object creation logic\",\n            when_to_apply=\"Scattered object creation with complex setup\",\n            effort=Effort.LOW,\n            steps=[\n                \"Identify repeated object creation patterns\",\n                \"Create factory class or method\",\n                \"Move creation logic to factory\",\n                \"Replace direct instantiation with factory calls\",\n            ],\n            benefits=[\n                \"Single place for creation logic\",\n                \"Easy to change implementations\",\n                \"Testable with mock factories\",\n            ],\n            risks=[\"Over-abstraction\"],\n            related_schemas=[\"BUILDER_PATTERN\", \"DEPENDENCY_INJECTION\"],\n        )\n        \n        # STRATEGY_PATTERN\n        self._schemas[\"STRATEGY_PATTERN\"] = OptimizationSchema(\n            id=\"STRATEGY_PATTERN\",\n            name=\"Strategy Pattern\",\n            category=SchemaCategory.REFACTORING,\n            description=\"Replace conditionals with polymorphism\",\n            when_to_apply=\"Switch/if-else chains based on type or strategy\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Define strategy interface\",\n                \"Create concrete strategies for each case\",\n                \"Replace conditionals with strategy dispatch\",\n                \"Use dependency injection for strategy selection\",\n            ],\n            benefits=[\n                \"Open/Closed Principle\",\n                \"Easy to add new strategies\",\n                \"Testable strategies in isolation\",\n            ],\n            risks=[\"More classes to maintain\"],\n            related_schemas=[\"FACTORY_METHOD\"],\n        )\n        \n        # DEPENDENCY_INJECTION\n        self._schemas[\"DEPENDENCY_INJECTION\"] = OptimizationSchema(\n            id=\"DEPENDENCY_INJECTION\",\n            name=\"Dependency Injection\",\n            category=SchemaCategory.ARCHITECTURE,\n            description=\"Inject dependencies instead of hard-coding them\",\n            when_to_apply=\"Hard-coded dependencies in constructors\",\n            effort=Effort.MEDIUM,\n            steps=[\n                \"Define interface for dependency\",\n                \"Accept dependency through constructor\",\n                \"Configure DI container or manual wiring\",\n                \"Create mock implementations for testing\",\n            ],\n            benefits=[\n                \"Testable with mocks\",\n                \"Swappable implementations\",\n                \"Loose coupling\",\n            ],\n            risks=[\"DI container complexity\"],\n            related_schemas=[\"REPOSITORY_PATTERN\", \"SERVICE_EXTRACTION\"],\n        )\n        \n        # ERROR_HANDLING\n        self._schemas[\"ERROR_HANDLING\"] = OptimizationSchema(\n            id=\"ERROR_HANDLING\",\n            name=\"Error Handling\",\n            category=SchemaCategory.REFACTORING,\n            description=\"Standardize exception handling patterns\",\n            when_to_apply=\"Inconsistent exception handling across codebase\",\n            effort=Effort.LOW,\n            steps=[\n                \"Define domain-specific exception hierarchy\",\n                \"Create error codes or types\",\n                \"Implement global exception handler\",\n                \"Log errors with context\",\n                \"Return consistent error responses\",\n            ],\n            benefits=[\n                \"Consistent error messages\",\n                \"Easier debugging\",\n                \"Better user experience\",\n            ],\n            risks=[\"Over-catching exceptions\"],\n            related_schemas=[],\n        )",
      "complexity": 0,
      "lines_of_code": 324,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.get",
      "name": "SchemaRepository.get",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 403,
      "end_line": 405,
      "role": "Query",
      "role_confidence": 0.8,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "schema_id",
          "type": "str"
        }
      ],
      "return_type": "Optional[OptimizationSchema]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get a schema by ID.",
      "signature": "def get(self, schema_id: str) -> Optional[OptimizationSchema]:",
      "body_source": "    def get(self, schema_id: str) -> Optional[OptimizationSchema]:\n        \"\"\"Get a schema by ID.\"\"\"\n        return self._schemas.get(schema_id)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.get_all",
      "name": "SchemaRepository.get_all",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 407,
      "end_line": 409,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[OptimizationSchema]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get all schemas.",
      "signature": "def get_all(self) -> List[OptimizationSchema]:",
      "body_source": "    def get_all(self) -> List[OptimizationSchema]:\n        \"\"\"Get all schemas.\"\"\"\n        return list(self._schemas.values())",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.get_by_category",
      "name": "SchemaRepository.get_by_category",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 411,
      "end_line": 413,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "category",
          "type": "SchemaCategory"
        }
      ],
      "return_type": "List[OptimizationSchema]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get schemas by category.",
      "signature": "def get_by_category(self, category: SchemaCategory) -> List[OptimizationSchema]:",
      "body_source": "    def get_by_category(self, category: SchemaCategory) -> List[OptimizationSchema]:\n        \"\"\"Get schemas by category.\"\"\"\n        return [s for s in self._schemas.values() if s.category == category]",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.get_by_effort",
      "name": "SchemaRepository.get_by_effort",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 415,
      "end_line": 417,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        },
        {
          "name": "effort",
          "type": "Effort"
        }
      ],
      "return_type": "List[OptimizationSchema]",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get schemas by effort level.",
      "signature": "def get_by_effort(self, effort: Effort) -> List[OptimizationSchema]:",
      "body_source": "    def get_by_effort(self, effort: Effort) -> List[OptimizationSchema]:\n        \"\"\"Get schemas by effort level.\"\"\"\n        return [s for s in self._schemas.values() if s.effort == effort]",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.list_ids",
      "name": "SchemaRepository.list_ids",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 419,
      "end_line": 421,
      "role": "Query",
      "role_confidence": 85.0,
      "discovery_method": "pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "List[str]",
      "base_classes": [],
      "decorators": [],
      "docstring": "List all schema IDs.",
      "signature": "def list_ids(self) -> List[str]:",
      "body_source": "    def list_ids(self) -> List[str]:\n        \"\"\"List all schema IDs.\"\"\"\n        return list(self._schemas.keys())",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:SchemaRepository.count",
      "name": "SchemaRepository.count",
      "kind": "method",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 423,
      "end_line": 425,
      "role": "Query",
      "role_confidence": 75.0,
      "discovery_method": "heuristic_pattern",
      "params": [
        {
          "name": "self"
        }
      ],
      "return_type": "int",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get total number of schemas.",
      "signature": "def count(self) -> int:",
      "body_source": "    def count(self) -> int:\n        \"\"\"Get total number of schemas.\"\"\"\n        return len(self._schemas)",
      "complexity": 0,
      "lines_of_code": 2,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    },
    {
      "id": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py:get_schema_repository",
      "name": "get_schema_repository",
      "kind": "function",
      "file_path": "/Users/lech/PROJECTS_all/PROJECT_elements/standard-model-of-code/core/registry/schema_repository.py",
      "start_line": 431,
      "end_line": 436,
      "role": "Query",
      "role_confidence": 95.0,
      "discovery_method": "pattern",
      "params": [],
      "return_type": "SchemaRepository",
      "base_classes": [],
      "decorators": [],
      "docstring": "Get the singleton schema repository.",
      "signature": "def get_schema_repository() -> SchemaRepository:",
      "body_source": "def get_schema_repository() -> SchemaRepository:\n    \"\"\"Get the singleton schema repository.\"\"\"\n    global _schema_repository\n    if _schema_repository is None:\n        _schema_repository = SchemaRepository()\n    return _schema_repository",
      "complexity": 0,
      "lines_of_code": 5,
      "in_degree": 0,
      "out_degree": 0,
      "layer": null,
      "metadata": {}
    }
  ],
  "edges": [
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._tokenize_identifier",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine.analyze_file",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_particles",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._classify_class_pattern",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._classify_function_pattern",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 273,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_base_class_names",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 537,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_decorators",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 553,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_function_body",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 587,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_function_params",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 598,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_docstring",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 642,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_return_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 649,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._measure_python_ast_depth",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 659,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine.evidence_for_line",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 681,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "Visitor",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 687,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_ClassDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 688,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_FunctionDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_AsyncFunctionDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 759,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine.evidence_for_line",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 774,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_function_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 940,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_particle_type_by_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 976,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._get_function_type_by_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1054,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._detect_by_keywords",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1148,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._calculate_confidence",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1159,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_touchpoints",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1175,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_raw_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_python_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1223,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_js_ts_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1258,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_java_like_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1319,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_csharp_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1329,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_go_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1341,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_rust_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1367,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_ruby_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1377,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_php_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1395,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._fallback_analysis",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1409,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine.analyze_directory",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine",
      "target": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "CodeGraph",
      "target": "CodeGraph.add_node",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 60,
      "metadata": {}
    },
    {
      "source": "CodeGraph",
      "target": "CodeGraph.add_edge",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "CodeGraph",
      "target": "CodeGraph.get_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 95,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._init_parsers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 99,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._extract_from_file",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.get_qualified_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 168,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.extract_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 173,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._extract_calls",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 299,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._extract_callee",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 317,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._extract_data_flow",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor._track_variable_usage",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.to_mermaid",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 378,
      "metadata": {}
    },
    {
      "source": "GraphExtractor",
      "target": "GraphExtractor.export_json",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 413,
      "metadata": {}
    },
    {
      "source": "AnalyzerConfig",
      "target": "AnalyzerConfig.config_hash",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "AnalyzerConfig",
      "target": "AnalyzerConfig.save",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "AnalyzerConfig",
      "target": "AnalyzerConfig.__str__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 53,
      "metadata": {}
    },
    {
      "source": "OllamaClient",
      "target": "OllamaClient.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "OllamaClient",
      "target": "OllamaClient._check_ollama_available",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 39,
      "metadata": {}
    },
    {
      "source": "OllamaClient",
      "target": "OllamaClient._get_cache_key",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 55,
      "metadata": {}
    },
    {
      "source": "OllamaClient",
      "target": "OllamaClient.classify",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 60,
      "metadata": {}
    },
    {
      "source": "OllamaClient",
      "target": "OllamaClient.is_available",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 47,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite.analyze_repository",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._find_source_files",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._detect_language",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 208,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._analyze_file",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._analyze_class",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 279,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._generate_refactor_suggestions",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 355,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite._generate_recommendations",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 386,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite",
      "target": "GodClassDetectorLite.generate_ascii_visualization",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 437,
      "metadata": {}
    },
    {
      "source": "UnifiedAnalysisOutput",
      "target": "UnifiedAnalysisOutput.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 166,
      "metadata": {}
    },
    {
      "source": "UnifiedAnalysisOutput",
      "target": "UnifiedAnalysisOutput.save",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 170,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._init_parsers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 201,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 249,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._extract_atoms",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 283,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._extract_molecules",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 314,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._classify_class",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._classify_function",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 398,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._has_io_calls",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 448,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor._infer_organelles",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.to_json",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 518,
      "metadata": {}
    },
    {
      "source": "AtomExtractor",
      "target": "AtomExtractor.summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 19,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator.generate_comprehensive_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator._generate_summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 83,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator._generate_detailed_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator._calculate_performance",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 179,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator.save_results",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator._write_human_readable_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "StatsGenerator",
      "target": "StatsGenerator._write_particles_csv",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "PurposeField",
      "target": "PurposeField.summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 58,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 156,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector.detect_field",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector._create_nodes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector._build_hierarchy",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector._compute_composite_purposes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 236,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector._assign_layers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 260,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector",
      "target": "PurposeFieldDetector._analyze_flow",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry._init_canonical_atoms",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry._add",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 246,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.add_discovery",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 260,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.get_by_ast_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 284,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.get_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 290,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.export_canon",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "AtomRegistry",
      "target": "AtomRegistry.print_summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer",
      "target": "DependencyAnalyzer.analyze_repository",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector",
      "target": "UniversalPatternDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector",
      "target": "UniversalPatternDetector.analyze_repository",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector",
      "target": "UniversalPatternDetector.get_quick_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 101,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 28,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier._build_lookups",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier.classify_by_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier.classify_semantic_id",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier.get_atom_info",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 256,
      "metadata": {}
    },
    {
      "source": "AtomClassifier",
      "target": "AtomClassifier.list_all_atoms",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 267,
      "metadata": {}
    },
    {
      "source": "PerformanceProfile",
      "target": "PerformanceProfile.summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 161,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor.predict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._build_nodes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._classify_time_types",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 250,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._estimate_costs",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._calculate_hotspots",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 300,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._find_critical_path",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 306,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor",
      "target": "PerformancePredictor._calculate_distributions",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "TotalityReport",
      "target": "TotalityReport.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 34,
      "metadata": {}
    },
    {
      "source": "Graph",
      "target": "Graph.add_component",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "Graph",
      "target": "Graph.add_edge",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "Graph",
      "target": "Graph.to_json",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 54,
      "metadata": {}
    },
    {
      "source": "Graph",
      "target": "Graph.default_serializer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 60,
      "metadata": {}
    },
    {
      "source": "EvaluationResult",
      "target": "EvaluationResult.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 76,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._load_laws",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 79,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator.evaluate",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._evaluate_law",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._check_particle",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._get_edge_types",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._get_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 258,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._get_calls",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 262,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._get_methods",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator",
      "target": "AntimatterEvaluator._get_fields",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 270,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 34,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry._build_registry",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.get_instance",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 79,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.normalize",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 85,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.is_valid",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 100,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.get_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 104,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.get_layer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 109,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.get_layer_weight",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.all_types",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 118,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.types_in_layer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 122,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.all_layers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 126,
      "metadata": {}
    },
    {
      "source": "TypeRegistry",
      "target": "TypeRegistry.get_rpbl",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.__post_init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 124,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID._compute_hash",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.stable_id",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 136,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.to_string",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 145,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.__str__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 168,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.__repr__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.parse",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.to_llm_context",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 228,
      "metadata": {}
    },
    {
      "source": "SemanticID",
      "target": "SemanticID.similarity_vector",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 256,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 282,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.generate_ids",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.from_function",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.from_class",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.from_atom",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 515,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator",
      "target": "SemanticIDGenerator.from_particle",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix",
      "target": "SemanticMatrix.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 621,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix",
      "target": "SemanticMatrix.add",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 628,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix",
      "target": "SemanticMatrix.to_llm_context",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 640,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix",
      "target": "SemanticMatrix.get_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 664,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix",
      "target": "SemanticMatrix.export_for_embedding",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "HeuristicClassifier",
      "target": "HeuristicClassifier.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "HeuristicClassifier",
      "target": "HeuristicClassifier.classify_by_pattern",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 168,
      "metadata": {}
    },
    {
      "source": "HeuristicClassifier",
      "target": "HeuristicClassifier.get_pattern_report",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 506,
      "metadata": {}
    },
    {
      "source": "LanguageLoader",
      "target": "LanguageLoader.load_all",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 34,
      "metadata": {}
    },
    {
      "source": "LanguageLoader",
      "target": "LanguageLoader.get_supported_languages",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 85,
      "metadata": {}
    },
    {
      "source": "ReportGenerator",
      "target": "ReportGenerator.generate",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator",
      "target": "ReportGenerator._write_components_csv",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "ReportGenerator",
      "target": "ReportGenerator._build_mermaid",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator",
      "target": "ReportGenerator._build_markdown",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "InferenceRule",
      "target": "InferenceRule.__post_init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference",
      "target": "GraphTypeInference.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference",
      "target": "GraphTypeInference.build_graph_index",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference",
      "target": "GraphTypeInference.get_neighbor_types",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference",
      "target": "GraphTypeInference.infer_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference",
      "target": "GraphTypeInference.infer_all",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "FixGenerator",
      "target": "FixGenerator.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 465,
      "metadata": {}
    },
    {
      "source": "FixGenerator",
      "target": "FixGenerator.generate_fix",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "FixGenerator",
      "target": "FixGenerator.generate_all_fixes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 507,
      "metadata": {}
    },
    {
      "source": "NewmanSuite",
      "target": "NewmanSuite.run_all",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "NewmanSuite",
      "target": "NewmanSuite.probe_universal_detector",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "NewmanSuite",
      "target": "NewmanSuite.probe_god_class_regex",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "NewmanSuite",
      "target": "NewmanSuite.probe_graph_integrity",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite",
      "target": "NewmanSuite.probe_ollama_connectivity",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "ExecutionFlow",
      "target": "ExecutionFlow.summary",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 88,
      "metadata": {}
    },
    {
      "source": "ExecutionFlow",
      "target": "ExecutionFlow.correlate_with_purpose",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 153,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector.detect_flow",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._build_nodes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._build_edges",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._is_entry_point",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 268,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._detect_entry_points",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._find_reachable",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 312,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._detect_orphans",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 329,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._build_chains",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._trace_chain",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 364,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._check_layer_violation",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 379,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector",
      "target": "ExecutionFlowDetector._detect_integration_errors",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 391,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 69,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor.extract_card",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._enrich_from_source",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 115,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._enrich_from_ast",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._get_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._get_decorator_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 207,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._extract_imports",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._infer_layer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 238,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor._add_graph_context",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 248,
      "metadata": {}
    },
    {
      "source": "SmartExtractor",
      "target": "SmartExtractor.extract_unknowns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "IntentAnalysis",
      "target": "IntentAnalysis.__post_init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 37,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector.analyze",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 98,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector._analyze_single",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector._detect_pattern_heuristic",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 172,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector._detect_smells_heuristic",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 190,
      "metadata": {}
    },
    {
      "source": "IntentDetector",
      "target": "IntentDetector._escalate_to_llm",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 209,
      "metadata": {}
    },
    {
      "source": "MockSID",
      "target": "MockSID.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 256,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 25,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.load_initial_graph",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.enrich_node",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 78,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.get_node",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 102,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.validate",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 105,
      "metadata": {}
    },
    {
      "source": "CodebaseState",
      "target": "CodebaseState.export",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 124,
      "metadata": {}
    },
    {
      "source": "shortest_path",
      "target": "shortest_path.resolve",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 237,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine.analyze",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_missing_repositories",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 276,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_missing_tests",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_service_layer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_cqrs_opportunity",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_god_class_risk",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 352,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_pure_function_opportunity",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 377,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine._check_layer_violations",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine",
      "target": "InsightsEngine.get_report",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 469,
      "metadata": {}
    },
    {
      "source": "SystemHealth",
      "target": "SystemHealth.check_all",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "SystemHealth",
      "target": "SystemHealth.print_checklist",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "CompleteCodebase",
      "target": "CompleteCodebase.get_stats",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 104,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 119,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._init_parsers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 123,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._extract_file",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._extract_function",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor.analyze_body",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 265,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._extract_class",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 322,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._extract_instance_vars",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 404,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor.visit",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 406,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._parse_parameters",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 418,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._get_callee",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 444,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor._extract_literals",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 460,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor",
      "target": "CompleteExtractor.export_json",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 480,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 55,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._check_availability",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 58,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector.analyze",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._parse_bandit_output",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._categorize_issue",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 136,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._build_purity_map",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 151,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._fallback_analysis",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 173,
      "metadata": {}
    },
    {
      "source": "PurityDetector",
      "target": "PurityDetector._heuristic_check",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 201,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier",
      "target": "ParticleClassifier.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 15,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier",
      "target": "ParticleClassifier.classify_particle",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 26,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier",
      "target": "ParticleClassifier._calculate_rpbl_score",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 53,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier",
      "target": "ParticleClassifier.get_all_particle_types",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier",
      "target": "ParticleClassifier.analyze_particle_distribution",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 14,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator.generate",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 20,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._process_graph",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._inject_data",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator.safe_json_dumps",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 139,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._apply_optimizations",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 181,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._inject_concept_images",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 196,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._generate_source_map",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 251,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator._generate_file_tree",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator",
      "target": "VisualizationGenerator.add_to_tree",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 299,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 30,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector._check_availability",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 34,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector.analyze",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 47,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector._parse_output",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector._extract_layer",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 116,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector",
      "target": "BoundaryDetector._infer_layers",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 131,
      "metadata": {}
    },
    {
      "source": "ClassifiedSymbol",
      "target": "ClassifiedSymbol.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 39,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier",
      "target": "SymbolClassifier.classify",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier",
      "target": "SymbolClassifier._classify_by_inheritance",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 243,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier",
      "target": "SymbolClassifier._classify_by_decorators",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 252,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier",
      "target": "SymbolClassifier._classify_by_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 267,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier",
      "target": "SymbolClassifier._classify_by_evidence",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 295,
      "metadata": {}
    },
    {
      "source": "PythonSymbol",
      "target": "PythonSymbol.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser.parse",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 52,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._measure_depth",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 72,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._extract_recursive",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "Visitor",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 89,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 90,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_ClassDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_FunctionDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 106,
      "metadata": {}
    },
    {
      "source": "Visitor",
      "target": "Visitor.visit_AsyncFunctionDef",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 111,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._extract_iterative",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 118,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._extract_class",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 146,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._extract_function",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_decorators",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 174,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_attribute_name",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 189,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_base_classes",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 200,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_params",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 210,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_return_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_docstring",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 254,
      "metadata": {}
    },
    {
      "source": "PythonASTParser",
      "target": "PythonASTParser._get_body_source",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 258,
      "metadata": {}
    },
    {
      "source": "Import",
      "target": "Import.__post_init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 21,
      "metadata": {}
    },
    {
      "source": "ImportExtractor",
      "target": "ImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 30,
      "metadata": {}
    },
    {
      "source": "PythonImportExtractor",
      "target": "PythonImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "JavaScriptImportExtractor",
      "target": "JavaScriptImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "JavaImportExtractor",
      "target": "JavaImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 115,
      "metadata": {}
    },
    {
      "source": "GoImportExtractor",
      "target": "GoImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 132,
      "metadata": {}
    },
    {
      "source": "RustImportExtractor",
      "target": "RustImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 163,
      "metadata": {}
    },
    {
      "source": "CSharpImportExtractor",
      "target": "CSharpImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "RubyImportExtractor",
      "target": "RubyImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 187,
      "metadata": {}
    },
    {
      "source": "PHPImportExtractor",
      "target": "PHPImportExtractor.extract",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 202,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 36,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository._load_from_canonical",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository._load_dunder_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository._load_decorator_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 112,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository._load_inheritance_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository._load_default_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_path_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 402,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_prefix_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 406,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_suffix_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 410,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_dunder_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 414,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_decorator_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 418,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_inheritance_patterns",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 422,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.get_all_roles",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_prefix",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 441,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_suffix",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 490,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_dunder",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 501,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_param_type",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 515,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_import",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 538,
      "metadata": {}
    },
    {
      "source": "PatternRepository",
      "target": "PatternRepository.classify_by_path",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 567,
      "metadata": {}
    },
    {
      "source": "OptimizationSchema",
      "target": "OptimizationSchema.to_dict",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.__init__",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 69,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository._load_default_schemas",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.get",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 403,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.get_all",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 407,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.get_by_category",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 411,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.get_by_effort",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 415,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.list_ids",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 419,
      "metadata": {}
    },
    {
      "source": "SchemaRepository",
      "target": "SchemaRepository.count",
      "edge_type": "contains",
      "weight": 1.0,
      "confidence": 1.0,
      "file_path": "",
      "line": 423,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_how",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 7,
      "metadata": {}
    },
    {
      "source": "_enrich_with_where",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "_enrich_with_where",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "_enrich_with_where",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "_enrich_with_where",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "_enrich_with_where",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.__init__",
      "target": "get_pattern_repository",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.__init__",
      "target": "get_pattern_repository",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.__init__",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "_fallback_analysis",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "_extract_python_particles_ast",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "_extract_particles",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "_extract_touchpoints",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_file",
      "target": "_extract_raw_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_particles",
      "target": "_classify_class_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_particles",
      "target": "_classify_class_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_particles",
      "target": "_classify_function_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_particles",
      "target": "_classify_class_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_particles",
      "target": "_classify_function_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_class_pattern",
      "target": "_get_particle_type_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_class_pattern",
      "target": "_detect_by_keywords",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_class_pattern",
      "target": "_calculate_confidence",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_function_pattern",
      "target": "_extract_function_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 273,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_function_pattern",
      "target": "_get_function_type_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 273,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_function_pattern",
      "target": "_calculate_confidence",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 273,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "classify_by_param_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "classify_by_path",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "classify_by_prefix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "classify_by_suffix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "_get_particle_type_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "_get_function_type_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._classify_extracted_symbol",
      "target": "normalize_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "visit_ClassDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_base_class_names",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "visit_FunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_function_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_function_params",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_docstring",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_get_return_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "visit_AsyncFunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "visit_FunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_recursive",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "_get_base_class_names",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 688,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 688,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 688,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 688,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_get_function_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_get_function_params",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_get_docstring",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_get_return_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 713,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_AsyncFunctionDef",
      "target": "visit_FunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 759,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_base_class_names",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_function_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_function_params",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_docstring",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_get_return_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast_iterative",
      "target": "evidence_for_line",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 766,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "_extract_particles",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "_measure_python_ast_depth",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "_extract_python_particles_ast_iterative",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "_extract_python_particles_ast_recursive",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_particles_ast",
      "target": "_extract_python_particles_ast_iterative",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 876,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._get_particle_type_by_name",
      "target": "_tokenize_identifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 976,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._get_function_type_by_name",
      "target": "_tokenize_identifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1054,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_touchpoints",
      "target": "count",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1175,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_python_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_js_ts_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_java_like_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_csharp_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_go_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_rust_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_ruby_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_raw_imports",
      "target": "_extract_php_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1203,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_python_imports",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1223,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._summarize_depth_metrics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1441,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "_extract_js_ts_directory_with_typescript",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "analyze_file",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine.analyze_directory",
      "target": "_summarize_depth_metrics",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1500,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "TreeSitterUniversalEngine._extract_js_ts_directory_with_typescript",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 1545,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.__init__",
      "target": "_init_parsers",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 95,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._init_parsers",
      "target": "load_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 99,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "CodeGraph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.extract",
      "target": "_extract_from_file",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 103,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "get_qualified_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "extract_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "extract_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "get_qualified_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "extract_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "get_qualified_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "_extract_calls",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "_extract_data_flow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_from_file",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 162,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "extract_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "get_qualified_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "extract_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "get_qualified_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "_extract_calls",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "_extract_data_flow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "_extract_callee",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_calls",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 296,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "_extract_callee",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 299,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 299,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 299,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "_track_variable_usage",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._extract_data_flow",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 326,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "Node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.visit",
      "target": "_track_variable_usage",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._track_variable_usage",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "GraphExtractor._track_variable_usage",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.to_mermaid",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 378,
      "metadata": {}
    },
    {
      "source": "GraphExtractor.export_json",
      "target": "get_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 413,
      "metadata": {}
    },
    {
      "source": "AnalyzerConfig.__str__",
      "target": "AnalyzerConfig",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 53,
      "metadata": {}
    },
    {
      "source": "OllamaClient.__init__",
      "target": "OllamaConfig",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "OllamaClient.__init__",
      "target": "_check_ollama_available",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "OllamaClient.classify",
      "target": "_get_cache_key",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 60,
      "metadata": {}
    },
    {
      "source": "classify_component_with_ollama",
      "target": "OllamaClient",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "classify_component_with_ollama",
      "target": "ComponentCard",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "classify_component_with_ollama",
      "target": "classify",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "classify_component_with_ollama",
      "target": "validate",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 129,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite.analyze_repository",
      "target": "_find_source_files",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite.analyze_repository",
      "target": "_detect_language",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite.analyze_repository",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite.analyze_repository",
      "target": "_analyze_file",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite.analyze_repository",
      "target": "_generate_recommendations",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 110,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._detect_language",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 208,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_file",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_file",
      "target": "_analyze_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_file",
      "target": "_analyze_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 223,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_class",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 279,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_class",
      "target": "_generate_refactor_suggestions",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 279,
      "metadata": {}
    },
    {
      "source": "GodClassDetectorLite._analyze_class",
      "target": "GodClassMetrics",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 279,
      "metadata": {}
    },
    {
      "source": "UnifiedAnalysisOutput.save",
      "target": "to_dict",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 170,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "UnifiedAnalysisOutput",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "create_unified_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 176,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "TreeSitterUniversalEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "analyze_file",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "analyze_directory",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "ParticleClassifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "classify_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "StatsGenerator",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "generate_comprehensive_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "extract_call_edges",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "apply_graph_inference",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "create_unified_output",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "analyze",
      "target": "save",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 298,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "extract_call_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 484,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.__init__",
      "target": "_init_parsers",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.extract",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 249,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.extract",
      "target": "_extract_atoms",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 249,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.extract",
      "target": "_extract_molecules",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 249,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.extract",
      "target": "_infer_organelles",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 249,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_atoms",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 283,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_atoms",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 283,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_atoms",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 283,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_atoms",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 283,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_molecules",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_molecules",
      "target": "_classify_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_molecules",
      "target": "_classify_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_molecules",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._extract_molecules",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 310,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "_classify_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 314,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "_classify_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 314,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_class",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_class",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_class",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_function",
      "target": "_has_io_calls",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 398,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_function",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 398,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_function",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 398,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._classify_function",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 398,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor._infer_organelles",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 462,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.visit",
      "target": "Hadron",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "AtomExtractor.summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "apply_heuristics",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "_generate_summary",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "_generate_detailed_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.generate_comprehensive_stats",
      "target": "_calculate_performance",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 83,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 83,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_summary",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 83,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._generate_detailed_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 108,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.save_results",
      "target": "_write_human_readable_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "StatsGenerator.save_results",
      "target": "_write_particles_csv",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "StatsGenerator._write_particles_csv",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "_create_nodes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "_build_hierarchy",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "_compute_composite_purposes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "_assign_layers",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "_analyze_flow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector.detect_field",
      "target": "PurposeField",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._create_nodes",
      "target": "PurposeNode",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 194,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "PurposeFieldDetector._analyze_flow",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "detect_purpose_field",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 328,
      "metadata": {}
    },
    {
      "source": "detect_purpose_field",
      "target": "summary",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 328,
      "metadata": {}
    },
    {
      "source": "detect_purpose_field",
      "target": "PurposeFieldDetector",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 328,
      "metadata": {}
    },
    {
      "source": "detect_purpose_field",
      "target": "detect_field",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 328,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "extract_name_from_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "normalize_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_semantic_duplicates",
      "target": "DuplicateGroup",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 81,
      "metadata": {}
    },
    {
      "source": "find_structural_duplicates",
      "target": "DuplicateGroup",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 134,
      "metadata": {}
    },
    {
      "source": "find_over_engineering",
      "target": "OverEngineeringSignal",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 177,
      "metadata": {}
    },
    {
      "source": "find_over_engineering",
      "target": "OverEngineeringSignal",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 177,
      "metadata": {}
    },
    {
      "source": "find_over_engineering",
      "target": "OverEngineeringSignal",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 177,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "load_graph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "load_semantic_ids",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "load_semantic_ids",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "find_semantic_duplicates",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "find_structural_duplicates",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "analyze_redundancy",
      "target": "find_over_engineering",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 331,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.__init__",
      "target": "_init_canonical_atoms",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._init_canonical_atoms",
      "target": "_add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 51,
      "metadata": {}
    },
    {
      "source": "AtomRegistry._add",
      "target": "AtomDefinition",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 246,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.add_discovery",
      "target": "AtomDefinition",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 260,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.get_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 290,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.get_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 290,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.get_stats",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 290,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.export_canon",
      "target": "get_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "AtomRegistry.print_summary",
      "target": "get_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "_try_rel",
      "target": "_posix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 25,
      "metadata": {}
    },
    {
      "source": "_try_rel",
      "target": "_posix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 25,
      "metadata": {}
    },
    {
      "source": "_python_module_from_rel",
      "target": "_posix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 124,
      "metadata": {}
    },
    {
      "source": "_build_python_module_index",
      "target": "_python_candidate_roots",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "_build_python_module_index",
      "target": "_python_module_from_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "_build_python_module_index",
      "target": "_try_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "_build_python_module_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "_resolve_js_relative",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "_resolve_js_relative",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "_resolve_js_relative",
      "target": "_try_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_build_python_module_index",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_python_stdlib_roots",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_try_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_python_resolve_relative",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_resolve_js_relative",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "_npm_package_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "DependencyAnalyzer.analyze_repository",
      "target": "ResolvedDependency",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 222,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "TreeSitterUniversalEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "ParticleClassifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "StatsGenerator",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "DependencyAnalyzer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "ReportGenerator",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.__init__",
      "target": "GodClassDetectorLite",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 23,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "analyze_directory",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "classify_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "generate_comprehensive_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "save_results",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.analyze_repository",
      "target": "generate",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 31,
      "metadata": {}
    },
    {
      "source": "UniversalPatternDetector.get_quick_stats",
      "target": "get_all_particle_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 101,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.__init__",
      "target": "_build_lookups",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 28,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.__init__",
      "target": "get_pattern_repository",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 28,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier._build_lookups",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 43,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get_path_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "AtomClassification",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get_prefix_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "AtomClassification",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get_suffix_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "AtomClassification",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_by_name",
      "target": "AtomClassification",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 82,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_semantic_id",
      "target": "classify_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.classify_semantic_id",
      "target": "classify_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.list_all_atoms",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 267,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.list_all_atoms",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 267,
      "metadata": {}
    },
    {
      "source": "AtomClassifier.list_all_atoms",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 267,
      "metadata": {}
    },
    {
      "source": "reclassify_semantic_ids",
      "target": "AtomClassifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 281,
      "metadata": {}
    },
    {
      "source": "reclassify_semantic_ids",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 281,
      "metadata": {}
    },
    {
      "source": "reclassify_semantic_ids",
      "target": "classify_semantic_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 281,
      "metadata": {}
    },
    {
      "source": "reclassify_semantic_ids",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 281,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_build_nodes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_classify_time_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_estimate_costs",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_calculate_hotspots",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_find_critical_path",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "_calculate_distributions",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor.predict",
      "target": "PerformanceProfile",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 164,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "PerformancePredictor._build_nodes",
      "target": "PerformanceNode",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 213,
      "metadata": {}
    },
    {
      "source": "predict_performance",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 340,
      "metadata": {}
    },
    {
      "source": "predict_performance",
      "target": "detect_execution_flow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 340,
      "metadata": {}
    },
    {
      "source": "predict_performance",
      "target": "summary",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 340,
      "metadata": {}
    },
    {
      "source": "predict_performance",
      "target": "PerformancePredictor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 340,
      "metadata": {}
    },
    {
      "source": "predict_performance",
      "target": "predict",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 340,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "TreeSitterUniversalEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "analyze_directory",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "collect_ast_node_types",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 45,
      "metadata": {}
    },
    {
      "source": "test_totality",
      "target": "TotalityReport",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "test_totality",
      "target": "classify_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "test_totality",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "test_totality",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "test_determinism",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 123,
      "metadata": {}
    },
    {
      "source": "test_determinism",
      "target": "classify_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 123,
      "metadata": {}
    },
    {
      "source": "test_determinism",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 123,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "TreeSitterUniversalEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "_classify_extracted_symbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "run_standard_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 197,
      "metadata": {}
    },
    {
      "source": "Graph.to_json",
      "target": "default_serializer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 54,
      "metadata": {}
    },
    {
      "source": "edges_from_internal_edges_list",
      "target": "Edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "edges_from_internal_edges_list",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator.__init__",
      "target": "_load_laws",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 76,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator.evaluate",
      "target": "EvaluationResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator.evaluate",
      "target": "_evaluate_law",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator.evaluate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._evaluate_law",
      "target": "_check_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 117,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_edge_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_calls",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_methods",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_fields",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_fields",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "_get_edge_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "Violation",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._check_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 148,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_edge_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_edge_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_edge_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_imports",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 258,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_calls",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 262,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_calls",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 262,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_methods",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_methods",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_fields",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 270,
      "metadata": {}
    },
    {
      "source": "AntimatterEvaluator._get_fields",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 270,
      "metadata": {}
    },
    {
      "source": "check_purity_violation",
      "target": "_check_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "check_purity_violation",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 280,
      "metadata": {}
    },
    {
      "source": "check_command_returns",
      "target": "_check_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "check_command_returns",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "check_query_mutation",
      "target": "_check_particle",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "check_query_mutation",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.__init__",
      "target": "_build_registry",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 34,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "CanonicalType",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry._build_registry",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 48,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.get_type",
      "target": "normalize",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 104,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.get_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 104,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.get_layer",
      "target": "get_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 109,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.get_layer_weight",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.types_in_layer",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 122,
      "metadata": {}
    },
    {
      "source": "TypeRegistry.get_rpbl",
      "target": "get_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 130,
      "metadata": {}
    },
    {
      "source": "get_registry",
      "target": "get_instance",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 137,
      "metadata": {}
    },
    {
      "source": "normalize_type",
      "target": "get_registry",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "normalize_type",
      "target": "normalize",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "is_valid_type",
      "target": "get_registry",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 147,
      "metadata": {}
    },
    {
      "source": "is_valid_type",
      "target": "is_valid",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 147,
      "metadata": {}
    },
    {
      "source": "get_all_types",
      "target": "get_registry",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 152,
      "metadata": {}
    },
    {
      "source": "get_all_types",
      "target": "all_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 152,
      "metadata": {}
    },
    {
      "source": "get_layer",
      "target": "get_registry",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 157,
      "metadata": {}
    },
    {
      "source": "SemanticID.__post_init__",
      "target": "_compute_hash",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 124,
      "metadata": {}
    },
    {
      "source": "SemanticID.__str__",
      "target": "to_string",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 168,
      "metadata": {}
    },
    {
      "source": "SemanticID.__repr__",
      "target": "SemanticID",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "SemanticID.__repr__",
      "target": "to_string",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 171,
      "metadata": {}
    },
    {
      "source": "SemanticID.parse",
      "target": "Continent",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "SemanticID.parse",
      "target": "Fundamental",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "SemanticID.parse",
      "target": "Level",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "TreeSitterUniversalEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "_extract_python_particles_ast",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "from_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.generate_ids",
      "target": "from_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 307,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_function",
      "target": "SemanticID",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 361,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_class",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_class",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_class",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_class",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_class",
      "target": "SemanticID",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 425,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_atom",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 515,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_atom",
      "target": "SemanticID",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 515,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticIDGenerator.from_particle",
      "target": "SemanticID",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 534,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix.to_llm_context",
      "target": "to_string",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 640,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix.export_for_embedding",
      "target": "to_string",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix.export_for_embedding",
      "target": "to_llm_context",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "SemanticMatrix.export_for_embedding",
      "target": "similarity_vector",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 673,
      "metadata": {}
    },
    {
      "source": "classify_component",
      "target": "format_card_for_llm",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "classify_component",
      "target": "call_ollama",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "classify_component",
      "target": "validate",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "test_pipeline",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "test_pipeline",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "test_pipeline",
      "target": "SmartExtractor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "test_pipeline",
      "target": "extract_unknowns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "test_pipeline",
      "target": "classify_component",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "apply_heuristics",
      "target": "HeuristicClassifier",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 527,
      "metadata": {}
    },
    {
      "source": "apply_heuristics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 527,
      "metadata": {}
    },
    {
      "source": "apply_heuristics",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 527,
      "metadata": {}
    },
    {
      "source": "apply_heuristics",
      "target": "classify_by_pattern",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 527,
      "metadata": {}
    },
    {
      "source": "apply_heuristics",
      "target": "get_pattern_report",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 527,
      "metadata": {}
    },
    {
      "source": "LanguageLoader.get_supported_languages",
      "target": "load_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 85,
      "metadata": {}
    },
    {
      "source": "_safe_rel",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 16,
      "metadata": {}
    },
    {
      "source": "_safe_rel",
      "target": "_posix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 16,
      "metadata": {}
    },
    {
      "source": "_safe_rel",
      "target": "_posix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 16,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_safe_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_safe_rel",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_stable_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "ComponentRow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_write_components_csv",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_build_mermaid",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator.generate",
      "target": "_build_markdown",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_group_key",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_group_key",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_group_key",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_mermaid_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_mermaid_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_mermaid",
      "target": "_mermaid_id",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "ReportGenerator._build_markdown",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 198,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "infer_from_structure",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 133,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.build_graph_index",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 226,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.get_neighbor_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.get_neighbor_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.get_neighbor_types",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.get_neighbor_types",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 257,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get_neighbor_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_type",
      "target": "get_neighbor_types",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 269,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_all",
      "target": "build_graph_index",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_all",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_all",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_all",
      "target": "infer_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "GraphTypeInference.infer_all",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 319,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "GraphTypeInference",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "infer_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "infer_from_structure",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "apply_graph_inference",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 360,
      "metadata": {}
    },
    {
      "source": "FixGenerator.generate_fix",
      "target": "CodeTemplate",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 468,
      "metadata": {}
    },
    {
      "source": "FixGenerator.generate_all_fixes",
      "target": "generate_fix",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 507,
      "metadata": {}
    },
    {
      "source": "generate_fixes",
      "target": "generate_insights",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 525,
      "metadata": {}
    },
    {
      "source": "generate_fixes",
      "target": "FixGenerator",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 525,
      "metadata": {}
    },
    {
      "source": "generate_fixes",
      "target": "generate_all_fixes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 525,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.run_all",
      "target": "probe_universal_detector",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.run_all",
      "target": "probe_god_class_regex",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.run_all",
      "target": "probe_graph_integrity",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.run_all",
      "target": "probe_ollama_connectivity",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 27,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_universal_detector",
      "target": "UniversalPatternDetector",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_universal_detector",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_universal_detector",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 35,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_god_class_regex",
      "target": "GodClassDetectorLite",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_god_class_regex",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_god_class_regex",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_god_class_regex",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 63,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "Graph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "Component",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "add_component",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "to_json",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_graph_integrity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "OllamaConfig",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "OllamaClient",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "is_available",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "NewmanSuite.probe_ollama_connectivity",
      "target": "ProbeResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 113,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_build_nodes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_build_edges",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_detect_entry_points",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_find_reachable",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_detect_orphans",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_build_chains",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "_detect_integration_errors",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector.detect_flow",
      "target": "ExecutionFlow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 158,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "_is_entry_point",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_nodes",
      "target": "FlowNode",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 204,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_edges",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 245,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._find_reachable",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 312,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._find_reachable",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 312,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_chains",
      "target": "_trace_chain",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_chains",
      "target": "_check_layer_violation",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._build_chains",
      "target": "CausalityChain",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 341,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._trace_chain",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 364,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._trace_chain",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 364,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._check_layer_violation",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 379,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._check_layer_violation",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 379,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._detect_integration_errors",
      "target": "IntegrationError",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 391,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._detect_integration_errors",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 391,
      "metadata": {}
    },
    {
      "source": "ExecutionFlowDetector._detect_integration_errors",
      "target": "IntegrationError",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 391,
      "metadata": {}
    },
    {
      "source": "detect_execution_flow",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 419,
      "metadata": {}
    },
    {
      "source": "detect_execution_flow",
      "target": "summary",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 419,
      "metadata": {}
    },
    {
      "source": "detect_execution_flow",
      "target": "ExecutionFlowDetector",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 419,
      "metadata": {}
    },
    {
      "source": "detect_execution_flow",
      "target": "detect_flow",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 419,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "ComponentCard",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "_enrich_from_source",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "_infer_layer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_card",
      "target": "_add_graph_context",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_source",
      "target": "_extract_imports",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 115,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_source",
      "target": "_enrich_from_ast",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 115,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_ast",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_ast",
      "target": "_get_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_ast",
      "target": "_get_decorator_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._enrich_from_ast",
      "target": "_get_decorator_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 143,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._get_decorator_name",
      "target": "_get_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 207,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._get_decorator_name",
      "target": "_get_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 207,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._extract_imports",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 217,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._add_graph_context",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 248,
      "metadata": {}
    },
    {
      "source": "SmartExtractor._add_graph_context",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 248,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_unknowns",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_unknowns",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_unknowns",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "SmartExtractor.extract_unknowns",
      "target": "extract_card",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 266,
      "metadata": {}
    },
    {
      "source": "IntentDetector.analyze",
      "target": "_analyze_single",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 98,
      "metadata": {}
    },
    {
      "source": "IntentDetector.analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 98,
      "metadata": {}
    },
    {
      "source": "IntentDetector.analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 98,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "IntentAnalysis",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "_detect_pattern_heuristic",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "_detect_smells_heuristic",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "_escalate_to_llm",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._analyze_single",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 138,
      "metadata": {}
    },
    {
      "source": "IntentDetector._detect_pattern_heuristic",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 172,
      "metadata": {}
    },
    {
      "source": "IntentDetector._detect_pattern_heuristic",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 172,
      "metadata": {}
    },
    {
      "source": "IntentDetector._detect_smells_heuristic",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 190,
      "metadata": {}
    },
    {
      "source": "IntentDetector._detect_smells_heuristic",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 190,
      "metadata": {}
    },
    {
      "source": "_enrich_with_why",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "_enrich_with_why",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "_enrich_with_why",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 220,
      "metadata": {}
    },
    {
      "source": "run_health_check",
      "target": "NewmanSuite",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 15,
      "metadata": {}
    },
    {
      "source": "run_health_check",
      "target": "run_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 15,
      "metadata": {}
    },
    {
      "source": "CodebaseState.load_initial_graph",
      "target": "to_dict",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "CodebaseState.load_initial_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "CodebaseState.load_initial_graph",
      "target": "to_dict",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "CodebaseState.get_node",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 102,
      "metadata": {}
    },
    {
      "source": "CodebaseState.validate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 105,
      "metadata": {}
    },
    {
      "source": "CodebaseState.validate",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 105,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "add_node",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "add_edge",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "load_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 65,
      "metadata": {}
    },
    {
      "source": "find_bottlenecks",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 107,
      "metadata": {}
    },
    {
      "source": "find_bottlenecks",
      "target": "NodeStats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 107,
      "metadata": {}
    },
    {
      "source": "find_bottlenecks",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 107,
      "metadata": {}
    },
    {
      "source": "find_bottlenecks",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 107,
      "metadata": {}
    },
    {
      "source": "find_bottlenecks",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 107,
      "metadata": {}
    },
    {
      "source": "find_pagerank",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "find_pagerank",
      "target": "NodeStats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "find_pagerank",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "find_pagerank",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "find_pagerank",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 142,
      "metadata": {}
    },
    {
      "source": "find_communities_leiden",
      "target": "find_communities_louvain",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "find_communities_leiden",
      "target": "Graph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 180,
      "metadata": {}
    },
    {
      "source": "find_communities",
      "target": "find_communities_leiden",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "find_communities",
      "target": "find_communities_louvain",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 253,
      "metadata": {}
    },
    {
      "source": "shortest_path",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 284,
      "metadata": {}
    },
    {
      "source": "shortest_path",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 284,
      "metadata": {}
    },
    {
      "source": "shortest_path",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 284,
      "metadata": {}
    },
    {
      "source": "shortest_path",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 284,
      "metadata": {}
    },
    {
      "source": "shortest_path.resolve",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "suggest_refactoring_cuts",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 318,
      "metadata": {}
    },
    {
      "source": "suggest_refactoring_cuts",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 318,
      "metadata": {}
    },
    {
      "source": "suggest_refactoring_cuts",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 318,
      "metadata": {}
    },
    {
      "source": "suggest_refactoring_cuts",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 318,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "load_graph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "GraphAnalysisResult",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "find_bottlenecks",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "find_pagerank",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "find_communities",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "analyze_full",
      "target": "find_bridges",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 350,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_missing_repositories",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_missing_tests",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_service_layer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_cqrs_opportunity",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_god_class_risk",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_pure_function_opportunity",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.analyze",
      "target": "_check_layer_violations",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 241,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_repositories",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 276,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_repositories",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 276,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_repositories",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 276,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_repositories",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 276,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_missing_tests",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 294,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_service_layer",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_service_layer",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_service_layer",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_service_layer",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 316,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_cqrs_opportunity",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_cqrs_opportunity",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_cqrs_opportunity",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 333,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_god_class_risk",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 352,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_god_class_risk",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 352,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_pure_function_opportunity",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 377,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_pure_function_opportunity",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 377,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_pure_function_opportunity",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 377,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine._check_layer_violations",
      "target": "Insight",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 395,
      "metadata": {}
    },
    {
      "source": "InsightsEngine.get_report",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 469,
      "metadata": {}
    },
    {
      "source": "generate_insights",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 505,
      "metadata": {}
    },
    {
      "source": "generate_insights",
      "target": "InsightsEngine",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 505,
      "metadata": {}
    },
    {
      "source": "generate_insights",
      "target": "analyze",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 505,
      "metadata": {}
    },
    {
      "source": "generate_insights",
      "target": "get_report",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 505,
      "metadata": {}
    },
    {
      "source": "SystemHealth.check_all",
      "target": "HealthStatus",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 32,
      "metadata": {}
    },
    {
      "source": "SystemHealth.print_checklist",
      "target": "check_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 74,
      "metadata": {}
    },
    {
      "source": "CompleteCodebase.get_stats",
      "target": "count",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 104,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.__init__",
      "target": "_init_parsers",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 119,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._init_parsers",
      "target": "load_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 123,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.extract",
      "target": "CompleteCodebase",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.extract",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.extract",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.extract",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.extract",
      "target": "_extract_file",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 127,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_file",
      "target": "_extract_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_file",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_file",
      "target": "_extract_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_file",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_file",
      "target": "_extract_literals",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 165,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "_parse_parameters",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "analyze_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "_get_callee",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "analyze_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "analyze_body",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_function",
      "target": "FunctionBody",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 212,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.analyze_body",
      "target": "_get_callee",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 265,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_class",
      "target": "_extract_instance_vars",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 322,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_class",
      "target": "ClassBody",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 322,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_instance_vars",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 404,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_instance_vars",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 404,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor._extract_instance_vars",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 404,
      "metadata": {}
    },
    {
      "source": "CompleteExtractor.export_json",
      "target": "get_stats",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 480,
      "metadata": {}
    },
    {
      "source": "PurityDetector.__init__",
      "target": "_check_availability",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 55,
      "metadata": {}
    },
    {
      "source": "PurityDetector.analyze",
      "target": "_fallback_analysis",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "PurityDetector.analyze",
      "target": "_parse_bandit_output",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "PurityDetector.analyze",
      "target": "_build_purity_map",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "PurityDetector.analyze",
      "target": "_fallback_analysis",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 71,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "_categorize_issue",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "PurityIssue",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._parse_bandit_output",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 114,
      "metadata": {}
    },
    {
      "source": "PurityDetector._fallback_analysis",
      "target": "_heuristic_check",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 173,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier.classify_particle",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 26,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier.classify_particle",
      "target": "_calculate_rpbl_score",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 26,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier.analyze_particle_distribution",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "ParticleClassifier.analyze_particle_distribution",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 67,
      "metadata": {}
    },
    {
      "source": "run_full_audit",
      "target": "resolve",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 57,
      "metadata": {}
    },
    {
      "source": "run_full_audit",
      "target": "NewmanSuite",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 57,
      "metadata": {}
    },
    {
      "source": "run_full_audit",
      "target": "run_all",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 57,
      "metadata": {}
    },
    {
      "source": "run_full_audit",
      "target": "_print_health",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 57,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator.generate",
      "target": "_process_graph",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 20,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator.generate",
      "target": "_inject_data",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 20,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator.generate",
      "target": "_apply_optimizations",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 20,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._process_graph",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 62,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._inject_data",
      "target": "safe_json_dumps",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._inject_data",
      "target": "safe_json_dumps",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._inject_data",
      "target": "safe_json_dumps",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._inject_data",
      "target": "_generate_source_map",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._inject_data",
      "target": "_generate_file_tree",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 135,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._apply_optimizations",
      "target": "_inject_concept_images",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 181,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_source_map",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 251,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_source_map",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 251,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_file_tree",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_file_tree",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_file_tree",
      "target": "add_to_tree",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_file_tree",
      "target": "add_to_tree",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "VisualizationGenerator._generate_file_tree",
      "target": "add_to_tree",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 287,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector.__init__",
      "target": "_check_availability",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 30,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector.analyze",
      "target": "_parse_output",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 47,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector.analyze",
      "target": "_infer_layers",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 47,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector._parse_output",
      "target": "BoundaryViolation",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector._parse_output",
      "target": "_extract_layer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector._parse_output",
      "target": "_extract_layer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 92,
      "metadata": {}
    },
    {
      "source": "BoundaryDetector._infer_layers",
      "target": "_extract_layer",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 131,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier.classify",
      "target": "_classify_by_inheritance",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier.classify",
      "target": "_classify_by_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier.classify",
      "target": "_classify_by_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier.classify",
      "target": "_classify_by_evidence",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "SymbolClassifier.classify",
      "target": "ClassifiedSymbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 199,
      "metadata": {}
    },
    {
      "source": "classify_symbol",
      "target": "SymbolKind",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 320,
      "metadata": {}
    },
    {
      "source": "classify_symbol",
      "target": "classify",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 320,
      "metadata": {}
    },
    {
      "source": "PythonASTParser.parse",
      "target": "_measure_depth",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 52,
      "metadata": {}
    },
    {
      "source": "PythonASTParser.parse",
      "target": "_extract_recursive",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 52,
      "metadata": {}
    },
    {
      "source": "PythonASTParser.parse",
      "target": "_extract_iterative",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 52,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "__init__",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "visit_ClassDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "_extract_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "visit_FunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "visit_AsyncFunctionDef",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_recursive",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 86,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "_extract_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "Visitor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_ClassDef",
      "target": "visit",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 94,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_FunctionDef",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 106,
      "metadata": {}
    },
    {
      "source": "Visitor.visit_AsyncFunctionDef",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 111,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_iterative",
      "target": "_extract_class",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 118,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_iterative",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 118,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_iterative",
      "target": "_extract_function",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 118,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_class",
      "target": "PythonSymbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 146,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_class",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 146,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_class",
      "target": "_get_base_classes",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 146,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_class",
      "target": "_get_docstring",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 146,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "PythonSymbol",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "_get_decorators",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "_get_params",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "_get_return_type",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "_get_docstring",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._extract_function",
      "target": "_get_body_source",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 159,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._get_decorators",
      "target": "_get_attribute_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 174,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._get_decorators",
      "target": "_get_attribute_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 174,
      "metadata": {}
    },
    {
      "source": "PythonASTParser._get_base_classes",
      "target": "_get_attribute_name",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 200,
      "metadata": {}
    },
    {
      "source": "parse_python",
      "target": "PythonASTParser",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 271,
      "metadata": {}
    },
    {
      "source": "parse_python",
      "target": "parse",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 271,
      "metadata": {}
    },
    {
      "source": "PythonImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "PythonImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 38,
      "metadata": {}
    },
    {
      "source": "JavaScriptImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "JavaScriptImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "JavaScriptImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 77,
      "metadata": {}
    },
    {
      "source": "JavaImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 115,
      "metadata": {}
    },
    {
      "source": "GoImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 132,
      "metadata": {}
    },
    {
      "source": "GoImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 132,
      "metadata": {}
    },
    {
      "source": "GoImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 132,
      "metadata": {}
    },
    {
      "source": "RustImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 163,
      "metadata": {}
    },
    {
      "source": "CSharpImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 175,
      "metadata": {}
    },
    {
      "source": "RubyImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 187,
      "metadata": {}
    },
    {
      "source": "RubyImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 187,
      "metadata": {}
    },
    {
      "source": "PHPImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 202,
      "metadata": {}
    },
    {
      "source": "PHPImportExtractor.extract",
      "target": "Import",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 202,
      "metadata": {}
    },
    {
      "source": "get_import_extractor",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 232,
      "metadata": {}
    },
    {
      "source": "get_import_extractor",
      "target": "PythonImportExtractor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 232,
      "metadata": {}
    },
    {
      "source": "extract_imports",
      "target": "get_import_extractor",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 240,
      "metadata": {}
    },
    {
      "source": "extract_imports",
      "target": "extract",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 240,
      "metadata": {}
    },
    {
      "source": "PatternRepository.__init__",
      "target": "_load_from_canonical",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 36,
      "metadata": {}
    },
    {
      "source": "PatternRepository.__init__",
      "target": "_load_default_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 36,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "get",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "_load_dunder_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "_load_decorator_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository._load_from_canonical",
      "target": "_load_inheritance_patterns",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 50,
      "metadata": {}
    },
    {
      "source": "PatternRepository.get_all_roles",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "PatternRepository.get_all_roles",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "PatternRepository.get_all_roles",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "PatternRepository.get_all_roles",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "PatternRepository.get_all_roles",
      "target": "add",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 426,
      "metadata": {}
    },
    {
      "source": "get_pattern_repository",
      "target": "PatternRepository",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 587,
      "metadata": {}
    },
    {
      "source": "SchemaRepository.__init__",
      "target": "_load_default_schemas",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 69,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "SchemaRepository._load_default_schemas",
      "target": "OptimizationSchema",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 73,
      "metadata": {}
    },
    {
      "source": "get_schema_repository",
      "target": "SchemaRepository",
      "edge_type": "calls",
      "weight": 1.0,
      "confidence": 0.7,
      "file_path": "",
      "line": 431,
      "metadata": {}
    }
  ],
  "stats": {
    "total_files": 51,
    "total_lines": 17193,
    "total_nodes": 556,
    "total_edges": 1687,
    "languages": [
      "python"
    ],
    "coverage_percentage": 100.0,
    "unknown_percentage": 0.0
  },
  "classification": {
    "by_role": {
      "Internal": 161,
      "DTO": 117,
      "Analyzer": 30,
      "Utility": 14,
      "Service": 9,
      "ApplicationService": 1,
      "Command": 17,
      "Query": 62,
      "Transformer": 51,
      "Factory": 48,
      "Validator": 10,
      "Specification": 10,
      "Repository": 3,
      "Test": 15,
      "View": 2,
      "Mapper": 2,
      "UIComponent": 1,
      "Exception": 1,
      "Stream": 1,
      "Entity": 1
    },
    "by_kind": {
      "function": 80,
      "class": 114,
      "method": 362
    },
    "by_layer": {},
    "by_confidence": {
      "high": 253,
      "medium": 299,
      "low": 4
    }
  },
  "auto_discovery": {
    "enabled": true,
    "patterns_applied": 82,
    "particles_reclassified": 82,
    "top_patterns": [
      [
        "noun_entity",
        38
      ],
      [
        "underscore_func",
        13
      ],
      [
        "visitor",
        9
      ],
      [
        "aggregate",
        5
      ],
      [
        "logging",
        2
      ],
      [
        "camelCase",
        2
      ],
      [
        "test_context",
        2
      ],
      [
        "property",
        1
      ],
      [
        "callback",
        1
      ],
      [
        "query_verb",
        1
      ],
      [
        "compare",
        1
      ],
      [
        "short_name",
        1
      ],
      [
        "template",
        1
      ],
      [
        "lifecycle",
        1
      ],
      [
        "fixture/example",
        1
      ],
      [
        "url",
        1
      ],
      [
        "data_ops",
        1
      ],
      [
        "auth",
        1
      ]
    ],
    "suggested_new_patterns": []
  },
  "dependencies": {
    "internal": [],
    "external": [],
    "stdlib": [],
    "analysis_status": "not_applied"
  },
  "antimatter": {
    "god_classes": [],
    "long_methods": [],
    "high_coupling": [],
    "analysis_status": "not_applied"
  },
  "architecture": {
    "detected_patterns": [],
    "layer_violations": [],
    "analysis_status": "applied",
    "graph_inference": {
      "total_inferred": 208,
      "rules_applied": 0,
      "details": [],
      "structural_boosted": 208,
      "parent_inherited": 0
    }
  },
  "llm_enrichment": {
    "enabled": false,
    "model": "none",
    "particles_enhanced": 0,
    "analysis_status": "not_applied"
  },
  "warnings": [],
  "recommendations": []
}